{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn, optim, cuda\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, \\\n",
    "                             TensorDataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm|\n",
    "\n",
    "from raw import load_documents\n",
    "from raw_handler import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_renet2_ft_cv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "loading word index from ./utils/word_index\n",
      "loaded word index, voc size 82948\n",
      "tokenizer size 82949\n",
      "fix input sentences# 400, tokens# 54, batch size 60\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # set up\n",
    "    parser = init_self_parser()\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available() and not args.no_cuda\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    set_seed(args)\n",
    "    args.device = device\n",
    "    print('using device', device)\n",
    "\n",
    "    args.ori_tokenizer = loading_tokenizer(args)\n",
    "    args.token_voc_l = len(args.ori_tokenizer)\n",
    "    print('tokenizer size %d' % (args.token_voc_l))\n",
    "    \n",
    "    print('fix input sentences# %d, tokens# %d, batch size %d' % (args.fix_snt_n, args.fix_token_n, args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.raw_data_dir = '../data/raw_data/ft/T3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated table shape (1556, 26)\n"
     ]
    }
   ],
   "source": [
    "old_data_dir = args.annotation_info_dir\n",
    "s_df = pd.read_csv(os.path.join(old_data_dir, \"ft_500_n.tsv\"), sep = '\\t')\n",
    "\n",
    "print('annotated table shape', s_df.shape)\n",
    "# s_df = s_df[(s_df['hit_cnt_1k'] >= 3) | ~(s_df['Ann error (GD/ikrm)'] == 1)]\n",
    "s_df = s_df[s_df['2nd_label'] != 0][['pmid', 'geneId', 'diseaseId', '2nd_label']]\n",
    "s_df.rename(columns={'2nd_label':'label'}, inplace=True)\n",
    "\n",
    "new_label_f_n = 'labels_n.txt'\n",
    "s_df.to_csv(args.raw_data_dir + new_label_f_n, sep=',', index=False)\n",
    "\n",
    "args.label_f_name = 'labels_n.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading input at ../data/raw_data/ft/T3/\n",
      "No cached file ../data/raw_data/ft/T3/cached_all_doc_0_400_54\n",
      "Creating features from dataset file at  ../data/raw_data/ft/T3/\n",
      "reading files docs.txt,sentences.txt,anns_n.txt,labels_n.txt\n",
      "load data in path ../data/raw_data/ft/T3/docs.txt\n",
      "*** Example ***                      \n",
      "unique_id: 16963499\n",
      "pairs 10 doc snt num 36\n",
      "token string: [13] characterization of (['<54487>'], ['DGCR8']) (['<54487>'], ['Pasha']) the essential cofactor for (['<29102>'], ['Drosha']) in primary mirna processing\n",
      "token ids   : [13] 1618 2 69929 69929 1 530 6954 9 40749 4 426 13 2149\n",
      "fix feature : [13] 0 0 5 5 0 0 0 0 2 0 0 0 0\n",
      "token string: [27] (['<54487>'], ['DGCR8']) (['<54487>'], ['Pasha']) is an essential cofactor for (['<29102>'], ['Drosha']) a nuclear rnase iii that cleaves the local hairpin structures embedded in long primary microrna transcripts pri-mirnas in eukaryotes\n",
      "token ids   : [27] 69929 69929 16 29 530 6954 9 40749 6 889 20168 961 14 19718 1 2099 14287 3682 10257 4 1400 426 9082 2356 13 4 20206\n",
      "fix feature : [27] 5 5 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "*** Example ***                      \n",
      "unique_id: 17579508\n",
      "pairs 1 doc snt num 9\n",
      "token string: [10] what is the best approach to (['<D012559>'], ['treating', 'schizophrenia']) in developing countries\n",
      "token ids   : [10] 3725 16 1 2048 749 7 141 4 421 1876\n",
      "fix feature : [10] 0 0 0 0 0 0 4 0 0 0\n",
      "token string: [24] background to the debate (['<D012559>'], ['Schizophrenia']) affects an estimated 25 million people in low- and middle-income countries with an average lifetime risk of about 1\n",
      "token ids   : [24] 132 7 1 8954 141 1118 29 712 583 4442 1266 4 11053 3 13 1876 5 29 1559 2625 17 2 682 89\n",
      "fix feature : [24] 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "read 494 doc, max token len 0 50\n",
      "\n",
      "reading label at ../data/raw_data/ft/T3/labels_n.txt\n",
      "loaded document #  13640 13640 13640\n",
      "padding document with fix length 400 54\n",
      "total doc # 13640, reach end # 0 (0.00%)\n",
      "total snt # 5456000, reach end # 21813 (0.40%)\n",
      "total token # 294624000, not empty # 30315301 valid rate: 10.29%\n",
      "Saving features into cached file  ../data/raw_data/ft/T3/cached_all_doc_0_400_54\n",
      "loading ended\n",
      "creading dataset, positive GDA 891, all GDA 13640, positive rate 6.53%\n",
      "--- 108.279 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "print('reading input at {}'.format(args.raw_data_dir))\n",
    "\n",
    "args.overwrite_cache = True\n",
    "\n",
    "start_time = time.time()\n",
    "features_ft_sub = load_and_cache_data(args)\n",
    "dataset_ft_sub, _, _ = convert_features_to_dataset_single(features_ft_sub)\n",
    "dataloader_ft_sub = DataLoader(dataset_ft_sub, batch_size=args.batch_size)\n",
    "print(\"--- %.3f seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 1489\n"
     ]
    }
   ],
   "source": [
    "# get the ABS model performance\n",
    "\n",
    "# tar_path = os.path.join(old_data_dir, \"full-text_sub_set_500_a_2nd.tsv\")\n",
    "tar_path = os.path.join(old_data_dir, \"ft_500_n.tsv\")\n",
    "s_df = pd.read_csv(tar_path, sep = '\\t')\n",
    "# s_df = s_df[(s_df['hit_cnt_1k'] >= 3) | ~(s_df['Ann error (GD/ikrm)'] == 1)]\n",
    "\n",
    "s_df['pmid'] = s_df['pmid'].astype(str)\n",
    "s_df['geneId'] = s_df['geneId'].astype(str)\n",
    "s_df['diseaseId'] = s_df['diseaseId'].astype(str)\n",
    "s_df.rename(columns={'2nd_label':'new_label'}, inplace=True)\n",
    "\n",
    "# annotated GDA\n",
    "abs_s_df = s_df[['pmid', 'geneId', 'diseaseId', 'new_label']].copy()\n",
    "abs_s_df_ner = s_df[['pmid', 'geneId', 'diseaseId', 'is_ner_error']].copy()\n",
    "# abs_s_df['abs_label'] = abs_s_df.apply(lambda x: x['new_label'] if x['hit_cnt_1k']>=3 else 0, axis = 1)\n",
    "\n",
    "# get the annotated GDA for model training\n",
    "n_features_ft_sub = get_ann_dataset(features_ft_sub, abs_s_df)\n",
    "N_ann_positive = len(n_features_ft_sub[1][n_features_ft_sub[1]['label']==1])\n",
    "N_ann_all = len(n_features_ft_sub[1])\n",
    "print(N_ann_positive, N_ann_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated non-NER errors GDA\n",
    "_s_df = s_df[ (s_df['is_ner_error'].isna())]\n",
    "_abs_s_df = _s_df[['pmid', 'geneId', 'diseaseId', 'new_label']].copy()\n",
    "\n",
    "ner_error_df = s_df[ ~(s_df['is_ner_error'].isna())]\n",
    "ner_error_df = ner_error_df[['pmid', 'geneId', 'diseaseId']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading BeFree and DT-Miner\n",
    "global BeFree_df, DTMiner_df, BioBERT_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get BeFree and DT-Miner data, shape (3497, 4) (4201, 4)\n"
     ]
    }
   ],
   "source": [
    "# reading BeFree GDA\n",
    "# _f = os.path.join(old_data_dir, \"classification_result_befree.txt\")\n",
    "# gdas = pd.read_csv(_f, sep=',')\n",
    "befree_f = '../benchmark/BeFree/n_out/classification_result_befree.txt'\n",
    "gdas = pd.read_csv(befree_f, sep=',')\n",
    "predicted_positive = gdas.drop_duplicates(['pmid', 'geneId', 'diseaseId'])\n",
    "BeFree_df = predicted_positive[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "BeFree_df = set_df_to_str(BeFree_df)\n",
    "BeFree_df['is_befree'] = 1\n",
    "#print(BeFree_df.shape, BeFree_df.head(3))\n",
    "\n",
    "# reading DTMiner GDA\n",
    "# _f = os.path.join(old_data_dir, \"classification_result_dtminer.txt\")\n",
    "# gdas = pd.read_csv(_f, sep=',')\n",
    "\n",
    "dtminer_f = '../benchmark/DTMiner/n_out/classification_result_dtminer.txt'\n",
    "gdas = pd.read_csv(dtminer_f, sep=',')\n",
    "predicted_positive = gdas.drop_duplicates(['pmid', 'geneId', 'diseaseId'])\n",
    "DTMiner_df = predicted_positive[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "DTMiner_df = set_df_to_str(DTMiner_df)\n",
    "DTMiner_df['is_dtminer'] = 1\n",
    "#print(DTMiner_df.shape, DTMiner_df.head(3))\n",
    "print('get BeFree and DT-Miner data, shape', BeFree_df.shape, DTMiner_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read gene/disease id\n",
    "# gdas = []\n",
    "# with open(\"../benchmark/BeFree/n_out/cooc.befree\") as f:\n",
    "#     for line in f:\n",
    "#         items = line.strip().split('\\t')\n",
    "#         gdas.append([int(items[0].split('|')[0]),\"F\", items[9], items[9], int(items[7].split('|')[0]), items[10], items[11], items[13], items[16], items[17], items[19]])\n",
    "# gdas_befree = pd.DataFrame(np.array(gdas), columns=[\"GAD_ID\",\"GAD_ASSOC\",\"GAD_GENE_SYMBOL\",\"GAD_GENE_NAME\",\"GAD_ENTREZ_ID\",\"NER_GENE_ENTITY\",\"NER_GENE_OFFSET\",\"GAD_DISEASE_NAME\",\"NER_DISEASE_ENTITY\",\"NER_DISEASE_OFFSET\",\"GAD_CONCLUSION\"])\n",
    "# gdas_befree.GAD_ID = gdas_befree.GAD_ID.astype(int)\n",
    "# gdas_befree.GAD_ENTREZ_ID = gdas_befree.GAD_ENTREZ_ID.astype(int)\n",
    "\n",
    "# gdas_for_biobert = gdas_befree  \n",
    "# gdas_for_biobert = gdas_for_biobert[['GAD_ID', 'GAD_ENTREZ_ID', 'GAD_DISEASE_NAME']].copy()\n",
    "# gdas_for_biobert = gdas_for_biobert.rename(columns={\"GAD_ID\": \"pmid\", \"GAD_ENTREZ_ID\": \"geneId\", \\\n",
    "#                                                    \"GAD_DISEASE_NAME\": \"diseaseId\"})\n",
    "\n",
    "# # read true label\n",
    "# true_pd = pd.read_csv('../benchmark/test.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "# bert_f = '../benchmark/re_outputs_1/test_results.tsv'\n",
    "# pred_df = pd.read_csv(bert_f, sep='\\t', header=None)\n",
    "# pred_df['pred'] = pred_df.apply(lambda x: 1 if x[1] > 0.5 else 0, axis = 1)\n",
    "\n",
    "# gdas_for_biobert['label'] = true_pd['label']\n",
    "# gdas_for_biobert['pred'] = pred_df['pred']\n",
    "# predicted_positive = gdas_for_biobert[gdas_for_biobert.pred==1][['pmid', 'geneId', 'diseaseId']].drop_duplicates()\n",
    "\n",
    "# predicted_positive.to_csv(\"classification_result_biobert.txt\", index=False, columns=['pmid', 'geneId', 'diseaseId'])\n",
    "\n",
    "# actual_positive = gdas_for_biobert[gdas_for_biobert['label'] == 1][['pmid', 'geneId', 'diseaseId']].drop_duplicates()\n",
    "# print(actual_positive.shape)\n",
    "# print(predicted_positive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_positive = set_df_to_str(actual_positive)\n",
    "# actual_positive_t = actual_positive.copy()\n",
    "# actual_positive_t['label'] = 1\n",
    "# tmp = abs_s_df[abs_s_df['new_label'] == 1].merge(actual_positive_t, on=['pmid', 'geneId', 'diseaseId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_n_d = {'DGCR8': {'54487'}, 'Pasha': {'54487'}, 'Drosha': {'29102'}, 'miR-17': {'723905'}, 'RNase III': {'29102'}, 'exportin-5': {'57510'}, 'Exp5': {'57510'}, 'Dicer': {'23405'}, 'Rsp5': {'23327'}, 'miR-16-1': {'406950'}, 'DG1': {'1828'}}\n",
    "# g_id_d = {'54487': {'DGCR8', 'Pasha'}, '29102': {'RNase III', 'Drosha'}, '723905': {'miR-17'}, '57510': {'exportin-5', 'Exp5'}, '23405': {'Dicer'}, '23327': {'Rsp5'}, '406950': {'miR-16-1'}, '1828': {'DG1'}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hit_t = {\n",
    "#     #id: times\n",
    "# }\n",
    "\n",
    "# sort_orders = sorted(orders.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# g_id_m = {}\n",
    "\n",
    "# for k, v in g_id_d:\n",
    "#     if k in g_id_m:\n",
    "#         continue\n",
    "#     _id_s, _name = {k}, v\n",
    "#     while True:\n",
    "#         _tmp_id_s = set()\n",
    "#         for _n in name:\n",
    "#             _tmp_id = g_n_d[_n]\n",
    "#             _tmp_id_s = _tmp_id_s | _tmp_id\n",
    "#         if _tmp_id_s == _id_s:\n",
    "#             break\n",
    "#         else:\n",
    "#             _id_s = _id_s | _tmp_id_s\n",
    "#     _tmp_hit_t = {i: hit_i[i] for i in _id_s}\n",
    "#     tar_id = sorted(_tmp_hit_t.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "#     print(sorted(_tmp_hit_t.items(), key=lambda x: x[1], reverse=True))\n",
    "#     for _id in _id_s:\n",
    "#         g_id_m[_id] = tar_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_f = '../benchmark/classification_result_biobert.txt'\n",
    "gdas = pd.read_csv(biobert_f, sep=',')\n",
    "predicted_positive = gdas.drop_duplicates(['pmid', 'geneId', 'diseaseId'])\n",
    "BioBERT_df = predicted_positive[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "BioBERT_df = set_df_to_str(BioBERT_df)\n",
    "BioBERT_df['is_biobert'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get BeFree, DT-Miner, and BioBERT data, shape (3497, 4) (4201, 4) (4598, 4)\n"
     ]
    }
   ],
   "source": [
    "print('get BeFree, DT-Miner, and BioBERT data, shape', BeFree_df.shape, DTMiner_df.shape, BioBERT_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeFree_df, DTMiner_df = None, None\n",
    "def evaluate_rst_all_info(ori_df, ori_c, ann_df, is_freeze_c = False, b_ann_df = False):\n",
    "    global BeFree_df, DTMiner_df, BioBERT_df\n",
    "    \n",
    "    if not is_freeze_c:\n",
    "        global ReNet_df\n",
    "\n",
    "    _rst = []\n",
    "    _df = ori_df.copy()\n",
    "    _c = ori_c\n",
    "    if not is_freeze_c:\n",
    "        _c = 'pred_tmp'\n",
    "        _df.rename(columns={ori_c:'pred_tmp'}, inplace=True)\n",
    "\n",
    "    # predicted GDA\n",
    "    _pred_N = sum(_df[_c])\n",
    "    _rst.append(_pred_N)\n",
    "\n",
    "    # precision on ann & based #\n",
    "    S1 = get_scores_a(_df, _c, ann_df, 'new_label')\n",
    "    _precision1, _recall, N_ann = S1\n",
    "    _rst.extend([_precision1])\n",
    "\n",
    "    # merging renet|befree|dtminer result first\n",
    "    if not is_freeze_c:\n",
    "        merge_t_df = _df.merge(BeFree_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "    else:\n",
    "        merge_t_df = ReNet_df.merge(BeFree_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "        \n",
    "    merge_t_df = merge_t_df.merge(DTMiner_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "    merge_t_df = merge_t_df.merge(BioBERT_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "    merge_t_df = merge_t_df.fillna(0)\n",
    "    ann_positive = ann_df.copy()\n",
    "    \n",
    "    df=merge_t_df\n",
    "    F=[1,1,1,1]\n",
    "    c=[_c,'is_dtminer','is_befree','is_biobert']\n",
    "#     print(c)\n",
    "    if is_freeze_c:\n",
    "        c=['is_renet','is_dtminer','is_befree','is_biobert']\n",
    "    \n",
    "    # c[0], c[2] = c[2], c[0]\n",
    "    mdl_positive = df[(df[c[0]]==F[0]) & ((df[c[1]]==F[1]) & (df[c[2]]==F[2]) & (df[c[3]]==F[3]))]\n",
    "    \n",
    "    if isinstance(b_ann_df, pd.DataFrame):\n",
    "        mdl_positive = b_ann_df.merge(mdl_positive, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "        mdl_positive = mdl_positive[mdl_positive['is_ner_error'].isna()]\n",
    "        del mdl_positive['is_ner_error']\n",
    "        \n",
    "    olp_positive = ann_positive.merge(mdl_positive, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "    \n",
    "    ann_n, mdl_n, olp_n = len(ann_positive), len(mdl_positive), len(olp_positive)\n",
    "#     print('---', ann_n, mdl_n, olp_n)\n",
    "    \n",
    "#     print('model confidence GDA {}, adding {} to TP, total TP {}'.format(mdl_n, olp_n-ann_n, olp_n))\n",
    "#     print(olp_positive.head(3))\n",
    "    \n",
    "    olp_positive['ann_label'] = olp_positive['new_label']\n",
    "    olp_positive['ann_label'] = olp_positive['ann_label'].fillna(0)\n",
    "    olp_positive = olp_positive[['pmid', 'geneId','diseaseId', 'new_label', 'ann_label']]\n",
    "    olp_positive['new_label'] = olp_positive['new_label'].fillna(1)\n",
    "    olp_positive.shape\n",
    "    targe_df = merge_t_df.merge(olp_positive, on=['pmid', 'geneId','diseaseId'], how='left')\n",
    "    targe_df['new_label'] = targe_df['new_label'].fillna(0)\n",
    "\n",
    "    tar_df = _df.copy()\n",
    "    if isinstance(b_ann_df, pd.DataFrame):\n",
    "        tar_df = tar_df.merge(b_ann_df, on=['pmid', 'geneId','diseaseId'], how='left')\n",
    "        tar_df['pred'] = tar_df.apply(lambda x: 1 if x[_c] ==1 and pd.isnull(x['is_ner_error']) else 0, axis=1)\n",
    "    else:\n",
    "        tar_df['pred'] = _df.apply(lambda x: 1 if x[_c] ==1 else 0, axis=1)\n",
    "    S2 = get_scores_a(tar_df, 'pred', targe_df, 'new_label')\n",
    "    _precision2, _, N_p2 = S2\n",
    "#     print(S2)\n",
    "    _rst.extend([_precision2])\n",
    "\n",
    "\n",
    "\n",
    "    _rst.append(_recall)\n",
    "#     print(','.join(list(map(str, _rst))))\n",
    "    return _rst\n",
    "\n",
    "def evaluate_rst_all_info_err(ori_df, ori_c, ann_df, _ann_df, abs_s_df_ner, is_freeze_c = False):\n",
    "    _rst1 = evaluate_rst_all_info(ori_df, ori_c, ann_df, is_freeze_c)\n",
    "    \n",
    "    _rst2 = evaluate_rst_all_info(ori_df, ori_c, _ann_df, is_freeze_c, abs_s_df_ner)\n",
    "#     _rst = _rst1[0], _rst1[1], _rst2[1], _rst1[2], _rst2[2], _rst1[4], _rst2[4], _rst1[6], _rst2[6]\n",
    "#     _rst = _rst1[0], _rst1[2], _rst2[2], _rst1[4], _rst2[4], _rst1[6], _rst2[6]\n",
    "    _rst = _rst1[0], _rst1[2], _rst2[2], _rst1[3], _rst2[3]\n",
    "#     _rst = _rst1, _rst2\n",
    "    return _rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sida_base_dir = args.annotation_info_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the result of the RENET trained based models, prepared for [enhanced data]\n",
      "10X modes' positive #, precision, precision[-], recall, recall[-]\n",
      "1183.0,0.6545914069081719,0.7348790322580645,0.7181208053691275,0.7251908396946565\n",
      "1333.0,0.6237845923709798,0.6971580817051509,0.7751677852348994,0.7888040712468194\n",
      "1149.0,0.6617519514310495,0.7497371188222923,0.7293064876957495,0.7353689567430025\n",
      "1258.0,0.6394611727416799,0.7157794676806084,0.7416107382550335,0.7442748091603053\n",
      "1403.0,0.6382373845060412,0.7108843537414966,0.8154362416107382,0.8180661577608143\n",
      "1235.0,0.6448748991121872,0.7255092143549952,0.761744966442953,0.7709923664122137\n",
      "1184.0,0.6717171717171717,0.755,0.7293064876957495,0.7442748091603053\n",
      "1130.0,0.691358024691358,0.7864184008762322,0.7639821029082774,0.7544529262086515\n",
      "1252.0,0.6648089171974523,0.741904761904762,0.743847874720358,0.7442748091603053\n",
      "1229.0,0.6593673965936739,0.7448275862068966,0.7706935123042505,0.7735368956743003\n",
      "avg,1235.6,0.6549952917269766,0.7362098017550498,0.7549217002237136,0.7599236641221375\n",
      "assemble, positive #, precision, precision[-], recall, recall[-]\n",
      "1,2208,0.566003616636528,0.6107660455486542,0.9272930648769575,0.9325699745547074\n",
      "2,1702,0.6172332942555686,0.6800554016620498,0.8847874720357942,0.8854961832061069\n",
      "3,1489,0.6329537843268587,0.7048917401764234,0.8422818791946308,0.8435114503816794\n",
      "4,1375,0.6475707034082668,0.7255244755244755,0.8232662192393736,0.8256997455470738\n",
      "5,1232,0.6618122977346278,0.7495049504950495,0.7953020134228188,0.7964376590330788\n",
      "6,1121,0.6817777777777778,0.7824175824175824,0.7684563758389261,0.7735368956743003\n",
      "7,1007,0.6993076162215628,0.8093480934809348,0.732662192393736,0.7404580152671756\n",
      "8,894,0.7293986636971047,0.8429752066115702,0.6845637583892618,0.693384223918575\n",
      "9,770,0.7467700258397932,0.875605815831987,0.6152125279642058,0.6234096692111959\n",
      "10,558,0.7829181494661922,0.911062906724512,0.4753914988814318,0.4847328244274809\n",
      "selected assemble models at 5+, shape (13641, 4), result:\n",
      "(1232, 0.6618122977346278, 0.7495049504950495, 0.7953020134228188, 0.7964376590330788)\n"
     ]
    }
   ],
   "source": [
    "print('get the result of the RENET2 trained based models, prepared for [enhanced data]')\n",
    "m_df = 0\n",
    "s_arr = []\n",
    "for _i in range(10):\n",
    "#     old_data_dir = '../data/ft_info'\n",
    "#     cls_file = os.path.join(old_data_dir, \"ft_base_model_rst/ft_rst_ann_2nd_%02d.tsv\" % (_i+1))\n",
    "    cls_file = os.path.join(_sida_base_dir, \"cls_rst/ft_base_dev_%02d.tsv\" % (_i+1))\n",
    "\n",
    "    y_info = pd.read_csv(cls_file, sep='\\t')\n",
    "    y_info['pmid'] = y_info['pmid'].astype(str)\n",
    "    y_info['geneId'] = y_info['geneId'].astype(str)\n",
    "    y_info['diseaseId'] = y_info['diseaseId'].astype(str)\n",
    "    y_info['label'] = y_info['label'].astype(str)\n",
    "\n",
    "    S = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "#     print(S)\n",
    "    s_arr.append(list(S))\n",
    "\n",
    "    y_info.rename(columns={'pred':'pred_%02d'%(_i+1)}, inplace=True)\n",
    "#     y_info = y_info[y_info['pred'] == 1]\n",
    "    m_df = y_info if _i == 0 else m_df.merge(y_info, on=['pmid', 'geneId','diseaseId', 'label'], how='outer')\n",
    "\n",
    "\n",
    "m_df['hit_cnt'] = m_df.apply(lambda x: sum([x['pred_%02d'%(_i+1)] for _i in range(10)]), axis=1)\n",
    "ori_m_df = m_df.copy()\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "print('10X modes\\' positive #, precision, precision[-], recall, recall[-]')\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "avg_l = list(np.mean(s_arr, axis=0))\n",
    "avg_l = ['avg'] + avg_l\n",
    "\n",
    "print(','.join(list(map(str, avg_l))))\n",
    "\n",
    "print('assemble, positive #, precision, precision[-], recall, recall[-]')\n",
    "for _i in range(1, 11):\n",
    "    ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "    ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= _i else 0, axis=1)\n",
    "\n",
    "    _rst = evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "    _rst = [_i] + list(_rst)\n",
    "    print(','.join(list(map(str, _rst))))\n",
    "\n",
    "the_p_cnt = 5\n",
    "ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= the_p_cnt else 0, axis=1)\n",
    "\n",
    "print('selected assemble models at {}+, shape {}, result:'.format(the_p_cnt, ReNet_df.shape))\n",
    "print(evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = ori_m_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected assemble models at 1+, shape (13641, 4), result:\n",
      "(2208, 0.566003616636528, 0.6107660455486542, 0.9272930648769575, 0.9325699745547074)\n"
     ]
    }
   ],
   "source": [
    "the_p_cnt = 1\n",
    "ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= the_p_cnt else 0, axis=1)\n",
    "\n",
    "print('selected assemble models at {}+, shape {}, result:'.format(the_p_cnt, ReNet_df.shape))\n",
    "print(evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497,0.7927519818799547,0.32085714285714284,0.7829977628635347\n",
      "4201,0.8104712041884817,0.2846611177170036,0.8657718120805369\n",
      "4598,0.7953216374269005,0.2692307692307692,0.912751677852349\n"
     ]
    }
   ],
   "source": [
    "_rst = evaluate_rst_all_info(BeFree_df, 'is_befree', abs_s_df, is_freeze_c=True)\n",
    "print(','.join(list(map(str, _rst))))\n",
    "\n",
    "_rst = evaluate_rst_all_info(DTMiner_df, 'is_dtminer', abs_s_df, is_freeze_c=True)\n",
    "print(','.join(list(map(str, _rst))))\n",
    "\n",
    "_rst = evaluate_rst_all_info(BioBERT_df, 'is_biobert', abs_s_df, is_freeze_c=True)\n",
    "print(','.join(list(map(str, _rst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_t_df = ReNet_df.merge(BeFree_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(DTMiner_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(BioBERT_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "\n",
    "merge_t_df = merge_t_df.fillna(0)\n",
    "\n",
    "F=[1,1,1,1]\n",
    "c=['is_renet','is_dtminer','is_befree','is_biobert']\n",
    "\n",
    "df = merge_t_df\n",
    "m_df = df[(df[c[0]]==F[0]) | ((df[c[1]]==F[1]) | (df[c[2]]==F[2]) | (df[c[3]]==F[3]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>geneId</th>\n",
       "      <th>diseaseId</th>\n",
       "      <th>is_renet</th>\n",
       "      <th>is_befree</th>\n",
       "      <th>is_dtminer</th>\n",
       "      <th>is_biobert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17579508</td>\n",
       "      <td>30818</td>\n",
       "      <td>D012559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10579720</td>\n",
       "      <td>109676</td>\n",
       "      <td>D009135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10579720</td>\n",
       "      <td>109676</td>\n",
       "      <td>D009140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10579720</td>\n",
       "      <td>109676</td>\n",
       "      <td>D007738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10579720</td>\n",
       "      <td>109676</td>\n",
       "      <td>D009224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13642</th>\n",
       "      <td>32313880</td>\n",
       "      <td>5879</td>\n",
       "      <td>D005355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13643</th>\n",
       "      <td>25965909</td>\n",
       "      <td>24148</td>\n",
       "      <td>D004194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13644</th>\n",
       "      <td>30571509</td>\n",
       "      <td>114548</td>\n",
       "      <td>D007239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13645</th>\n",
       "      <td>30571509</td>\n",
       "      <td>3553</td>\n",
       "      <td>D007239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13646</th>\n",
       "      <td>19133133</td>\n",
       "      <td>3303</td>\n",
       "      <td>D012769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5389 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pmid  geneId diseaseId  is_renet  is_befree  is_dtminer  is_biobert\n",
       "0      17579508   30818   D012559       1.0        0.0         0.0         0.0\n",
       "1      10579720  109676   D009135       1.0        1.0         1.0         1.0\n",
       "2      10579720  109676   D009140       1.0        1.0         1.0         1.0\n",
       "3      10579720  109676   D007738       1.0        1.0         1.0         1.0\n",
       "4      10579720  109676   D009224       1.0        0.0         0.0         0.0\n",
       "...         ...     ...       ...       ...        ...         ...         ...\n",
       "13642  32313880    5879   D005355       0.0        1.0         0.0         1.0\n",
       "13643  25965909   24148   D004194       0.0        1.0         0.0         1.0\n",
       "13644  30571509  114548   D007239       0.0        1.0         1.0         1.0\n",
       "13645  30571509    3553   D007239       0.0        1.0         1.0         1.0\n",
       "13646  19133133    3303   D012769       0.0        0.0         1.0         1.0\n",
       "\n",
       "[5389 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_idx = []\n",
    "rst_idx.append(list(m_df[m_df[c[0]] == 1].index))\n",
    "rst_idx.append(list(m_df[m_df[c[1]] == 1].index))\n",
    "rst_idx.append(list(m_df[m_df[c[2]] == 1].index))\n",
    "rst_idx.append(list(m_df[m_df[c[3]] == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from utils import venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = venn.get_labels(rst_idx, fill=['number'])\n",
    "fig, ax = venn.venn4(labels, names=['RENET2', 'BeFree', 'DTMiner', 'BioBERT'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get enhanced data\n",
      "887 1489 285 32.13077790304397 1774\n"
     ]
    }
   ],
   "source": [
    "print('get enhanced data')\n",
    "\n",
    "# addon_olp3p_feature = get_addon_data_ft(features_ft_sub, mdl_df, -1, 1, False)\n",
    "# merging renet|befree|dtminer result first\n",
    "merge_t_df = ReNet_df.merge(BeFree_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(DTMiner_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(BioBERT_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df[~merge_t_df.is_renet.isnull()]\n",
    "merge_t_df = merge_t_df.fillna(0)\n",
    "\n",
    "F=[0, 0, 0, 0] #all negative\n",
    "# F=[1, 1, 1] #all positive\n",
    "c=['is_renet','is_dtminer','is_befree', 'is_biobert']\n",
    "df = merge_t_df\n",
    "mdl_df = df[(df[c[0]]==F[0]) & ((df[c[1]]==F[1]) & (df[c[2]]==F[2]) & (df[c[3]]==F[3]))].copy()\n",
    "mdl_df['is_tar'] = 1\n",
    "\n",
    "# opt 1 filter the GDA annotated\n",
    "mdl_df = mdl_df.merge(abs_s_df, how='left')\n",
    "mdl_df = mdl_df[mdl_df.new_label.isnull()]\n",
    "mdl_df_n = mdl_df[['pmid', 'geneId','diseaseId', 'is_tar']].copy()\n",
    "\n",
    "N_ann_positive = len(n_features_ft_sub[1][n_features_ft_sub[1]['label']==1])\n",
    "N_ann_all = len(n_features_ft_sub[1])\n",
    "#print(N_ann_positive, N_ann_all)\n",
    "# N_ann_positive += len(addon_olp3p_feature[1])\n",
    "add_to_rate = 2\n",
    "N_add_nan = int(N_ann_positive * add_to_rate - N_ann_all)\n",
    "# N_add_nan = int(500 * (5/4))\n",
    "print(N_ann_positive, N_ann_all, N_add_nan, 100.*N_add_nan/N_ann_positive, N_add_nan+N_ann_all)\n",
    "addon_nan_feature = get_addon_data_ft(features_ft_sub, mdl_df_n, N_add_nan, 0, True)\n",
    "\n",
    "\n",
    "# reading pmid list, for CV\n",
    "_f = os.path.join(old_data_dir, 'PMID_lst')\n",
    "with open(_f, 'rb' ) as fp:\n",
    "    PMID_lst = pickle.load(fp)\n",
    "#len(PMID_lst), PMID_lst[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw_data/ft/T3/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dev():\n",
    "    IS_DEBUG = False\n",
    "    IS_RUN_1 = False\n",
    "    IS_ADD_FT_N = False\n",
    "    IS_ADD_FT_P = False\n",
    "\n",
    "\n",
    "    _start_time = time.time()\n",
    "\n",
    "\n",
    "    # testing ann\n",
    "    tar_feature = n_features_ft_sub\n",
    "    all_feature = features_ft_sub\n",
    "\n",
    "    add_feature_n = addon_nan_feature if IS_ADD_FT_N else None\n",
    "    add_feature_p = addon_olp3p_feature if IS_ADD_FT_P else None\n",
    "\n",
    "    run_cls_df_l = []\n",
    "    adj_arr = []\n",
    "    ori_arr = []\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "    for idx, (train_pmid_lst_i, test_pmid_lst_i) in enumerate(kf.split(PMID_lst)):\n",
    "    #     if idx < 3:\n",
    "    #         continue\n",
    "        print(\"CV run %s/5\" % (idx+1))\n",
    "    #     print(len(train_pmid_lst_i), train_pmid_lst_i[:3])\n",
    "        #print(len(test_pmid_lst_i), test_pmid_lst_i[:3])\n",
    "\n",
    "        train_pmid_lst = [PMID_lst[i] for i in train_pmid_lst_i]\n",
    "        test_pmid_lst = [PMID_lst[i] for i in test_pmid_lst_i]\n",
    "\n",
    "        train_index = tar_feature[1][(tar_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "        test_index = tar_feature[1][(tar_feature[1].pmid.isin(test_pmid_lst))].index\n",
    "\n",
    "        _train_dev_ds, _test_ds = convert_features_to_dataset_cv(tar_feature, train_index), \\\n",
    "                                convert_features_to_dataset_cv(tar_feature, test_index)\n",
    "\n",
    "        if IS_ADD_FT_N:\n",
    "            #add data in training dataset\n",
    "            add_feature = add_feature_n\n",
    "            add_index = add_feature[1][(add_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "            #print('using add feature: %s' % len(add_index))\n",
    "            add_dataset = convert_features_to_dataset_cv(add_feature, add_index)\n",
    "            _train_dev_ds = torch.utils.data.ConcatDataset([_train_dev_ds, add_dataset]) \n",
    "\n",
    "        if IS_ADD_FT_P:\n",
    "            #add data in training dataset\n",
    "            add_feature = add_feature_p\n",
    "            add_index = add_feature[1][(add_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "            #print('using add feature: %s' % len(add_index))\n",
    "            add_dataset = convert_features_to_dataset_cv(add_feature, add_index)\n",
    "            _train_dev_ds = torch.utils.data.ConcatDataset([_train_dev_ds, add_dataset]) \n",
    "\n",
    "        train_dataloader_ds = DataLoader(_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "        print('training dataset size {}'.format(len(_train_dev_ds)))\n",
    "\n",
    "\n",
    "    #     train_dataloader_ds = DataLoader(_train_dev_ds, batch_size=args.batch_size)\n",
    "        test_dataloader_ds = DataLoader(_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "        train_dt, dev_dt, test_dt = train_dataloader_ds, None, test_dataloader_ds\n",
    "\n",
    "        n_test_index = all_feature[1][(all_feature[1].pmid.isin(test_pmid_lst))].index\n",
    "        n_test_ds = convert_features_to_dataset_cv(all_feature, n_test_index)\n",
    "        n_test_dt = DataLoader(n_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "        # loading pretrained abstract model\n",
    "        #args.modle_dir = '../models/'\n",
    "        #model_name_prefix = 'Bst_abs_10'\n",
    "\n",
    "        #print('loading based model from', model_name_prefix)\n",
    "        #checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "        #config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "        checkpoint_f = os.path.join(args.pretrained_model_p + \".ckp\")\n",
    "        config_save_f = os.path.join(args.pretrained_model_p + \".cf\")\n",
    "\n",
    "        config = torch.load(config_save_f)\n",
    "        model, _, _ = load_checkpoint(config, checkpoint_f)\n",
    "\n",
    "\n",
    "        # model config\n",
    "        args.EB_dp = 0.3\n",
    "        args.FC_dp = 0.1\n",
    "\n",
    "        # training config\n",
    "        args.use_new_loss = False\n",
    "        args.use_cls_loss = False\n",
    "\n",
    "\n",
    "        #args.epochs = 10\n",
    "        #args.epochs = 1\n",
    "        args.warmup_epoch = 0\n",
    "        args.patience_epoch = 3\n",
    "\n",
    "        args.learning_rate = args.lr\n",
    "        #args.learning_rate = 8e-4\n",
    "        args.weight_decay = 8e-6\n",
    "        args.l2_weight_decay = 5e-5\n",
    "        args.max_grad_norm = 2.0\n",
    "        args.lr_reduce_factor = 0.5\n",
    "        args.lr_cooldown = 0\n",
    "        args.threshold = .5\n",
    "        args.adam_epsilon = 1e-8\n",
    "        args.use_loss_sh = False\n",
    "#         args.is_iterare_info = not IS_DEBUG\n",
    "        args.is_iterare_info = True\n",
    "\n",
    "\n",
    "        args.device = device\n",
    "\n",
    "\n",
    "        config = update_model_config(args, config, False)\n",
    "\n",
    "        model.update_model_config(config)\n",
    "\n",
    "#         if torch.cuda.device_count() > 1:\n",
    "#             print(\"use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#             model = nn.DataParallel(model)\n",
    "\n",
    "        model.to(device)\n",
    "        optimizer, scheduler = init_model_optimizer(model, args)\n",
    "        scheduler.step(0)\n",
    "\n",
    "        is_save_new_train = False\n",
    "#         args.modle_dir = '../models'\n",
    "        args.modle_dir = args.raw_data_dir\n",
    "        model_name_prefix = 'build_ft_base'\n",
    "\n",
    "        args.checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "        args.config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "        if is_save_new_train:\n",
    "            torch.save(config, args.config_save_f)\n",
    "\n",
    "        if IS_DEBUG:\n",
    "            print(\"testing------\")\n",
    "\n",
    "            pred_l, tru_l, S, pred_o = eval(model, n_test_dt, args, 'test')\n",
    "            y_info = all_feature[1].iloc[n_test_index].copy()\n",
    "            y_info['pred'] = pred_l\n",
    "            _rst = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df[(abs_s_df.pmid.isin(test_pmid_lst))], _abs_s_df, abs_s_df_ner)\n",
    "            Tar_l = list(_rst) + list(S[1:4]) \n",
    "            print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), Tar_l)))\n",
    "            ori_arr.append(Tar_l[:])\n",
    "\n",
    "        print(\"training------\")\n",
    "\n",
    "        _, _, S_ori, _ = train(model, optimizer, scheduler, train_dt, dev_dt, args, test_dt, is_save_new_train)\n",
    "\n",
    "        pred_l, tru_l, S, pred_o = eval(model, n_test_dt, args, 'test')\n",
    "        y_info = all_feature[1].iloc[n_test_index].copy()\n",
    "        y_info['pred'] = pred_l\n",
    "        y_info['prob'] = pred_o\n",
    "        _rst = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df[(abs_s_df.pmid.isin(test_pmid_lst))], _abs_s_df, abs_s_df_ner)\n",
    "        Tar_l = list(_rst) + list(S_ori[1:4])\n",
    "        print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), Tar_l)))\n",
    "\n",
    "        run_cls_df_l.append(y_info)\n",
    "        adj_arr.append(Tar_l[:])\n",
    "\n",
    "        model = None\n",
    "        if not args.no_cuda:\n",
    "            free_cuda()\n",
    "        print(\"*********\")\n",
    "\n",
    "        if IS_RUN_1:\n",
    "            break\n",
    "\n",
    "    print('CV end')\n",
    "\n",
    "\n",
    "    if IS_DEBUG:\n",
    "        ori_arr = np.array(ori_arr)\n",
    "\n",
    "        for r in ori_arr:\n",
    "            print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), r)))\n",
    "\n",
    "        print('---')\n",
    "\n",
    "    adj_arr = np.array(adj_arr)\n",
    "\n",
    "    for r in adj_arr:\n",
    "        print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), r)))\n",
    "\n",
    "\n",
    "    print('CV mean, positive #, precision, precision[-], recall, recall[-]')\n",
    "    print(list(np.mean(adj_arr, axis=0)))\n",
    "    print('mlt train ended')\n",
    "\n",
    "\n",
    "    mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "\n",
    "    _rst = evaluate_rst_all_info_err(mer_pd_rst, 'pred', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "    print('CV merge, positive #, precision, precision[-], recall, recall[-]')\n",
    "    print(','.join(list(map(str, _rst))))\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - _start_time))   \n",
    "    return _rst, mer_pd_rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****\n",
      "begin training 10X RENET2 models 1\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00435, 0.5889, 0.9551, 0.7286, 0.6435]\n",
      "e_2 * test rst: [0.00423, 0.6683, 0.8526, 0.7493, 0.6526]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00432, 0.6135, 0.8141, 0.6997, 0.6325]\n",
      "e_4 * test rst: [0.00443, 0.6327, 0.9167, 0.7487, 0.6537]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00462, 0.6226, 0.6346, 0.6286, 0.6319]\n",
      "e_6 * test rst: [0.00456, 0.6620, 0.6026, 0.6309, 0.6447]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00444, 0.6364, 0.8077, 0.7119, 0.6522]\n",
      "e_8 * test rst: [0.00449, 0.6349, 0.7692, 0.6957, 0.6307]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00472, 0.6039, 0.5962, 0.6000, 0.6266]\n",
      "e_10 * test rst: [0.00470, 0.6062, 0.6218, 0.6139, 0.6271]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 192.20 s\n",
      "205,0.59024,0.68831,0.61783,0.10433,0.60625,0.62179,0.61392\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00410, 0.6891, 0.7268, 0.7074, 0.6252]\n",
      "e_2 * test rst: [0.00398, 0.6519, 0.9617, 0.7770, 0.6605]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00403, 0.7102, 0.6831, 0.6964, 0.6637]\n",
      "e_4 * test rst: [0.00369, 0.7308, 0.8306, 0.7775, 0.7176]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00392, 0.7222, 0.7814, 0.7507, 0.6987]\n",
      "e_6 * test rst: [0.00412, 0.7216, 0.7650, 0.7427, 0.6852]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00417, 0.7240, 0.7596, 0.7413, 0.6783]\n",
      "e_8 * test rst: [0.00430, 0.7302, 0.7541, 0.7419, 0.6761]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00432, 0.7326, 0.7486, 0.7405, 0.6747]\n",
      "e_10 * test rst: [0.00438, 0.7249, 0.7486, 0.7366, 0.6719]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 193.30 s\n",
      "234,0.68644,0.76959,0.73656,0.16539,0.72487,0.74863,0.73656\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00406, 0.6644, 0.7462, 0.7029, 0.6569]\n",
      "e_2 * test rst: [0.00376, 0.7143, 0.6538, 0.6827, 0.7123]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00351, 0.6534, 0.8846, 0.7516, 0.7508]\n",
      "e_4 * test rst: [0.00354, 0.7231, 0.7231, 0.7231, 0.7482]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00354, 0.7000, 0.8077, 0.7500, 0.7577]\n",
      "e_6 * test rst: [0.00362, 0.6815, 0.8231, 0.7456, 0.7520]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00360, 0.7132, 0.7462, 0.7293, 0.7569]\n",
      "e_8 * test rst: [0.00368, 0.7154, 0.7154, 0.7154, 0.7544]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00369, 0.6939, 0.7846, 0.7365, 0.7546]\n",
      "e_10 * test rst: [0.00375, 0.7176, 0.7231, 0.7203, 0.7511]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 205.58 s\n",
      "156,0.69231,0.79104,0.72308,0.11705,0.71756,0.72308,0.72031\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00416, 0.7087, 0.7342, 0.7212, 0.6833]\n",
      "e_2 * test rst: [0.00415, 0.6346, 0.8919, 0.7416, 0.7064]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00415, 0.6317, 0.8964, 0.7412, 0.7359]\n",
      "e_4 * test rst: [0.00430, 0.6258, 0.8964, 0.7370, 0.7343]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00400, 0.7004, 0.7793, 0.7377, 0.7408]\n",
      "e_6 * test rst: [0.00401, 0.6953, 0.8018, 0.7448, 0.7469]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00396, 0.7040, 0.7928, 0.7458, 0.7492]\n",
      "e_8 * test rst: [0.00392, 0.7073, 0.7838, 0.7436, 0.7493]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00396, 0.7040, 0.7928, 0.7458, 0.7498]\n",
      "e_10 * test rst: [0.00395, 0.7097, 0.7928, 0.7489, 0.7505]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 190.73 s\n",
      "282,0.69965,0.76518,0.78924,0.20483,0.70968,0.79279,0.74894\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00479, 0.6313, 0.6378, 0.6345, 0.5493]\n",
      "e_2 * test rst: [0.00526, 0.6368, 0.6173, 0.6269, 0.5852]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00490, 0.6357, 0.8367, 0.7225, 0.5854]\n",
      "e_4 * test rst: [0.00484, 0.6473, 0.9082, 0.7558, 0.5953]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00495, 0.6616, 0.6684, 0.6650, 0.6180]\n",
      "e_6 * test rst: [0.00481, 0.6795, 0.8112, 0.7395, 0.6327]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00484, 0.6532, 0.8265, 0.7297, 0.6341]\n",
      "e_8 * test rst: [0.00482, 0.6844, 0.7857, 0.7316, 0.6487]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00483, 0.6752, 0.8061, 0.7349, 0.6469]\n",
      "e_10 * test rst: [0.00483, 0.6830, 0.7806, 0.7286, 0.6555]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 196.98 s\n",
      "249,0.69200,0.79888,0.77273,0.14885,0.68304,0.78061,0.72857\n",
      "*********\n",
      "CV end\n",
      "205.0,0.59024,0.68831,0.61783,0.10433,0.60625,0.62179,0.61392\n",
      "234.0,0.68644,0.76959,0.73656,0.16539,0.72487,0.74863,0.73656\n",
      "156.0,0.69231,0.79104,0.72308,0.11705,0.71756,0.72308,0.72031\n",
      "282.0,0.69965,0.76518,0.78924,0.20483,0.70968,0.79279,0.74894\n",
      "249.0,0.69200,0.79888,0.77273,0.14885,0.68304,0.78061,0.72857\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[225.2, 0.6721277831644719, 0.7626013171372753, 0.7278870797310072, 0.1480916030534351, 0.688277622083335, 0.7333821424687936, 0.7096594605224029]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1126,0.6743362831858407,0.7636949516648764,0.7348993288590604,0.7404580152671756\n",
      "--- 1061.1140775680542 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_01.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 2\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00460, 0.5126, 0.3910, 0.4436, 0.5219]\n",
      "e_2 * test rst: [0.00422, 0.6720, 0.8141, 0.7362, 0.6540]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00422, 0.6617, 0.8526, 0.7451, 0.6459]\n",
      "e_4 * test rst: [0.00484, 0.6420, 0.6667, 0.6541, 0.6254]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00427, 0.6435, 0.8910, 0.7473, 0.6775]\n",
      "e_6 * test rst: [0.00498, 0.6481, 0.6731, 0.6604, 0.6200]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00484, 0.6310, 0.6795, 0.6543, 0.6251]\n",
      "e_8 * test rst: [0.00501, 0.6531, 0.6154, 0.6337, 0.6177]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00473, 0.6374, 0.6987, 0.6667, 0.6326]\n",
      "e_10 * test rst: [0.00488, 0.6369, 0.6859, 0.6605, 0.6275]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 203.13 s\n",
      "190,0.66316,0.80882,0.68153,0.11578,0.63690,0.68590,0.66049\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00393, 0.7014, 0.8470, 0.7673, 0.6272]\n",
      "e_2 * test rst: [0.00384, 0.7245, 0.7760, 0.7493, 0.6742]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00395, 0.7231, 0.7705, 0.7460, 0.6455]\n",
      "e_4 * test rst: [0.00387, 0.7295, 0.8251, 0.7744, 0.6763]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00395, 0.7033, 0.8033, 0.7500, 0.6400]\n",
      "e_6 * test rst: [0.00392, 0.7123, 0.8251, 0.7646, 0.6514]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00395, 0.7216, 0.7650, 0.7427, 0.6667]\n",
      "e_8 * test rst: [0.00422, 0.7289, 0.6612, 0.6934, 0.6680]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00403, 0.7231, 0.7705, 0.7460, 0.6598]\n",
      "e_10 * test rst: [0.00418, 0.7198, 0.7158, 0.7178, 0.6548]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.95 s\n",
      "241,0.64198,0.70982,0.70430,0.15522,0.71978,0.71585,0.71781\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00405, 0.5722, 0.8538, 0.6852, 0.6502]\n",
      "e_2 * test rst: [0.00407, 0.7019, 0.5615, 0.6239, 0.6986]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00385, 0.6667, 0.8308, 0.7397, 0.7045]\n",
      "e_4 * test rst: [0.00392, 0.6533, 0.7538, 0.7000, 0.7086]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00395, 0.6471, 0.8462, 0.7333, 0.7293]\n",
      "e_6 * test rst: [0.00398, 0.6424, 0.8154, 0.7186, 0.7152]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00408, 0.6522, 0.6923, 0.6716, 0.7086]\n",
      "e_8 * test rst: [0.00405, 0.6531, 0.7385, 0.6931, 0.7201]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00409, 0.6596, 0.7154, 0.6863, 0.7178]\n",
      "e_10 * test rst: [0.00411, 0.6458, 0.7154, 0.6788, 0.7161]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.82 s\n",
      "174,0.61494,0.68831,0.71538,0.11705,0.64583,0.71538,0.67883\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00420, 0.6103, 0.9595, 0.7461, 0.6666]\n",
      "e_2 * test rst: [0.00395, 0.6630, 0.8243, 0.7349, 0.7270]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00382, 0.6853, 0.7748, 0.7273, 0.7441]\n",
      "e_4 * test rst: [0.00389, 0.7237, 0.7432, 0.7333, 0.7388]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00396, 0.7476, 0.7072, 0.7269, 0.7473]\n",
      "e_6 * test rst: [0.00405, 0.6769, 0.7928, 0.7303, 0.7395]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00411, 0.7257, 0.7387, 0.7321, 0.7402]\n",
      "e_8 * test rst: [0.00412, 0.7571, 0.7162, 0.7361, 0.7413]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00419, 0.7633, 0.7117, 0.7366, 0.7402]\n",
      "e_10 * test rst: [0.00421, 0.7670, 0.7117, 0.7383, 0.7413]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 193.93 s\n",
      "244,0.75918,0.82949,0.70852,0.18575,0.76699,0.71171,0.73832\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00462, 0.6207, 0.9184, 0.7407, 0.5341]\n",
      "e_2 * test rst: [0.00497, 0.6163, 0.7704, 0.6848, 0.5711]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00582, 0.7273, 0.4898, 0.5854, 0.6217]\n",
      "e_4 * test rst: [0.00470, 0.6798, 0.7908, 0.7311, 0.6621]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00494, 0.7438, 0.6071, 0.6685, 0.6758]\n",
      "e_6 * test rst: [0.00469, 0.6927, 0.7245, 0.7082, 0.6656]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00474, 0.7173, 0.6990, 0.7080, 0.6681]\n",
      "e_8 * test rst: [0.00456, 0.6867, 0.8724, 0.7685, 0.6872]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00455, 0.7222, 0.7296, 0.7259, 0.6872]\n",
      "e_10 * test rst: [0.00448, 0.7116, 0.7806, 0.7445, 0.6936]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 198.31 s\n",
      "241,0.70661,0.80108,0.77273,0.15903,0.71163,0.78061,0.74453\n",
      "*********\n",
      "CV end\n",
      "190.0,0.66316,0.80882,0.68153,0.11578,0.63690,0.68590,0.66049\n",
      "241.0,0.64198,0.70982,0.70430,0.15522,0.71978,0.71585,0.71781\n",
      "174.0,0.61494,0.68831,0.71538,0.11705,0.64583,0.71538,0.67883\n",
      "244.0,0.75918,0.82949,0.70852,0.18575,0.76699,0.71171,0.73832\n",
      "241.0,0.70661,0.80108,0.77273,0.15903,0.71163,0.78061,0.74453\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[218.0, 0.6771741951663544, 0.7675050005339379, 0.716492361034657, 0.14656488549618318, 0.6962273026514391, 0.7218906004854483, 0.7079954935162998]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1090,0.6819012797074955,0.7677208287895311,0.7181208053691275,0.732824427480916\n",
      "--- 1084.1011416912079 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_02.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 3\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00440, 0.6061, 0.8974, 0.7235, 0.5951]\n",
      "e_2 * test rst: [0.00445, 0.6163, 0.6795, 0.6463, 0.6053]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00433, 0.6604, 0.8974, 0.7609, 0.6174]\n",
      "e_4 * test rst: [0.00506, 0.5878, 0.4936, 0.5366, 0.5864]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00471, 0.6262, 0.8269, 0.7127, 0.5842]\n",
      "e_6 * test rst: [0.00444, 0.6588, 0.8910, 0.7575, 0.6199]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00463, 0.6425, 0.7372, 0.6866, 0.6058]\n",
      "e_8 * test rst: [0.00469, 0.6573, 0.7500, 0.7006, 0.6130]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00465, 0.6630, 0.7692, 0.7122, 0.6181]\n",
      "e_10 * test rst: [0.00473, 0.6610, 0.7500, 0.7027, 0.6111]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 203.16 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300,0.60333,0.67220,0.74522,0.12468,0.66102,0.75000,0.70270\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00388, 0.6844, 0.9126, 0.7822, 0.6616]\n",
      "e_2 * test rst: [0.00447, 0.7582, 0.6339, 0.6905, 0.6830]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00350, 0.7461, 0.7869, 0.7660, 0.7412]\n",
      "e_4 * test rst: [0.00382, 0.7687, 0.6175, 0.6848, 0.7302]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00413, 0.7971, 0.6011, 0.6854, 0.7321]\n",
      "e_6 * test rst: [0.00393, 0.7654, 0.7486, 0.7569, 0.7278]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00400, 0.7194, 0.7705, 0.7441, 0.7126]\n",
      "e_8 * test rst: [0.00396, 0.7245, 0.7760, 0.7493, 0.7290]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00402, 0.7172, 0.7760, 0.7454, 0.7157]\n",
      "e_10 * test rst: [0.00409, 0.7382, 0.7705, 0.7540, 0.7281]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.92 s\n",
      "214,0.72222,0.80402,0.75806,0.16921,0.73822,0.77049,0.75401\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00404, 0.5804, 1.0000, 0.7345, 0.6878]\n",
      "e_2 * test rst: [0.00357, 0.7250, 0.6692, 0.6960, 0.7475]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00333, 0.7426, 0.7769, 0.7594, 0.7713]\n",
      "e_4 * test rst: [0.00333, 0.7185, 0.7462, 0.7321, 0.7796]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00346, 0.7438, 0.6923, 0.7171, 0.7828]\n",
      "e_6 * test rst: [0.00350, 0.7344, 0.7231, 0.7287, 0.7748]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00336, 0.7153, 0.7538, 0.7341, 0.7819]\n",
      "e_8 * test rst: [0.00346, 0.7440, 0.7154, 0.7294, 0.7833]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00351, 0.7438, 0.6923, 0.7171, 0.7857]\n",
      "e_10 * test rst: [0.00351, 0.7460, 0.7231, 0.7344, 0.7803]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.21 s\n",
      "187,0.69519,0.76048,0.72308,0.11578,0.74603,0.72308,0.73437\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00421, 0.6039, 0.9820, 0.7479, 0.6602]\n",
      "e_2 * test rst: [0.00371, 0.7102, 0.7838, 0.7452, 0.7377]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00376, 0.7166, 0.7973, 0.7548, 0.7387]\n",
      "e_4 * test rst: [0.00375, 0.7826, 0.7297, 0.7552, 0.7526]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00358, 0.7783, 0.7432, 0.7604, 0.7709]\n",
      "e_6 * test rst: [0.00375, 0.7810, 0.7387, 0.7593, 0.7635]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00368, 0.7312, 0.8333, 0.7789, 0.7657]\n",
      "e_8 * test rst: [0.00371, 0.7565, 0.7838, 0.7699, 0.7565]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00376, 0.7788, 0.7613, 0.7699, 0.7594]\n",
      "e_10 * test rst: [0.00376, 0.7658, 0.7658, 0.7658, 0.7574]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 191.42 s\n",
      "281,0.69504,0.76518,0.76233,0.19975,0.76577,0.76577,0.76577\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00461, 0.6267, 0.9592, 0.7581, 0.5364]\n",
      "e_2 * test rst: [0.00468, 0.6255, 0.8776, 0.7304, 0.6126]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00474, 0.6977, 0.6122, 0.6522, 0.6543]\n",
      "e_4 * test rst: [0.00470, 0.7516, 0.5867, 0.6590, 0.6897]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00427, 0.6970, 0.8214, 0.7541, 0.6938]\n",
      "e_6 * test rst: [0.00430, 0.7295, 0.7704, 0.7494, 0.7036]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00430, 0.6949, 0.8367, 0.7593, 0.6989]\n",
      "e_8 * test rst: [0.00448, 0.7287, 0.6990, 0.7135, 0.6945]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00427, 0.7087, 0.8316, 0.7653, 0.7030]\n",
      "e_10 * test rst: [0.00433, 0.7212, 0.7653, 0.7426, 0.7014]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 196.88 s\n",
      "219,0.72273,0.85065,0.75758,0.14758,0.72115,0.76531,0.74257\n",
      "*********\n",
      "CV end\n",
      "300.0,0.60333,0.67220,0.74522,0.12468,0.66102,0.75000,0.70270\n",
      "214.0,0.72222,0.80402,0.75806,0.16921,0.73822,0.77049,0.75401\n",
      "187.0,0.69519,0.76048,0.72308,0.11578,0.74603,0.72308,0.73437\n",
      "281.0,0.69504,0.76518,0.76233,0.19975,0.76577,0.76577,0.76577\n",
      "219.0,0.72273,0.85065,0.75758,0.14758,0.72115,0.76531,0.74257\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[240.2, 0.6877010910102274, 0.770505969885466, 0.7492543930566082, 0.15139949109414758, 0.7264376404783717, 0.7549281229140714, 0.7398856842162752]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1201,0.6821576763485477,0.7628968253968254,0.7516778523489933,0.7569974554707379\n",
      "--- 1079.0331308841705 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_03.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 4\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00444, 0.6237, 0.7436, 0.6784, 0.5989]\n",
      "e_2 * test rst: [0.00460, 0.6091, 0.8590, 0.7128, 0.6141]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00416, 0.6545, 0.8013, 0.7205, 0.6648]\n",
      "e_4 * test rst: [0.00423, 0.6619, 0.8910, 0.7596, 0.6519]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00447, 0.6429, 0.7500, 0.6923, 0.6642]\n",
      "e_6 * test rst: [0.00429, 0.6541, 0.7756, 0.7097, 0.6665]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00434, 0.6648, 0.7756, 0.7160, 0.6624]\n",
      "e_8 * test rst: [0.00427, 0.6702, 0.8205, 0.7378, 0.6606]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00442, 0.6571, 0.7372, 0.6949, 0.6576]\n",
      "e_10 * test rst: [0.00439, 0.6556, 0.7564, 0.7024, 0.6542]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.89 s\n",
      "245,0.61633,0.70370,0.75159,0.12723,0.65556,0.75641,0.70238\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00390, 0.6475, 0.9235, 0.7613, 0.6391]\n",
      "e_2 * test rst: [0.00387, 0.7487, 0.8142, 0.7801, 0.6705]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00420, 0.7091, 0.6393, 0.6724, 0.6535]\n",
      "e_4 * test rst: [0.00397, 0.6770, 0.8361, 0.7482, 0.6580]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00378, 0.7170, 0.8306, 0.7696, 0.6804]\n",
      "e_6 * test rst: [0.00398, 0.7044, 0.7814, 0.7409, 0.6736]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00401, 0.7310, 0.6831, 0.7062, 0.6895]\n",
      "e_8 * test rst: [0.00412, 0.7317, 0.6557, 0.6916, 0.6764]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00411, 0.7168, 0.6776, 0.6966, 0.6781]\n",
      "e_10 * test rst: [0.00423, 0.7308, 0.6230, 0.6726, 0.6758]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 201.78 s\n",
      "202,0.68627,0.78919,0.61290,0.13740,0.73077,0.62295,0.67257\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00434, 0.6829, 0.4308, 0.5283, 0.5455]\n",
      "e_2 * test rst: [0.00414, 0.5915, 0.7462, 0.6599, 0.6257]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00412, 0.6338, 0.6923, 0.6618, 0.6583]\n",
      "e_4 * test rst: [0.00429, 0.6458, 0.7154, 0.6788, 0.6196]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00391, 0.6761, 0.7385, 0.7059, 0.7048]\n",
      "e_6 * test rst: [0.00415, 0.6591, 0.6692, 0.6641, 0.6732]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00413, 0.6667, 0.7692, 0.7143, 0.6746]\n",
      "e_8 * test rst: [0.00393, 0.6871, 0.7769, 0.7292, 0.7140]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00399, 0.6934, 0.7308, 0.7116, 0.7054]\n",
      "e_10 * test rst: [0.00406, 0.6779, 0.7769, 0.7240, 0.6923]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.05 s\n",
      "238,0.57983,0.62500,0.77692,0.12468,0.67785,0.77692,0.72401\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00426, 0.6536, 0.8243, 0.7291, 0.6627]\n",
      "e_2 * test rst: [0.00386, 0.7022, 0.7117, 0.7069, 0.7335]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00369, 0.7015, 0.8468, 0.7673, 0.7614]\n",
      "e_4 * test rst: [0.00375, 0.6799, 0.8514, 0.7560, 0.7698]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00357, 0.7171, 0.8333, 0.7708, 0.7774]\n",
      "e_6 * test rst: [0.00371, 0.7295, 0.8018, 0.7639, 0.7748]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00362, 0.7166, 0.7973, 0.7548, 0.7723]\n",
      "e_8 * test rst: [0.00366, 0.7589, 0.7658, 0.7623, 0.7723]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00380, 0.7061, 0.8333, 0.7645, 0.7701]\n",
      "e_10 * test rst: [0.00369, 0.7344, 0.7973, 0.7646, 0.7760]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 190.52 s\n",
      "300,0.68439,0.75094,0.79372,0.20865,0.73444,0.79730,0.76458\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00457, 0.6280, 0.9388, 0.7526, 0.5470]\n",
      "e_2 * test rst: [0.00460, 0.6272, 0.9184, 0.7453, 0.5689]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00455, 0.6442, 0.8776, 0.7430, 0.6684]\n",
      "e_4 * test rst: [0.00474, 0.6389, 0.9388, 0.7603, 0.6638]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00438, 0.6679, 0.9133, 0.7716, 0.6840]\n",
      "e_6 * test rst: [0.00469, 0.6424, 0.9439, 0.7645, 0.6710]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00436, 0.6706, 0.8724, 0.7583, 0.6840]\n",
      "e_8 * test rst: [0.00461, 0.6502, 0.9388, 0.7683, 0.6726]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00440, 0.6680, 0.8724, 0.7566, 0.6838]\n",
      "e_10 * test rst: [0.00445, 0.6667, 0.9082, 0.7689, 0.6825]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 199.07 s\n",
      "329,0.64545,0.73171,0.89899,0.17684,0.66667,0.90816,0.76890\n",
      "*********\n",
      "CV end\n",
      "245.0,0.61633,0.70370,0.75159,0.12723,0.65556,0.75641,0.70238\n",
      "202.0,0.68627,0.78919,0.61290,0.13740,0.73077,0.62295,0.67257\n",
      "238.0,0.57983,0.62500,0.77692,0.12468,0.67785,0.77692,0.72401\n",
      "300.0,0.68439,0.75094,0.79372,0.20865,0.73444,0.79730,0.76458\n",
      "329.0,0.64545,0.73171,0.89899,0.17684,0.66667,0.90816,0.76890\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[262.8, 0.6424545801407244, 0.7201087212384957, 0.7668261063002992, 0.1549618320610687, 0.6930567272019276, 0.7723489431217769, 0.7264877965588372]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1314,0.6433990895295902,0.7202543142597638,0.7695749440715883,0.7748091603053435\n",
      "--- 1079.5332736968994 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_04.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 5\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00431, 0.5992, 0.9295, 0.7286, 0.6158]\n",
      "e_2 * test rst: [0.00460, 0.6307, 0.7115, 0.6687, 0.6230]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00436, 0.6364, 0.8974, 0.7447, 0.6612]\n",
      "e_4 * test rst: [0.00432, 0.6332, 0.9295, 0.7532, 0.6883]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00425, 0.6770, 0.6987, 0.6877, 0.6722]\n",
      "e_6 * test rst: [0.00448, 0.6591, 0.7436, 0.6988, 0.6678]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00448, 0.6436, 0.8333, 0.7263, 0.6551]\n",
      "e_8 * test rst: [0.00454, 0.6304, 0.7436, 0.6824, 0.6369]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00458, 0.6216, 0.7372, 0.6745, 0.6399]\n",
      "e_10 * test rst: [0.00460, 0.6310, 0.7564, 0.6880, 0.6384]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 205.96 s\n",
      "220,0.62727,0.74096,0.75159,0.13104,0.63102,0.75641,0.68805\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00400, 0.6888, 0.9071, 0.7830, 0.6274]\n",
      "e_2 * test rst: [0.00381, 0.7398, 0.7923, 0.7652, 0.6708]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00389, 0.7014, 0.8087, 0.7513, 0.6567]\n",
      "e_4 * test rst: [0.00443, 0.7234, 0.5574, 0.6296, 0.6541]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00398, 0.6991, 0.8251, 0.7569, 0.6589]\n",
      "e_6 * test rst: [0.00402, 0.7184, 0.6831, 0.7003, 0.6798]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00404, 0.7189, 0.7268, 0.7228, 0.6625]\n",
      "e_8 * test rst: [0.00396, 0.7113, 0.7541, 0.7321, 0.6688]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00393, 0.7150, 0.7814, 0.7467, 0.6732]\n",
      "e_10 * test rst: [0.00395, 0.7114, 0.7814, 0.7448, 0.6702]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.96 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268,0.67407,0.76132,0.76882,0.17048,0.71144,0.78142,0.74479\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00408, 0.6199, 0.8154, 0.7043, 0.6377]\n",
      "e_2 * test rst: [0.00384, 0.6733, 0.7769, 0.7214, 0.7041]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00390, 0.6280, 0.7923, 0.7007, 0.7093]\n",
      "e_4 * test rst: [0.00396, 0.6407, 0.8231, 0.7205, 0.7081]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00405, 0.6767, 0.6923, 0.6844, 0.7039]\n",
      "e_6 * test rst: [0.00392, 0.6500, 0.8000, 0.7172, 0.7150]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00400, 0.6573, 0.7231, 0.6886, 0.7180]\n",
      "e_8 * test rst: [0.00416, 0.6643, 0.7154, 0.6889, 0.7101]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00419, 0.6494, 0.7692, 0.7042, 0.7049]\n",
      "e_10 * test rst: [0.00420, 0.6573, 0.7231, 0.6886, 0.7016]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 209.02 s\n",
      "194,0.60825,0.68235,0.72308,0.11705,0.65734,0.72308,0.68864\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00428, 0.6571, 0.7252, 0.6895, 0.6564]\n",
      "e_2 * test rst: [0.00417, 0.7136, 0.6622, 0.6869, 0.6916]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00374, 0.7681, 0.7162, 0.7413, 0.7519]\n",
      "e_4 * test rst: [0.00371, 0.7172, 0.7883, 0.7511, 0.7642]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00385, 0.7178, 0.7793, 0.7473, 0.7744]\n",
      "e_6 * test rst: [0.00377, 0.7261, 0.7883, 0.7559, 0.7645]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00385, 0.7700, 0.7387, 0.7540, 0.7679]\n",
      "e_8 * test rst: [0.00391, 0.7664, 0.7387, 0.7523, 0.7625]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00390, 0.7389, 0.7523, 0.7455, 0.7673]\n",
      "e_10 * test rst: [0.00397, 0.7783, 0.7432, 0.7604, 0.7633]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 191.10 s\n",
      "241,0.74380,0.82381,0.73991,0.19338,0.77830,0.74324,0.76037\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00457, 0.6134, 0.9796, 0.7544, 0.5544]\n",
      "e_2 * test rst: [0.00464, 0.6241, 0.8724, 0.7277, 0.6240]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00455, 0.6638, 0.7959, 0.7239, 0.6482]\n",
      "e_4 * test rst: [0.00442, 0.6777, 0.8367, 0.7489, 0.6861]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00447, 0.6804, 0.7602, 0.7181, 0.6974]\n",
      "e_6 * test rst: [0.00444, 0.6847, 0.7755, 0.7273, 0.6981]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00432, 0.6962, 0.8418, 0.7621, 0.7168]\n",
      "e_8 * test rst: [0.00428, 0.7043, 0.8265, 0.7606, 0.7240]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00429, 0.6987, 0.8163, 0.7529, 0.7215]\n",
      "e_10 * test rst: [0.00431, 0.7035, 0.8112, 0.7536, 0.7209]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 196.46 s\n",
      "243,0.70492,0.83041,0.80303,0.15649,0.70354,0.81122,0.75355\n",
      "*********\n",
      "CV end\n",
      "220.0,0.62727,0.74096,0.75159,0.13104,0.63102,0.75641,0.68805\n",
      "268.0,0.67407,0.76132,0.76882,0.17048,0.71144,0.78142,0.74479\n",
      "194.0,0.60825,0.68235,0.72308,0.11705,0.65734,0.72308,0.68864\n",
      "241.0,0.74380,0.82381,0.73991,0.19338,0.77830,0.74324,0.76037\n",
      "243.0,0.70492,0.83041,0.80303,0.15649,0.70354,0.81122,0.75355\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[233.2, 0.6716627819413322, 0.7677705099121621, 0.7572854201995091, 0.15368956743002543, 0.6963286391988721, 0.7630751355107326, 0.7270812337011627]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1166,0.6752136752136753,0.7697916666666667,0.7595078299776287,0.7684478371501272\n",
      "--- 1082.4334518909454 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_05.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 6\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00429, 0.6471, 0.9167, 0.7586, 0.6276]\n",
      "e_2 * test rst: [0.00433, 0.6293, 0.9359, 0.7526, 0.6679]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00424, 0.6364, 0.9423, 0.7597, 0.6373]\n",
      "e_4 * test rst: [0.00457, 0.6364, 0.8974, 0.7447, 0.6749]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00429, 0.6441, 0.9167, 0.7566, 0.6261]\n",
      "e_6 * test rst: [0.00434, 0.6486, 0.9231, 0.7619, 0.6365]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00446, 0.6212, 0.7885, 0.6949, 0.6299]\n",
      "e_8 * test rst: [0.00487, 0.6090, 0.6090, 0.6090, 0.6030]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00456, 0.6305, 0.8205, 0.7131, 0.6334]\n",
      "e_10 * test rst: [0.00482, 0.5989, 0.6795, 0.6366, 0.6141]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 203.94 s\n",
      "195,0.61538,0.74286,0.67516,0.11450,0.59887,0.67949,0.63664\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00387, 0.7094, 0.9071, 0.7962, 0.6690]\n",
      "e_2 * test rst: [0.00358, 0.7488, 0.8470, 0.7949, 0.7125]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00340, 0.7246, 0.8197, 0.7692, 0.7553]\n",
      "e_4 * test rst: [0.00359, 0.7376, 0.8142, 0.7740, 0.7494]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00335, 0.7357, 0.9126, 0.8146, 0.7708]\n",
      "e_6 * test rst: [0.00394, 0.7452, 0.6393, 0.6882, 0.7282]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00388, 0.7251, 0.6776, 0.7006, 0.7218]\n",
      "e_8 * test rst: [0.00383, 0.7557, 0.7268, 0.7409, 0.7318]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00363, 0.7475, 0.8087, 0.7769, 0.7380]\n",
      "e_10 * test rst: [0.00379, 0.7459, 0.7541, 0.7500, 0.7293]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.77 s\n",
      "239,0.70124,0.77578,0.74194,0.16539,0.74595,0.75410,0.75000\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00389, 0.6828, 0.7615, 0.7200, 0.6793]\n",
      "e_2 * test rst: [0.00395, 0.7475, 0.5692, 0.6463, 0.7378]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00351, 0.7424, 0.7538, 0.7481, 0.7544]\n",
      "e_4 * test rst: [0.00350, 0.7442, 0.7385, 0.7413, 0.7667]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00359, 0.7407, 0.7692, 0.7547, 0.7603]\n",
      "e_6 * test rst: [0.00361, 0.7313, 0.7538, 0.7424, 0.7582]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00365, 0.7308, 0.7308, 0.7308, 0.7572]\n",
      "e_8 * test rst: [0.00365, 0.7259, 0.7538, 0.7396, 0.7624]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00373, 0.6918, 0.7769, 0.7319, 0.7575]\n",
      "e_10 * test rst: [0.00376, 0.7197, 0.7308, 0.7252, 0.7575]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.33 s\n",
      "199,0.66332,0.73743,0.73077,0.12087,0.71970,0.73077,0.72519\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00417, 0.6752, 0.7117, 0.6930, 0.6869]\n",
      "e_2 * test rst: [0.00392, 0.6630, 0.8243, 0.7349, 0.7256]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00372, 0.7061, 0.7793, 0.7409, 0.7399]\n",
      "e_4 * test rst: [0.00358, 0.7788, 0.7613, 0.7699, 0.7752]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00387, 0.7008, 0.8018, 0.7479, 0.7375]\n",
      "e_6 * test rst: [0.00378, 0.7477, 0.7477, 0.7477, 0.7599]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00386, 0.7393, 0.7793, 0.7588, 0.7539]\n",
      "e_8 * test rst: [0.00403, 0.7047, 0.8063, 0.7521, 0.7354]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00399, 0.7456, 0.7658, 0.7556, 0.7382]\n",
      "e_10 * test rst: [0.00397, 0.7243, 0.7928, 0.7570, 0.7445]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 191.46 s\n",
      "302,0.69307,0.76136,0.78924,0.20483,0.72428,0.79279,0.75699\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00460, 0.6283, 0.8622, 0.7269, 0.5302]\n",
      "e_2 * test rst: [0.00506, 0.6279, 0.8265, 0.7137, 0.5721]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00477, 0.6447, 0.7500, 0.6934, 0.6164]\n",
      "e_4 * test rst: [0.00463, 0.6471, 0.8980, 0.7521, 0.6522]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00456, 0.6604, 0.8929, 0.7592, 0.6622]\n",
      "e_6 * test rst: [0.00458, 0.6654, 0.8929, 0.7625, 0.6776]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00453, 0.6732, 0.8724, 0.7600, 0.6810]\n",
      "e_8 * test rst: [0.00473, 0.6531, 0.9031, 0.7580, 0.6770]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00458, 0.6731, 0.8929, 0.7675, 0.6841]\n",
      "e_10 * test rst: [0.00462, 0.6616, 0.8878, 0.7582, 0.6848]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 198.13 s\n",
      "320,0.64798,0.73029,0.87879,0.17303,0.66160,0.88776,0.75817\n",
      "*********\n",
      "CV end\n",
      "195.0,0.61538,0.74286,0.67516,0.11450,0.59887,0.67949,0.63664\n",
      "239.0,0.70124,0.77578,0.74194,0.16539,0.74595,0.75410,0.75000\n",
      "199.0,0.66332,0.73743,0.73077,0.12087,0.71970,0.73077,0.72519\n",
      "302.0,0.69307,0.76136,0.78924,0.20483,0.72428,0.79279,0.75699\n",
      "320.0,0.64798,0.73029,0.87879,0.17303,0.66160,0.88776,0.75817\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[251.0, 0.664198079277902, 0.7495452313226617, 0.7631778994516605, 0.15572519083969466, 0.6900779531411885, 0.7689805331491513, 0.7253973316567287]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1255,0.6664019062748213,0.7507163323782235,0.7706935123042505,0.7786259541984732\n",
      "--- 1082.8354675769806 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_06.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 7\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00441, 0.5968, 0.9487, 0.7327, 0.5952]\n",
      "e_2 * test rst: [0.00426, 0.6667, 0.8077, 0.7304, 0.6301]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00429, 0.6824, 0.6474, 0.6645, 0.6585]\n",
      "e_4 * test rst: [0.00461, 0.5980, 0.7628, 0.6704, 0.5910]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00421, 0.6754, 0.8269, 0.7435, 0.6597]\n",
      "e_6 * test rst: [0.00474, 0.5961, 0.7756, 0.6741, 0.5824]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00440, 0.6543, 0.6795, 0.6667, 0.6391]\n",
      "e_8 * test rst: [0.00454, 0.7068, 0.6026, 0.6505, 0.6450]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00443, 0.6903, 0.6859, 0.6881, 0.6567]\n",
      "e_10 * test rst: [0.00451, 0.6753, 0.6667, 0.6710, 0.6502]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.62 s\n",
      "224,0.66071,0.73333,0.66242,0.11196,0.67532,0.66667,0.67097\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00425, 0.6723, 0.6503, 0.6611, 0.5897]\n",
      "e_2 * test rst: [0.00383, 0.7320, 0.7760, 0.7533, 0.6613]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00420, 0.6849, 0.8197, 0.7463, 0.6254]\n",
      "e_4 * test rst: [0.00389, 0.6722, 0.8852, 0.7642, 0.6804]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00380, 0.6707, 0.9126, 0.7731, 0.7083]\n",
      "e_6 * test rst: [0.00414, 0.7047, 0.7432, 0.7234, 0.7112]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00382, 0.7042, 0.8197, 0.7576, 0.7086]\n",
      "e_8 * test rst: [0.00408, 0.7358, 0.7760, 0.7553, 0.7143]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00401, 0.7192, 0.7978, 0.7565, 0.7175]\n",
      "e_10 * test rst: [0.00417, 0.7296, 0.7814, 0.7546, 0.7133]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.36 s\n",
      "217,0.72146,0.81500,0.76882,0.17303,0.72959,0.78142,0.75462\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00404, 0.6383, 0.6923, 0.6642, 0.6412]\n",
      "e_2 * test rst: [0.00409, 0.6577, 0.5615, 0.6058, 0.6725]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00392, 0.6786, 0.7308, 0.7037, 0.7075]\n",
      "e_4 * test rst: [0.00387, 0.6387, 0.7615, 0.6947, 0.7172]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00397, 0.6822, 0.6769, 0.6795, 0.7143]\n",
      "e_6 * test rst: [0.00372, 0.6774, 0.8077, 0.7368, 0.7354]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00377, 0.6803, 0.7692, 0.7220, 0.7314]\n",
      "e_8 * test rst: [0.00378, 0.6667, 0.8000, 0.7273, 0.7313]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00374, 0.6755, 0.7846, 0.7260, 0.7331]\n",
      "e_10 * test rst: [0.00380, 0.6689, 0.7769, 0.7189, 0.7311]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.70 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285,0.54386,0.57692,0.77692,0.12214,0.66887,0.77692,0.71886\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00421, 0.6266, 0.8919, 0.7361, 0.6493]\n",
      "e_2 * test rst: [0.00372, 0.6715, 0.8378, 0.7455, 0.7464]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00379, 0.7500, 0.7162, 0.7327, 0.7399]\n",
      "e_4 * test rst: [0.00371, 0.7359, 0.7658, 0.7506, 0.7600]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00375, 0.7068, 0.7928, 0.7473, 0.7560]\n",
      "e_6 * test rst: [0.00382, 0.7412, 0.7613, 0.7511, 0.7573]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00375, 0.7339, 0.7703, 0.7516, 0.7586]\n",
      "e_8 * test rst: [0.00383, 0.7195, 0.7973, 0.7564, 0.7601]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00385, 0.7213, 0.7928, 0.7554, 0.7581]\n",
      "e_10 * test rst: [0.00380, 0.7593, 0.7387, 0.7489, 0.7604]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 189.12 s\n",
      "245,0.72764,0.80751,0.73543,0.19211,0.75926,0.73874,0.74886\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00447, 0.6276, 0.9286, 0.7490, 0.6060]\n",
      "e_2 * test rst: [0.00504, 0.6280, 0.9388, 0.7526, 0.6418]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00458, 0.6320, 0.8061, 0.7085, 0.6419]\n",
      "e_4 * test rst: [0.00487, 0.6330, 0.9592, 0.7627, 0.6345]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00507, 0.6491, 0.7551, 0.6981, 0.6466]\n",
      "e_6 * test rst: [0.00497, 0.6346, 0.8418, 0.7237, 0.6588]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00483, 0.6667, 0.6939, 0.6800, 0.6563]\n",
      "e_8 * test rst: [0.00495, 0.6636, 0.7245, 0.6927, 0.6546]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00495, 0.6573, 0.7143, 0.6846, 0.6542]\n",
      "e_10 * test rst: [0.00498, 0.6547, 0.7449, 0.6969, 0.6565]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 198.26 s\n",
      "291,0.61301,0.66516,0.73737,0.13740,0.65471,0.74490,0.69690\n",
      "*********\n",
      "CV end\n",
      "224.0,0.66071,0.73333,0.66242,0.11196,0.67532,0.66667,0.67097\n",
      "217.0,0.72146,0.81500,0.76882,0.17303,0.72959,0.78142,0.75462\n",
      "285.0,0.54386,0.57692,0.77692,0.12214,0.66887,0.77692,0.71886\n",
      "245.0,0.72764,0.80751,0.73543,0.19211,0.75926,0.73874,0.74886\n",
      "291.0,0.61301,0.66516,0.73737,0.13740,0.65471,0.74490,0.69690\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[252.4, 0.6533382194209211, 0.7195853036772673, 0.7361920819464209, 0.14732824427480914, 0.6975516927366863, 0.7417294413078956, 0.7180404376676347]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1262,0.6469194312796208,0.7113594040968343,0.7360178970917226,0.7366412213740458\n",
      "--- 1077.9008555412292 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_07.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 8\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00448, 0.6389, 0.8846, 0.7419, 0.6307]\n",
      "e_2 * test rst: [0.00479, 0.6341, 0.6667, 0.6500, 0.6099]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00452, 0.6322, 0.7051, 0.6667, 0.6100]\n",
      "e_4 * test rst: [0.00475, 0.6173, 0.7756, 0.6875, 0.6090]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00471, 0.6443, 0.8013, 0.7143, 0.6033]\n",
      "e_6 * test rst: [0.00481, 0.6119, 0.7885, 0.6891, 0.5935]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00494, 0.6196, 0.7308, 0.6706, 0.5929]\n",
      "e_8 * test rst: [0.00483, 0.6368, 0.7756, 0.6994, 0.6008]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00486, 0.6329, 0.8397, 0.7218, 0.6013]\n",
      "e_10 * test rst: [0.00485, 0.6480, 0.8141, 0.7216, 0.6013]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.57 s\n",
      "325,0.59385,0.65918,0.80892,0.13995,0.64796,0.81410,0.72159\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00414, 0.6826, 0.6230, 0.6514, 0.6134]\n",
      "e_2 * test rst: [0.00391, 0.7121, 0.7705, 0.7402, 0.6448]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00399, 0.7032, 0.8415, 0.7662, 0.6458]\n",
      "e_4 * test rst: [0.00411, 0.7073, 0.7923, 0.7474, 0.6321]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00403, 0.7115, 0.8087, 0.7570, 0.6607]\n",
      "e_6 * test rst: [0.00412, 0.7107, 0.7650, 0.7368, 0.6596]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00403, 0.7065, 0.7760, 0.7396, 0.6647]\n",
      "e_8 * test rst: [0.00411, 0.7056, 0.7596, 0.7316, 0.6646]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00406, 0.7136, 0.8033, 0.7558, 0.6717]\n",
      "e_10 * test rst: [0.00409, 0.7164, 0.7869, 0.7500, 0.6669]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 203.53 s\n",
      "244,0.68293,0.77027,0.77419,0.17176,0.71642,0.78689,0.75000\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00386, 0.6196, 0.7769, 0.6894, 0.7010]\n",
      "e_2 * test rst: [0.00389, 0.7071, 0.7615, 0.7333, 0.7054]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00370, 0.6933, 0.8692, 0.7713, 0.7146]\n",
      "e_4 * test rst: [0.00397, 0.7266, 0.7154, 0.7209, 0.7009]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00385, 0.7214, 0.7769, 0.7481, 0.7103]\n",
      "e_6 * test rst: [0.00382, 0.7203, 0.7923, 0.7546, 0.7152]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00382, 0.7279, 0.8231, 0.7726, 0.7179]\n",
      "e_8 * test rst: [0.00384, 0.7203, 0.7923, 0.7546, 0.7207]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00384, 0.7260, 0.8154, 0.7681, 0.7216]\n",
      "e_10 * test rst: [0.00382, 0.7230, 0.8231, 0.7698, 0.7232]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.39 s\n",
      "301,0.55150,0.57609,0.82308,0.12723,0.72297,0.82308,0.76978\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00417, 0.6006, 0.9279, 0.7292, 0.7021]\n",
      "e_2 * test rst: [0.00415, 0.6373, 0.8784, 0.7386, 0.7323]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00382, 0.6704, 0.8063, 0.7321, 0.7630]\n",
      "e_4 * test rst: [0.00413, 0.6527, 0.9144, 0.7617, 0.7598]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00385, 0.6567, 0.8874, 0.7548, 0.7747]\n",
      "e_6 * test rst: [0.00383, 0.7554, 0.7928, 0.7736, 0.7755]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00394, 0.7214, 0.8514, 0.7810, 0.7704]\n",
      "e_8 * test rst: [0.00388, 0.7679, 0.7748, 0.7713, 0.7746]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00393, 0.7555, 0.7793, 0.7672, 0.7767]\n",
      "e_10 * test rst: [0.00399, 0.7439, 0.8243, 0.7821, 0.7742]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 190.44 s\n",
      "307,0.68506,0.75000,0.82063,0.21247,0.74390,0.82432,0.78205\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00454, 0.6092, 0.7398, 0.6682, 0.5813]\n",
      "e_2 * test rst: [0.00471, 0.6504, 0.8163, 0.7240, 0.6246]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00479, 0.6423, 0.8520, 0.7325, 0.6131]\n",
      "e_4 * test rst: [0.00474, 0.6471, 0.7296, 0.6859, 0.6256]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00485, 0.6733, 0.6939, 0.6834, 0.6518]\n",
      "e_6 * test rst: [0.00494, 0.6667, 0.7347, 0.6990, 0.6400]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00488, 0.6847, 0.7092, 0.6967, 0.6557]\n",
      "e_8 * test rst: [0.00487, 0.7135, 0.6480, 0.6791, 0.6670]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00485, 0.6796, 0.7143, 0.6965, 0.6615]\n",
      "e_10 * test rst: [0.00492, 0.7039, 0.6429, 0.6720, 0.6700]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 198.59 s\n",
      "219,0.68182,0.76331,0.63636,0.12595,0.70391,0.64286,0.67200\n",
      "*********\n",
      "CV end\n",
      "325.0,0.59385,0.65918,0.80892,0.13995,0.64796,0.81410,0.72159\n",
      "244.0,0.68293,0.77027,0.77419,0.17176,0.71642,0.78689,0.75000\n",
      "301.0,0.55150,0.57609,0.82308,0.12723,0.72297,0.82308,0.76978\n",
      "307.0,0.68506,0.75000,0.82063,0.21247,0.74390,0.82432,0.78205\n",
      "219.0,0.68182,0.76331,0.63636,0.12595,0.70391,0.64286,0.67200\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[279.2, 0.6390302233217717, 0.7037693732444024, 0.7726358215940936, 0.155470737913486, 0.7070326241287468, 0.7782492400525187, 0.7390852727608124]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1396,0.6342857142857142,0.6955074875207987,0.7684563758389261,0.77735368956743\n",
      "--- 1077.8360781669617 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_08.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 9\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00427, 0.6368, 0.9103, 0.7493, 0.6507]\n",
      "e_2 * test rst: [0.00439, 0.6118, 0.9295, 0.7379, 0.5879]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00484, 0.6209, 0.6090, 0.6149, 0.5974]\n",
      "e_4 * test rst: [0.00442, 0.6442, 0.8590, 0.7363, 0.6156]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00450, 0.6443, 0.8013, 0.7143, 0.6225]\n",
      "e_6 * test rst: [0.00601, 0.6146, 0.3782, 0.4683, 0.5822]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00520, 0.6370, 0.5513, 0.5911, 0.5925]\n",
      "e_8 * test rst: [0.00560, 0.6283, 0.4551, 0.5279, 0.5965]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00551, 0.6484, 0.5321, 0.5845, 0.5901]\n",
      "e_10 * test rst: [0.00563, 0.6525, 0.4936, 0.5620, 0.5978]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 201.69 s\n",
      "159,0.68553,0.79339,0.49045,0.08142,0.65254,0.49359,0.56204\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00430, 0.6788, 0.5082, 0.5813, 0.5969]\n",
      "e_2 * test rst: [0.00458, 0.6498, 0.7705, 0.7050, 0.6065]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00443, 0.7211, 0.5792, 0.6424, 0.6194]\n",
      "e_4 * test rst: [0.00415, 0.6851, 0.6776, 0.6813, 0.6393]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00456, 0.7483, 0.6011, 0.6667, 0.6573]\n",
      "e_6 * test rst: [0.00412, 0.7251, 0.6776, 0.7006, 0.6599]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00416, 0.7216, 0.6940, 0.7075, 0.6703]\n",
      "e_8 * test rst: [0.00422, 0.7174, 0.7213, 0.7193, 0.6627]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00427, 0.7216, 0.6940, 0.7075, 0.6658]\n",
      "e_10 * test rst: [0.00424, 0.7209, 0.6776, 0.6986, 0.6675]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.21 s\n",
      "237,0.68201,0.77273,0.66667,0.15140,0.72093,0.67760,0.69859\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00383, 0.5968, 0.8538, 0.7025, 0.7062]\n",
      "e_2 * test rst: [0.00368, 0.7105, 0.6231, 0.6639, 0.7324]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00368, 0.7025, 0.6538, 0.6773, 0.7562]\n",
      "e_4 * test rst: [0.00348, 0.7183, 0.7846, 0.7500, 0.7688]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00340, 0.7124, 0.8385, 0.7703, 0.7854]\n",
      "e_6 * test rst: [0.00347, 0.7143, 0.8077, 0.7581, 0.7837]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00354, 0.7203, 0.7923, 0.7546, 0.7788]\n",
      "e_8 * test rst: [0.00355, 0.7192, 0.8077, 0.7609, 0.7771]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00358, 0.7143, 0.8077, 0.7581, 0.7782]\n",
      "e_10 * test rst: [0.00359, 0.6908, 0.8077, 0.7447, 0.7785]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 208.16 s\n",
      "218,0.65138,0.70918,0.80769,0.12977,0.69079,0.80769,0.74468\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00418, 0.6391, 0.8694, 0.7366, 0.6729]\n",
      "e_2 * test rst: [0.00401, 0.6630, 0.8063, 0.7276, 0.6746]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00376, 0.6611, 0.8964, 0.7610, 0.7630]\n",
      "e_4 * test rst: [0.00369, 0.6977, 0.8108, 0.7500, 0.7639]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00357, 0.7773, 0.7703, 0.7738, 0.7741]\n",
      "e_6 * test rst: [0.00366, 0.7114, 0.7883, 0.7479, 0.7687]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00360, 0.7860, 0.7613, 0.7735, 0.7785]\n",
      "e_8 * test rst: [0.00363, 0.7544, 0.7748, 0.7644, 0.7777]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00369, 0.7913, 0.7342, 0.7617, 0.7830]\n",
      "e_10 * test rst: [0.00363, 0.7798, 0.7658, 0.7727, 0.7817]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 191.69 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256,0.74708,0.83333,0.76233,0.19975,0.77982,0.76577,0.77273\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00456, 0.6216, 0.7041, 0.6603, 0.5972]\n",
      "e_2 * test rst: [0.00449, 0.6652, 0.7602, 0.7095, 0.6378]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00448, 0.6681, 0.7908, 0.7243, 0.6782]\n",
      "e_4 * test rst: [0.00443, 0.6786, 0.8724, 0.7634, 0.6770]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00460, 0.6806, 0.7500, 0.7136, 0.6793]\n",
      "e_6 * test rst: [0.00456, 0.6996, 0.8316, 0.7599, 0.6849]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00482, 0.6942, 0.7296, 0.7114, 0.6792]\n",
      "e_8 * test rst: [0.00494, 0.7068, 0.6888, 0.6977, 0.6815]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00464, 0.7019, 0.7449, 0.7228, 0.6882]\n",
      "e_10 * test rst: [0.00457, 0.7067, 0.7500, 0.7277, 0.6942]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 197.95 s\n",
      "228,0.71179,0.85987,0.74242,0.14377,0.70673,0.75000,0.72772\n",
      "*********\n",
      "CV end\n",
      "159.0,0.68553,0.79339,0.49045,0.08142,0.65254,0.49359,0.56204\n",
      "237.0,0.68201,0.77273,0.66667,0.15140,0.72093,0.67760,0.69859\n",
      "218.0,0.65138,0.70918,0.80769,0.12977,0.69079,0.80769,0.74468\n",
      "256.0,0.74708,0.83333,0.76233,0.19975,0.77982,0.76577,0.77273\n",
      "228.0,0.71179,0.85987,0.74242,0.14377,0.70673,0.75000,0.72772\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[219.6, 0.6955582422520307, 0.7937010641494057, 0.6939121830441701, 0.14122137404580154, 0.7101618724231886, 0.6989286890926235, 0.7011532481969086]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1098,0.6978221415607986,0.7914847161572053,0.6968680089485458,0.7061068702290076\n",
      "--- 1078.8415124416351 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_09.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 10\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1219\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00434, 0.5922, 0.9679, 0.7348, 0.6111]\n",
      "e_2 * test rst: [0.00446, 0.6535, 0.8462, 0.7374, 0.6267]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00432, 0.6685, 0.7885, 0.7235, 0.6363]\n",
      "e_4 * test rst: [0.00494, 0.6395, 0.6026, 0.6205, 0.6285]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00471, 0.6424, 0.6795, 0.6604, 0.6362]\n",
      "e_6 * test rst: [0.00428, 0.7041, 0.7628, 0.7323, 0.6459]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00444, 0.6987, 0.6987, 0.6987, 0.6427]\n",
      "e_8 * test rst: [0.00422, 0.6633, 0.8333, 0.7386, 0.6468]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00459, 0.6842, 0.6667, 0.6753, 0.6339]\n",
      "e_10 * test rst: [0.00433, 0.6684, 0.8013, 0.7289, 0.6437]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.88 s\n",
      "203,0.67980,0.82877,0.79618,0.13740,0.66845,0.80128,0.72886\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1202\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00401, 0.6957, 0.7869, 0.7385, 0.6252]\n",
      "e_2 * test rst: [0.00399, 0.7350, 0.8033, 0.7676, 0.6924]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00384, 0.7040, 0.8579, 0.7734, 0.6949]\n",
      "e_4 * test rst: [0.00416, 0.7239, 0.6448, 0.6821, 0.6432]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00395, 0.7070, 0.8306, 0.7638, 0.6538]\n",
      "e_6 * test rst: [0.00388, 0.7282, 0.8197, 0.7712, 0.6860]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00399, 0.7143, 0.7923, 0.7513, 0.6744]\n",
      "e_8 * test rst: [0.00413, 0.7198, 0.7158, 0.7178, 0.6649]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00414, 0.7068, 0.7377, 0.7219, 0.6635]\n",
      "e_10 * test rst: [0.00418, 0.7167, 0.7049, 0.7107, 0.6665]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 202.20 s\n",
      "213,0.69302,0.79275,0.69355,0.15394,0.71667,0.70492,0.71074\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1257\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00410, 0.5652, 0.9000, 0.6944, 0.6151]\n",
      "e_2 * test rst: [0.00398, 0.6585, 0.6231, 0.6403, 0.6623]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00394, 0.6541, 0.6692, 0.6616, 0.6878]\n",
      "e_4 * test rst: [0.00389, 0.6794, 0.6846, 0.6820, 0.7094]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00384, 0.6561, 0.7923, 0.7178, 0.7083]\n",
      "e_6 * test rst: [0.00399, 0.6213, 0.8077, 0.7023, 0.6919]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00409, 0.6402, 0.8077, 0.7143, 0.6888]\n",
      "e_8 * test rst: [0.00414, 0.6733, 0.7769, 0.7214, 0.6970]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00420, 0.6601, 0.7769, 0.7138, 0.6926]\n",
      "e_10 * test rst: [0.00422, 0.6474, 0.7769, 0.7063, 0.6864]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 209.56 s\n",
      "227,0.61233,0.65672,0.77692,0.11959,0.64744,0.77692,0.70629\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1111\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00421, 0.6164, 0.8829, 0.7259, 0.6760]\n",
      "e_2 * test rst: [0.00370, 0.7028, 0.7883, 0.7431, 0.7453]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00386, 0.6643, 0.8559, 0.7480, 0.7478]\n",
      "e_4 * test rst: [0.00373, 0.6792, 0.8108, 0.7392, 0.7582]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00370, 0.7534, 0.7432, 0.7483, 0.7622]\n",
      "e_6 * test rst: [0.00376, 0.8000, 0.7207, 0.7583, 0.7624]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00377, 0.6908, 0.8153, 0.7479, 0.7600]\n",
      "e_8 * test rst: [0.00378, 0.7339, 0.7703, 0.7516, 0.7525]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00379, 0.7403, 0.7703, 0.7550, 0.7550]\n",
      "e_10 * test rst: [0.00382, 0.7455, 0.7523, 0.7489, 0.7582]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 188.62 s\n",
      "254,0.72549,0.80822,0.74888,0.19466,0.74554,0.75225,0.74888\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1167\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00454, 0.6199, 0.9235, 0.7418, 0.5568]\n",
      "e_2 * test rst: [0.00484, 0.6338, 0.9184, 0.7500, 0.6335]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00453, 0.6434, 0.8469, 0.7313, 0.6391]\n",
      "e_4 * test rst: [0.00459, 0.6545, 0.8214, 0.7285, 0.6588]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00479, 0.7044, 0.7296, 0.7168, 0.6615]\n",
      "e_6 * test rst: [0.00476, 0.6537, 0.8571, 0.7417, 0.6413]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00466, 0.6468, 0.8878, 0.7484, 0.6478]\n",
      "e_8 * test rst: [0.00489, 0.6426, 0.8622, 0.7364, 0.6451]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00478, 0.6610, 0.7959, 0.7222, 0.6589]\n",
      "e_10 * test rst: [0.00485, 0.6523, 0.8520, 0.7389, 0.6440]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 197.41 s\n",
      "465,0.48712,0.50919,0.84343,0.16285,0.65234,0.85204,0.73894\n",
      "*********\n",
      "CV end\n",
      "203.0,0.67980,0.82877,0.79618,0.13740,0.66845,0.80128,0.72886\n",
      "213.0,0.69302,0.79275,0.69355,0.15394,0.71667,0.70492,0.71074\n",
      "227.0,0.61233,0.65672,0.77692,0.11959,0.64744,0.77692,0.70629\n",
      "254.0,0.72549,0.80822,0.74888,0.19466,0.74554,0.75225,0.74888\n",
      "465.0,0.48712,0.50919,0.84343,0.16285,0.65234,0.85204,0.73894\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[272.4, 0.6395551345677675, 0.7191270369951971, 0.7717926150340111, 0.15368956743002543, 0.6860862452498482, 0.7774832459141592, 0.7267434917143387]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1362,0.6134699853587116,0.6815789473684211,0.7706935123042505,0.7684478371501272\n",
      "--- 1078.9655163288116 seconds ---\n",
      "rst at ../data/raw_data/ft/T3/ft_base_10.tsv\n",
      "--- 10785.04301905632 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# RST = run_dev()\n",
    "\n",
    "all_RST = []\n",
    "\n",
    "for _i in range(1, 11):\n",
    "#     continue\n",
    "    print('****\\nbegin training 10X RENET2 models {}\\n'.format(_i))\n",
    "    RST = run_dev()\n",
    "    all_RST.append(RST)\n",
    "    mer_pd_rst = RST[1]\n",
    "#     cls_rst_file = os.path.join(args.raw_data_dir, \"ft_rst_ann_2nd_dev_exp_w%d_%02d.tsv\" % (IG_N, _i))\n",
    "\n",
    "    cls_rst_file = os.path.join(args.raw_data_dir, \"ft_base_%02d.tsv\" % (_i))\n",
    "    mer_pd_rst.to_csv(cls_rst_file, sep='\\t', index=False)\n",
    "    print('rst at {}'.format(cls_rst_file))\n",
    "    #break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10X #, positive #, precision, precision[-], recall, recall[-]\n",
      "1126.0,0.6743362831858407,0.7636949516648764,0.7348993288590604,0.7404580152671756\n",
      "1090.0,0.6819012797074955,0.7677208287895311,0.7181208053691275,0.732824427480916\n",
      "1201.0,0.6821576763485477,0.7628968253968254,0.7516778523489933,0.7569974554707379\n",
      "1314.0,0.6433990895295902,0.7202543142597638,0.7695749440715883,0.7748091603053435\n",
      "1166.0,0.6752136752136753,0.7697916666666667,0.7595078299776287,0.7684478371501272\n",
      "1255.0,0.6664019062748213,0.7507163323782235,0.7706935123042505,0.7786259541984732\n",
      "1262.0,0.6469194312796208,0.7113594040968343,0.7360178970917226,0.7366412213740458\n",
      "1396.0,0.6342857142857142,0.6955074875207987,0.7684563758389261,0.77735368956743\n",
      "1098.0,0.6978221415607986,0.7914847161572053,0.6968680089485458,0.7061068702290076\n",
      "1362.0,0.6134699853587116,0.6815789473684211,0.7706935123042505,0.7684478371501272\n",
      "avg,1227.0,0.6615907182744816,0.7415005474299146,0.7476510067114093,0.7540712468193383\n",
      "assemble, positive #, precision, precision[-], recall, recall[-]\n",
      "1,2325,0.5487333619579219,0.5867768595041323,0.9250559284116331,0.9312977099236641\n",
      "2,1785,0.6087199552822806,0.6675375571521881,0.8791946308724832,0.8842239185750637\n",
      "3,1505,0.6500994035785288,0.723404255319149,0.8534675615212528,0.8600508905852418\n",
      "4,1284,0.6770186335403726,0.7641509433962265,0.8187919463087249,0.821882951653944\n",
      "5,1167,0.6883005977796754,0.7821989528795812,0.79082774049217,0.7938931297709924\n",
      "6,1054,0.7022684310018904,0.8084606345475911,0.7494407158836689,0.7519083969465649\n",
      "7,960,0.7136929460580913,0.8298701298701299,0.7080536912751678,0.712468193384224\n",
      "8,872,0.728310502283105,0.8483547925608012,0.6633109619686801,0.6666666666666666\n",
      "9,748,0.7593085106382979,0.8707592891760905,0.5995525727069351,0.6106870229007634\n",
      "10,570,0.7926829268292683,0.9092783505154639,0.4888143176733781,0.5076335877862596\n",
      "selected assemble models at 1+, shape (13640, 4), result:\n",
      "(2325, 0.5487333619579219, 0.5867768595041323, 0.9250559284116331, 0.9312977099236641)\n"
     ]
    }
   ],
   "source": [
    "m_df = 0\n",
    "s_arr = []\n",
    "for _i in range(10):\n",
    "    cls_file = os.path.join(args.raw_data_dir, \"ft_base_%02d.tsv\" % (_i+1))\n",
    "\n",
    "    y_info = pd.read_csv(cls_file, sep='\\t')\n",
    "    y_info['pmid'] = y_info['pmid'].astype(str)\n",
    "    y_info['geneId'] = y_info['geneId'].astype(str)\n",
    "    y_info['diseaseId'] = y_info['diseaseId'].astype(str)\n",
    "    y_info['label'] = y_info['label'].astype(str)\n",
    "\n",
    "    S = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "#     print(S)\n",
    "    s_arr.append(list(S))\n",
    "\n",
    "    y_info.rename(columns={'prob':'prob_%02d'%(_i+1)}, inplace=True)\n",
    "    y_info.rename(columns={'pred':'pred_%02d'%(_i+1)}, inplace=True)\n",
    "#     y_info = y_info[y_info['pred'] == 1]\n",
    "    m_df = y_info if _i == 0 else m_df.merge(y_info, on=['pmid', 'geneId','diseaseId', 'label'], how='outer')\n",
    "\n",
    "\n",
    "m_df['hit_cnt'] = m_df.apply(lambda x: sum([x['pred_%02d'%(_i+1)] for _i in range(10)]), axis=1)\n",
    "\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "print('10X #, positive #, precision, precision[-], recall, recall[-]')\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "#print(list(np.mean(s_arr, axis=0)))\n",
    "\n",
    "\n",
    "avg_l = list(np.mean(s_arr, axis=0))\n",
    "avg_l = ['avg'] + avg_l\n",
    "\n",
    "print(','.join(list(map(str, avg_l))))\n",
    "\n",
    "print('assemble, positive #, precision, precision[-], recall, recall[-]')\n",
    "for _i in range(1, 11):\n",
    "    ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "    ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= _i else 0, axis=1)\n",
    "\n",
    "    _rst = evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "    _rst = [_i] + list(_rst)\n",
    "    print(','.join(list(map(str, _rst))))\n",
    "\n",
    "the_p_cnt = 1\n",
    "ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= the_p_cnt else 0, axis=1)\n",
    "\n",
    "print('selected assemble models at {}+, shape {}, result:'.format(the_p_cnt, ReNet_df.shape))\n",
    "print(evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get enhanced data\n",
      "887 1489 285 32.13077790304397 1774\n"
     ]
    }
   ],
   "source": [
    "print('get enhanced data')\n",
    "\n",
    "# addon_olp3p_feature = get_addon_data_ft(features_ft_sub, mdl_df, -1, 1, False)\n",
    "# merging renet|befree|dtminer result first\n",
    "merge_t_df = ReNet_df.merge(BeFree_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(DTMiner_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df.merge(BioBERT_df, on=['pmid', 'geneId','diseaseId'], how='outer')\n",
    "merge_t_df = merge_t_df[~merge_t_df.is_renet.isnull()]\n",
    "merge_t_df = merge_t_df.fillna(0)\n",
    "\n",
    "F=[0, 0, 0, 0] #all negative\n",
    "# F=[1, 1, 1] #all positive\n",
    "c=['is_renet','is_dtminer','is_befree', 'is_biobert']\n",
    "df = merge_t_df\n",
    "mdl_df = df[(df[c[0]]==F[0]) & ((df[c[1]]==F[1]) & (df[c[2]]==F[2]) & (df[c[3]]==F[3]))].copy()\n",
    "mdl_df['is_tar'] = 1\n",
    "\n",
    "# opt 1 filter the GDA annotated\n",
    "mdl_df = mdl_df.merge(abs_s_df, how='left')\n",
    "mdl_df = mdl_df[mdl_df.new_label.isnull()]\n",
    "mdl_df_n = mdl_df[['pmid', 'geneId','diseaseId', 'is_tar']].copy()\n",
    "\n",
    "N_ann_positive = len(n_features_ft_sub[1][n_features_ft_sub[1]['label']==1])\n",
    "N_ann_all = len(n_features_ft_sub[1])\n",
    "#print(N_ann_positive, N_ann_all)\n",
    "# N_ann_positive += len(addon_olp3p_feature[1])\n",
    "add_to_rate = 2\n",
    "N_add_nan = int(N_ann_positive * add_to_rate - N_ann_all)\n",
    "# N_add_nan = int(500 * (5/4))\n",
    "print(N_ann_positive, N_ann_all, N_add_nan, 100.*N_add_nan/N_ann_positive, N_add_nan+N_ann_all)\n",
    "addon_nan_feature = get_addon_data_ft(features_ft_sub, mdl_df_n, N_add_nan, 0, True)\n",
    "\n",
    "\n",
    "# reading pmid list, for CV\n",
    "_f = os.path.join(old_data_dir, 'PMID_lst')\n",
    "with open(_f, 'rb' ) as fp:\n",
    "    PMID_lst = pickle.load(fp)\n",
    "#len(PMID_lst), PMID_lst[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dev():\n",
    "    IS_DEBUG = False\n",
    "    IS_RUN_1 = False\n",
    "    IS_ADD_FT_N = True\n",
    "    IS_ADD_FT_P = False\n",
    "\n",
    "\n",
    "    _start_time = time.time()\n",
    "\n",
    "\n",
    "    # testing ann\n",
    "    tar_feature = n_features_ft_sub\n",
    "    all_feature = features_ft_sub\n",
    "\n",
    "    add_feature_n = addon_nan_feature if IS_ADD_FT_N else None\n",
    "    add_feature_p = addon_olp3p_feature if IS_ADD_FT_P else None\n",
    "\n",
    "    run_cls_df_l = []\n",
    "    adj_arr = []\n",
    "    ori_arr = []\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "    for idx, (train_pmid_lst_i, test_pmid_lst_i) in enumerate(kf.split(PMID_lst)):\n",
    "    #     if idx < 3:\n",
    "    #         continue\n",
    "        print(\"CV run %s/5\" % (idx+1))\n",
    "    #     print(len(train_pmid_lst_i), train_pmid_lst_i[:3])\n",
    "        #print(len(test_pmid_lst_i), test_pmid_lst_i[:3])\n",
    "\n",
    "        train_pmid_lst = [PMID_lst[i] for i in train_pmid_lst_i]\n",
    "        test_pmid_lst = [PMID_lst[i] for i in test_pmid_lst_i]\n",
    "\n",
    "        train_index = tar_feature[1][(tar_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "        test_index = tar_feature[1][(tar_feature[1].pmid.isin(test_pmid_lst))].index\n",
    "\n",
    "        _train_dev_ds, _test_ds = convert_features_to_dataset_cv(tar_feature, train_index), \\\n",
    "                                convert_features_to_dataset_cv(tar_feature, test_index)\n",
    "\n",
    "        if IS_ADD_FT_N:\n",
    "            #add data in training dataset\n",
    "            add_feature = add_feature_n\n",
    "            add_index = add_feature[1][(add_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "            #print('using add feature: %s' % len(add_index))\n",
    "            add_dataset = convert_features_to_dataset_cv(add_feature, add_index)\n",
    "            _train_dev_ds = torch.utils.data.ConcatDataset([_train_dev_ds, add_dataset]) \n",
    "\n",
    "        if IS_ADD_FT_P:\n",
    "            #add data in training dataset\n",
    "            add_feature = add_feature_p\n",
    "            add_index = add_feature[1][(add_feature[1].pmid.isin(train_pmid_lst))].index\n",
    "            #print('using add feature: %s' % len(add_index))\n",
    "            add_dataset = convert_features_to_dataset_cv(add_feature, add_index)\n",
    "            _train_dev_ds = torch.utils.data.ConcatDataset([_train_dev_ds, add_dataset]) \n",
    "\n",
    "        train_dataloader_ds = DataLoader(_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "        print('training dataset size {}'.format(len(_train_dev_ds)))\n",
    "\n",
    "\n",
    "    #     train_dataloader_ds = DataLoader(_train_dev_ds, batch_size=args.batch_size)\n",
    "        test_dataloader_ds = DataLoader(_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "        train_dt, dev_dt, test_dt = train_dataloader_ds, None, test_dataloader_ds\n",
    "\n",
    "        n_test_index = all_feature[1][(all_feature[1].pmid.isin(test_pmid_lst))].index\n",
    "        n_test_ds = convert_features_to_dataset_cv(all_feature, n_test_index)\n",
    "        n_test_dt = DataLoader(n_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "        # loading pretrained abstract model\n",
    "        #args.modle_dir = '../models/'\n",
    "        #model_name_prefix = 'Bst_abs_10'\n",
    "\n",
    "        #print('loading based model from', model_name_prefix)\n",
    "        #checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "        #config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "        checkpoint_f = os.path.join(args.pretrained_model_p + \".ckp\")\n",
    "        config_save_f = os.path.join(args.pretrained_model_p + \".cf\")\n",
    "\n",
    "        config = torch.load(config_save_f)\n",
    "        model, _, _ = load_checkpoint(config, checkpoint_f)\n",
    "\n",
    "\n",
    "        # model config\n",
    "        args.EB_dp = 0.3\n",
    "        args.FC_dp = 0.1\n",
    "\n",
    "        # training config\n",
    "        args.use_new_loss = False\n",
    "        args.use_cls_loss = False\n",
    "\n",
    "\n",
    "        #args.epochs = 10\n",
    "        #args.epochs = 1\n",
    "        args.warmup_epoch = 0\n",
    "        args.patience_epoch = 3\n",
    "\n",
    "        args.learning_rate = args.lr\n",
    "        #args.learning_rate = 8e-4\n",
    "        args.weight_decay = 8e-6\n",
    "        args.l2_weight_decay = 5e-5\n",
    "        args.max_grad_norm = 2.0\n",
    "        args.lr_reduce_factor = 0.5\n",
    "        args.lr_cooldown = 0\n",
    "        args.threshold = .5\n",
    "        args.adam_epsilon = 1e-8\n",
    "        args.use_loss_sh = False\n",
    "#         args.is_iterare_info = not IS_DEBUG\n",
    "        args.is_iterare_info = True\n",
    "\n",
    "\n",
    "        args.device = device\n",
    "\n",
    "\n",
    "        config = update_model_config(args, config, False)\n",
    "\n",
    "        model.update_model_config(config)\n",
    "\n",
    "#         if torch.cuda.device_count() > 1:\n",
    "#             print(\"use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#             model = nn.DataParallel(model)\n",
    "\n",
    "        model.to(device)\n",
    "        optimizer, scheduler = init_model_optimizer(model, args)\n",
    "        scheduler.step(0)\n",
    "\n",
    "        is_save_new_train = False\n",
    "        args.modle_dir = '../models'\n",
    "        model_name_prefix = 'build_ft_2nd_tr1'\n",
    "\n",
    "        args.checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "        args.config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "        if is_save_new_train:\n",
    "            torch.save(config, args.config_save_f)\n",
    "\n",
    "        if IS_DEBUG:\n",
    "            print(\"testing------\")\n",
    "\n",
    "            pred_l, tru_l, S, pred_o = eval(model, n_test_dt, args, 'test')\n",
    "            y_info = all_feature[1].iloc[n_test_index].copy()\n",
    "            y_info['pred'] = pred_l\n",
    "            _rst = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df[(abs_s_df.pmid.isin(test_pmid_lst))], _abs_s_df, abs_s_df_ner)\n",
    "            Tar_l = list(_rst) + list(S[1:4]) \n",
    "            print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), Tar_l)))\n",
    "            ori_arr.append(Tar_l[:])\n",
    "\n",
    "        print(\"training------\")\n",
    "\n",
    "        _, _, S_ori, _ = train(model, optimizer, scheduler, train_dt, dev_dt, args, test_dt, is_save_new_train)\n",
    "\n",
    "        pred_l, tru_l, S, pred_o = eval(model, n_test_dt, args, 'test')\n",
    "        y_info = all_feature[1].iloc[n_test_index].copy()\n",
    "        y_info['pred'] = pred_l\n",
    "        y_info['prob'] = pred_o\n",
    "        _rst = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df[(abs_s_df.pmid.isin(test_pmid_lst))], _abs_s_df, abs_s_df_ner)\n",
    "        Tar_l = list(_rst) + list(S_ori[1:4])\n",
    "        print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), Tar_l)))\n",
    "\n",
    "        run_cls_df_l.append(y_info)\n",
    "        adj_arr.append(Tar_l[:])\n",
    "\n",
    "        model = None\n",
    "        if not args.no_cuda:\n",
    "            free_cuda()\n",
    "        print(\"*********\")\n",
    "\n",
    "        if IS_RUN_1:\n",
    "            break\n",
    "\n",
    "    print('CV end')\n",
    "\n",
    "\n",
    "    if IS_DEBUG:\n",
    "        ori_arr = np.array(ori_arr)\n",
    "\n",
    "        for r in ori_arr:\n",
    "            print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), r)))\n",
    "\n",
    "        print('---')\n",
    "\n",
    "    adj_arr = np.array(adj_arr)\n",
    "\n",
    "    for r in adj_arr:\n",
    "        print(','.join(map(lambda x: '%.5f'%(x) if int(x) != x else str(x), r)))\n",
    "\n",
    "\n",
    "    print('CV mean, positive #, precision, precision[-], recall, recall[-]')\n",
    "    print(list(np.mean(adj_arr, axis=0)))\n",
    "    print('mlt train ended')\n",
    "\n",
    "\n",
    "    mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "\n",
    "    _rst = evaluate_rst_all_info_err(mer_pd_rst, 'pred', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "    print('CV merge, positive #, precision, precision[-], recall, recall[-]')\n",
    "    print(','.join(list(map(str, _rst))))\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - _start_time))   \n",
    "    return _rst, mer_pd_rst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****\n",
      "begin training 10X RENET2 models 1\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00433, 0.6497, 0.8205, 0.7252, 0.6101]\n",
      "e_2 * test rst: [0.00507, 0.6449, 0.4423, 0.5247, 0.5843]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00446, 0.6629, 0.7436, 0.7009, 0.6080]\n",
      "e_4 * test rst: [0.00523, 0.6698, 0.4551, 0.5420, 0.6296]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00480, 0.6014, 0.5705, 0.5855, 0.5915]\n",
      "e_6 * test rst: [0.00543, 0.6552, 0.4872, 0.5588, 0.5983]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00502, 0.5966, 0.6731, 0.6325, 0.5937]\n",
      "e_8 * test rst: [0.00492, 0.6023, 0.6603, 0.6300, 0.6001]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00511, 0.6434, 0.5321, 0.5825, 0.6113]\n",
      "e_10 * test rst: [0.00526, 0.5935, 0.5897, 0.5916, 0.5990]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.62 s\n",
      "176,0.60795,0.74797,0.58599,0.09796,0.59355,0.58974,0.59164\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00413, 0.6587, 0.7486, 0.7008, 0.6058]\n",
      "e_2 * test rst: [0.00379, 0.7130, 0.8689, 0.7833, 0.6651]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00380, 0.7051, 0.9016, 0.7914, 0.6735]\n",
      "e_4 * test rst: [0.00374, 0.7454, 0.8798, 0.8070, 0.6682]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00391, 0.7387, 0.8033, 0.7696, 0.6533]\n",
      "e_6 * test rst: [0.00381, 0.7249, 0.9071, 0.8058, 0.6450]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00387, 0.7285, 0.8798, 0.7970, 0.6468]\n",
      "e_8 * test rst: [0.00392, 0.7441, 0.8579, 0.7970, 0.6566]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00385, 0.7269, 0.9016, 0.8049, 0.6484]\n",
      "e_10 * test rst: [0.00393, 0.7393, 0.8525, 0.7919, 0.6585]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.74 s\n",
      "259,0.69732,0.78814,0.83871,0.18830,0.73934,0.85246,0.79188\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00418, 0.6774, 0.6462, 0.6614, 0.6743]\n",
      "e_2 * test rst: [0.00383, 0.7525, 0.5846, 0.6580, 0.7389]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00384, 0.7451, 0.5846, 0.6552, 0.7506]\n",
      "e_4 * test rst: [0.00379, 0.7959, 0.6000, 0.6842, 0.7707]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00371, 0.7642, 0.6231, 0.6864, 0.7514]\n",
      "e_6 * test rst: [0.00353, 0.7343, 0.8077, 0.7692, 0.7577]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00346, 0.7674, 0.7615, 0.7645, 0.7716]\n",
      "e_8 * test rst: [0.00362, 0.7706, 0.6462, 0.7029, 0.7686]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00352, 0.7647, 0.7000, 0.7309, 0.7718]\n",
      "e_10 * test rst: [0.00347, 0.7581, 0.7231, 0.7402, 0.7756]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 93.89 s\n",
      "141,0.75887,0.87179,0.72308,0.11323,0.75806,0.72308,0.74016\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00400, 0.7751, 0.7297, 0.7517, 0.7187]\n",
      "e_2 * test rst: [0.00386, 0.7456, 0.7658, 0.7556, 0.7337]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00399, 0.6463, 0.8559, 0.7364, 0.7523]\n",
      "e_4 * test rst: [0.00385, 0.7052, 0.8514, 0.7714, 0.7474]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00369, 0.7595, 0.8108, 0.7843, 0.7577]\n",
      "e_6 * test rst: [0.00383, 0.7438, 0.8108, 0.7759, 0.7476]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00369, 0.7259, 0.8468, 0.7817, 0.7712]\n",
      "e_8 * test rst: [0.00385, 0.7710, 0.7432, 0.7569, 0.7613]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00390, 0.7699, 0.7838, 0.7768, 0.7533]\n",
      "e_10 * test rst: [0.00384, 0.7379, 0.8243, 0.7787, 0.7584]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.38 s\n",
      "281,0.70922,0.77447,0.82063,0.20229,0.73790,0.82432,0.77872\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00481, 0.6485, 0.6684, 0.6583, 0.5977]\n",
      "e_2 * test rst: [0.00439, 0.6426, 0.9082, 0.7526, 0.6663]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00453, 0.6944, 0.7653, 0.7282, 0.6587]\n",
      "e_4 * test rst: [0.00452, 0.6629, 0.9031, 0.7646, 0.6576]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00486, 0.7158, 0.6939, 0.7047, 0.6618]\n",
      "e_6 * test rst: [0.00452, 0.6693, 0.8571, 0.7517, 0.6654]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00455, 0.6856, 0.8010, 0.7388, 0.6710]\n",
      "e_8 * test rst: [0.00463, 0.6667, 0.8571, 0.7500, 0.6614]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00459, 0.6995, 0.7602, 0.7286, 0.6749]\n",
      "e_10 * test rst: [0.00455, 0.6838, 0.8163, 0.7442, 0.6722]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.37 s\n",
      "262,0.68441,0.78836,0.80808,0.15649,0.68376,0.81633,0.74419\n",
      "*********\n",
      "CV end\n",
      "176.0,0.60795,0.74797,0.58599,0.09796,0.59355,0.58974,0.59164\n",
      "259.0,0.69732,0.78814,0.83871,0.18830,0.73934,0.85246,0.79188\n",
      "141.0,0.75887,0.87179,0.72308,0.11323,0.75806,0.72308,0.74016\n",
      "281.0,0.70922,0.77447,0.82063,0.20229,0.73790,0.82432,0.77872\n",
      "262.0,0.68441,0.78836,0.80808,0.15649,0.68376,0.81633,0.74419\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[223.8, 0.6915536611776385, 0.7941451636312358, 0.7552964944828331, 0.15165394402035623, 0.7025226611367874, 0.761186076830105, 0.7293169950106754]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1119,0.691006233303651,0.79,0.7662192393736018,0.7582697201017812\n",
      "--- 505.97354674339294 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_01.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 2\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00461, 0.6284, 0.5962, 0.6118, 0.6046]\n",
      "e_2 * test rst: [0.00431, 0.6150, 0.7372, 0.6706, 0.6259]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00439, 0.6338, 0.8654, 0.7317, 0.6359]\n",
      "e_4 * test rst: [0.00429, 0.6209, 0.8397, 0.7139, 0.6175]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00472, 0.5988, 0.6603, 0.6280, 0.6238]\n",
      "e_6 * test rst: [0.00490, 0.6213, 0.6731, 0.6462, 0.6016]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00495, 0.6188, 0.6346, 0.6266, 0.6072]\n",
      "e_8 * test rst: [0.00508, 0.5964, 0.6346, 0.6149, 0.6054]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00531, 0.6557, 0.5128, 0.5755, 0.6105]\n",
      "e_10 * test rst: [0.00540, 0.6529, 0.5064, 0.5704, 0.6101]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.70 s\n",
      "147,0.68027,0.78378,0.50318,0.08397,0.65289,0.50641,0.57040\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00404, 0.6566, 0.7104, 0.6824, 0.6229]\n",
      "e_2 * test rst: [0.00427, 0.7062, 0.7486, 0.7268, 0.6400]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00442, 0.7318, 0.7158, 0.7238, 0.6547]\n",
      "e_4 * test rst: [0.00407, 0.7033, 0.8033, 0.7500, 0.6748]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00442, 0.7143, 0.7923, 0.7513, 0.6500]\n",
      "e_6 * test rst: [0.00419, 0.7102, 0.6831, 0.6964, 0.6604]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00468, 0.7086, 0.5847, 0.6407, 0.6456]\n",
      "e_8 * test rst: [0.00444, 0.7115, 0.6066, 0.6549, 0.6549]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00478, 0.7211, 0.5792, 0.6424, 0.6449]\n",
      "e_10 * test rst: [0.00480, 0.7075, 0.5683, 0.6303, 0.6431]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.60 s\n",
      "186,0.66489,0.77976,0.55914,0.12468,0.70748,0.56831,0.63030\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00400, 0.6405, 0.7538, 0.6926, 0.6485]\n",
      "e_2 * test rst: [0.00403, 0.6463, 0.8154, 0.7211, 0.6859]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00388, 0.7071, 0.7615, 0.7333, 0.6850]\n",
      "e_4 * test rst: [0.00404, 0.7045, 0.7154, 0.7099, 0.6955]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00393, 0.7317, 0.6923, 0.7115, 0.7235]\n",
      "e_6 * test rst: [0.00421, 0.7016, 0.6692, 0.6850, 0.6900]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00415, 0.6892, 0.7846, 0.7338, 0.6865]\n",
      "e_8 * test rst: [0.00419, 0.6894, 0.7000, 0.6947, 0.6892]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00419, 0.6853, 0.7538, 0.7179, 0.6892]\n",
      "e_10 * test rst: [0.00422, 0.6806, 0.7538, 0.7153, 0.6873]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.38 s\n",
      "176,0.65909,0.74026,0.75385,0.12214,0.68056,0.75385,0.71533\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00398, 0.6445, 0.8739, 0.7419, 0.7165]\n",
      "e_2 * test rst: [0.00391, 0.7059, 0.7568, 0.7304, 0.7353]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00398, 0.6738, 0.8559, 0.7540, 0.7242]\n",
      "e_4 * test rst: [0.00419, 0.6728, 0.8243, 0.7409, 0.6807]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00422, 0.6643, 0.8559, 0.7480, 0.7203]\n",
      "e_6 * test rst: [0.00408, 0.6800, 0.8423, 0.7525, 0.7259]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00410, 0.6703, 0.8423, 0.7465, 0.7291]\n",
      "e_8 * test rst: [0.00421, 0.6786, 0.8559, 0.7570, 0.7348]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00414, 0.6950, 0.8108, 0.7484, 0.7262]\n",
      "e_10 * test rst: [0.00417, 0.6891, 0.8288, 0.7526, 0.7272]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 90.21 s\n",
      "315,0.67089,0.72924,0.82511,0.21374,0.68914,0.82883,0.75256\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00468, 0.6151, 0.9133, 0.7351, 0.5403]\n",
      "e_2 * test rst: [0.00499, 0.7143, 0.6633, 0.6878, 0.6471]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00456, 0.7068, 0.6888, 0.6977, 0.6827]\n",
      "e_4 * test rst: [0.00488, 0.6345, 0.9388, 0.7572, 0.6792]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00478, 0.7010, 0.6939, 0.6974, 0.6760]\n",
      "e_6 * test rst: [0.00470, 0.6842, 0.7959, 0.7358, 0.6849]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00473, 0.7056, 0.7092, 0.7074, 0.6895]\n",
      "e_8 * test rst: [0.00477, 0.7085, 0.7194, 0.7139, 0.6895]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00466, 0.6751, 0.8163, 0.7390, 0.6913]\n",
      "e_10 * test rst: [0.00463, 0.6837, 0.7500, 0.7153, 0.7002]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.50 s\n",
      "239,0.67917,0.80368,0.74242,0.13868,0.68372,0.75000,0.71533\n",
      "*********\n",
      "CV end\n",
      "147.0,0.68027,0.78378,0.50318,0.08397,0.65289,0.50641,0.57040\n",
      "186.0,0.66489,0.77976,0.55914,0.12468,0.70748,0.56831,0.63030\n",
      "176.0,0.65909,0.74026,0.75385,0.12214,0.68056,0.75385,0.71533\n",
      "315.0,0.67089,0.72924,0.82511,0.21374,0.68914,0.82883,0.75256\n",
      "239.0,0.67917,0.80368,0.74242,0.13868,0.68372,0.75000,0.71533\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[212.6, 0.6708618755143514, 0.7673456575313677, 0.6767414004431496, 0.1366412213740458, 0.6827581235495781, 0.6814782500028402, 0.6767826627483542]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1063,0.6710402999062793,0.7617411225658648,0.6845637583892618,0.683206106870229\n",
      "--- 506.30492901802063 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_02.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 3\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00433, 0.6126, 0.8718, 0.7196, 0.6359]\n",
      "e_2 * test rst: [0.00450, 0.6199, 0.6795, 0.6483, 0.5964]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00469, 0.6357, 0.5705, 0.6014, 0.6375]\n",
      "e_4 * test rst: [0.00526, 0.6012, 0.6282, 0.6144, 0.5920]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00507, 0.6696, 0.4936, 0.5683, 0.6275]\n",
      "e_6 * test rst: [0.00501, 0.6434, 0.5897, 0.6154, 0.6027]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00522, 0.6222, 0.5385, 0.5773, 0.6031]\n",
      "e_8 * test rst: [0.00544, 0.6452, 0.5128, 0.5714, 0.5984]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00587, 0.6569, 0.4295, 0.5194, 0.5937]\n",
      "e_10 * test rst: [0.00583, 0.6339, 0.4551, 0.5299, 0.5914]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.78 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135,0.65185,0.80198,0.45223,0.08142,0.63393,0.45513,0.52985\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00404, 0.7056, 0.6940, 0.6997, 0.6318]\n",
      "e_2 * test rst: [0.00388, 0.6737, 0.8689, 0.7589, 0.6396]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00398, 0.6894, 0.8852, 0.7751, 0.6377]\n",
      "e_4 * test rst: [0.00389, 0.6862, 0.8962, 0.7773, 0.6507]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00423, 0.7209, 0.6776, 0.6986, 0.6403]\n",
      "e_6 * test rst: [0.00394, 0.7220, 0.8087, 0.7629, 0.6444]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00407, 0.7202, 0.7596, 0.7394, 0.6535]\n",
      "e_8 * test rst: [0.00431, 0.7189, 0.7268, 0.7228, 0.6367]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00432, 0.7135, 0.7213, 0.7174, 0.6423]\n",
      "e_10 * test rst: [0.00442, 0.7158, 0.7158, 0.7158, 0.6393]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.39 s\n",
      "228,0.66087,0.75598,0.70430,0.15903,0.71585,0.71585,0.71585\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00368, 0.6667, 0.7231, 0.6937, 0.7299]\n",
      "e_2 * test rst: [0.00343, 0.6957, 0.8615, 0.7698, 0.7745]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00324, 0.7432, 0.8462, 0.7914, 0.7915]\n",
      "e_4 * test rst: [0.00350, 0.6727, 0.8538, 0.7525, 0.7769]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00326, 0.7500, 0.8077, 0.7778, 0.7908]\n",
      "e_6 * test rst: [0.00332, 0.7647, 0.8000, 0.7820, 0.7892]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00332, 0.7591, 0.8000, 0.7790, 0.7937]\n",
      "e_8 * test rst: [0.00329, 0.7571, 0.8154, 0.7852, 0.7974]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00329, 0.7518, 0.8154, 0.7823, 0.7998]\n",
      "e_10 * test rst: [0.00335, 0.7500, 0.8077, 0.7778, 0.7967]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.49 s\n",
      "185,0.67027,0.72121,0.80769,0.12723,0.75000,0.80769,0.77778\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00413, 0.6271, 0.8559, 0.7238, 0.6840]\n",
      "e_2 * test rst: [0.00371, 0.7107, 0.7748, 0.7414, 0.7362]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00369, 0.7185, 0.7703, 0.7435, 0.7534]\n",
      "e_4 * test rst: [0.00367, 0.7798, 0.7658, 0.7727, 0.7693]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00375, 0.7403, 0.7703, 0.7550, 0.7618]\n",
      "e_6 * test rst: [0.00387, 0.7721, 0.7477, 0.7597, 0.7633]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00381, 0.7626, 0.7523, 0.7574, 0.7625]\n",
      "e_8 * test rst: [0.00388, 0.7467, 0.7703, 0.7583, 0.7616]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00394, 0.7746, 0.7432, 0.7586, 0.7635]\n",
      "e_10 * test rst: [0.00404, 0.7700, 0.7387, 0.7540, 0.7603]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 90.10 s\n",
      "237,0.76471,0.83981,0.73543,0.18957,0.76995,0.73874,0.75402\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00477, 0.6178, 0.8163, 0.7033, 0.5848]\n",
      "e_2 * test rst: [0.00477, 0.6318, 0.8929, 0.7400, 0.6073]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00454, 0.6774, 0.7500, 0.7119, 0.6585]\n",
      "e_4 * test rst: [0.00466, 0.6544, 0.9082, 0.7607, 0.6536]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00486, 0.6902, 0.6480, 0.6684, 0.6585]\n",
      "e_6 * test rst: [0.00469, 0.6761, 0.8520, 0.7540, 0.6629]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00476, 0.6798, 0.7908, 0.7311, 0.6635]\n",
      "e_8 * test rst: [0.00484, 0.6796, 0.7143, 0.6965, 0.6584]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00474, 0.6767, 0.8010, 0.7336, 0.6676]\n",
      "e_10 * test rst: [0.00476, 0.6895, 0.7704, 0.7277, 0.6721]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.78 s\n",
      "238,0.69456,0.83230,0.76263,0.14377,0.68950,0.77041,0.72771\n",
      "*********\n",
      "CV end\n",
      "135.0,0.65185,0.80198,0.45223,0.08142,0.63393,0.45513,0.52985\n",
      "228.0,0.66087,0.75598,0.70430,0.15903,0.71585,0.71585,0.71585\n",
      "185.0,0.67027,0.72121,0.80769,0.12723,0.75000,0.80769,0.77778\n",
      "237.0,0.76471,0.83981,0.73543,0.18957,0.76995,0.73874,0.75402\n",
      "238.0,0.69456,0.83230,0.76263,0.14377,0.68950,0.77041,0.72771\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[204.6, 0.6884516478297044, 0.7902554284729246, 0.6924549907838109, 0.14020356234096693, 0.711845266900452, 0.6975628818720153, 0.701041870092239]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1023,0.6932814021421616,0.7897862232779097,0.6957494407158836,0.7010178117048346\n",
      "--- 506.2337963581085 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_03.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 4\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00443, 0.6335, 0.7756, 0.6974, 0.6036]\n",
      "e_2 * test rst: [0.00430, 0.6579, 0.8013, 0.7225, 0.6267]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00444, 0.6531, 0.8205, 0.7273, 0.6127]\n",
      "e_4 * test rst: [0.00447, 0.6576, 0.7756, 0.7118, 0.6332]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00445, 0.6576, 0.7756, 0.7118, 0.6441]\n",
      "e_6 * test rst: [0.00452, 0.6573, 0.7500, 0.7006, 0.6400]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00450, 0.6474, 0.7179, 0.6809, 0.6348]\n",
      "e_8 * test rst: [0.00484, 0.6912, 0.6026, 0.6438, 0.6189]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00462, 0.6457, 0.7244, 0.6828, 0.6363]\n",
      "e_10 * test rst: [0.00491, 0.6752, 0.6795, 0.6773, 0.6270]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 93.05 s\n",
      "174,0.68391,0.83206,0.67516,0.12214,0.67516,0.67949,0.67732\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00407, 0.7072, 0.6995, 0.7033, 0.6685]\n",
      "e_2 * test rst: [0.00413, 0.7545, 0.6885, 0.7200, 0.7238]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00417, 0.7062, 0.7486, 0.7268, 0.6561]\n",
      "e_4 * test rst: [0.00449, 0.7321, 0.6721, 0.7009, 0.6686]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00412, 0.7171, 0.5956, 0.6507, 0.6639]\n",
      "e_6 * test rst: [0.00443, 0.7308, 0.6230, 0.6726, 0.6700]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00498, 0.7311, 0.4754, 0.5762, 0.6583]\n",
      "e_8 * test rst: [0.00487, 0.7273, 0.5246, 0.6095, 0.6548]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00533, 0.7317, 0.4918, 0.5882, 0.6441]\n",
      "e_10 * test rst: [0.00501, 0.7293, 0.5301, 0.6139, 0.6459]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.63 s\n",
      "174,0.67614,0.78882,0.52151,0.11832,0.72932,0.53005,0.61392\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00419, 0.6122, 0.6923, 0.6498, 0.6313]\n",
      "e_2 * test rst: [0.00385, 0.6296, 0.7846, 0.6986, 0.6922]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00361, 0.6550, 0.8615, 0.7442, 0.7495]\n",
      "e_4 * test rst: [0.00381, 0.6688, 0.8077, 0.7317, 0.7268]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00366, 0.7462, 0.7462, 0.7462, 0.7523]\n",
      "e_6 * test rst: [0.00395, 0.7339, 0.7000, 0.7165, 0.7336]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00374, 0.7385, 0.7385, 0.7385, 0.7508]\n",
      "e_8 * test rst: [0.00377, 0.7090, 0.7308, 0.7197, 0.7479]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00383, 0.7323, 0.7154, 0.7237, 0.7498]\n",
      "e_10 * test rst: [0.00389, 0.7398, 0.7000, 0.7194, 0.7476]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.43 s\n",
      "143,0.74126,0.81600,0.70000,0.11069,0.73984,0.70000,0.71937\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00415, 0.6903, 0.7027, 0.6964, 0.6903]\n",
      "e_2 * test rst: [0.00409, 0.7093, 0.7252, 0.7171, 0.6809]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00368, 0.7490, 0.8063, 0.7766, 0.7503]\n",
      "e_4 * test rst: [0.00406, 0.7839, 0.7027, 0.7411, 0.7537]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00398, 0.7857, 0.7432, 0.7639, 0.7523]\n",
      "e_6 * test rst: [0.00398, 0.7812, 0.7883, 0.7848, 0.7503]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00401, 0.7793, 0.7793, 0.7793, 0.7528]\n",
      "e_8 * test rst: [0.00394, 0.7696, 0.7973, 0.7832, 0.7607]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00405, 0.7840, 0.7523, 0.7678, 0.7550]\n",
      "e_10 * test rst: [0.00401, 0.7768, 0.7838, 0.7803, 0.7572]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.47 s\n",
      "258,0.75676,0.84234,0.78027,0.20229,0.77679,0.78378,0.78027\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00486, 0.6273, 0.7041, 0.6635, 0.5793]\n",
      "e_2 * test rst: [0.00469, 0.6340, 0.8571, 0.7289, 0.5999]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00483, 0.6716, 0.6990, 0.6850, 0.6480]\n",
      "e_4 * test rst: [0.00465, 0.6568, 0.9082, 0.7623, 0.6521]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00460, 0.6763, 0.8316, 0.7460, 0.6704]\n",
      "e_6 * test rst: [0.00481, 0.7216, 0.6480, 0.6828, 0.6791]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00463, 0.6916, 0.7551, 0.7220, 0.6916]\n",
      "e_8 * test rst: [0.00481, 0.7247, 0.6582, 0.6898, 0.6861]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00465, 0.6938, 0.7398, 0.7160, 0.6958]\n",
      "e_10 * test rst: [0.00470, 0.7044, 0.7296, 0.7168, 0.6991]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.31 s\n",
      "223,0.70089,0.80380,0.72222,0.13613,0.70443,0.72959,0.71679\n",
      "*********\n",
      "CV end\n",
      "174.0,0.68391,0.83206,0.67516,0.12214,0.67516,0.67949,0.67732\n",
      "174.0,0.67614,0.78882,0.52151,0.11832,0.72932,0.53005,0.61392\n",
      "143.0,0.74126,0.81600,0.70000,0.11069,0.73984,0.70000,0.71937\n",
      "258.0,0.75676,0.84234,0.78027,0.20229,0.77679,0.78378,0.78027\n",
      "223.0,0.70089,0.80380,0.72222,0.13613,0.70443,0.72959,0.71679\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[194.4, 0.7117905529543461, 0.8166041510350921, 0.6798311785062123, 0.13791348600508907, 0.7251078308272221, 0.6845834889628801, 0.7015337943482537]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "972,0.7141393442622951,0.8180677540777918,0.6834451901565995,0.6895674300254453\n",
      "--- 506.74110412597656 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_04.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 5\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00444, 0.6250, 0.8013, 0.7022, 0.6104]\n",
      "e_2 * test rst: [0.00490, 0.6164, 0.5769, 0.5960, 0.5845]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00492, 0.6377, 0.5641, 0.5986, 0.6083]\n",
      "e_4 * test rst: [0.00533, 0.6691, 0.5833, 0.6233, 0.5779]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00456, 0.6429, 0.6923, 0.6667, 0.6075]\n",
      "e_6 * test rst: [0.00459, 0.6607, 0.7115, 0.6852, 0.6218]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00513, 0.6621, 0.6154, 0.6379, 0.5939]\n",
      "e_8 * test rst: [0.00524, 0.6642, 0.5705, 0.6138, 0.5906]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00532, 0.6741, 0.5833, 0.6254, 0.5887]\n",
      "e_10 * test rst: [0.00517, 0.6853, 0.6282, 0.6555, 0.5946]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 93.97 s\n",
      "161,0.67702,0.82609,0.62420,0.10687,0.68531,0.62821,0.65552\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00438, 0.7350, 0.4699, 0.5733, 0.6212]\n",
      "e_2 * test rst: [0.00380, 0.7400, 0.8087, 0.7728, 0.6857]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00377, 0.7269, 0.9016, 0.8049, 0.6974]\n",
      "e_4 * test rst: [0.00365, 0.7333, 0.8415, 0.7837, 0.7148]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00363, 0.7477, 0.8907, 0.8130, 0.7088]\n",
      "e_6 * test rst: [0.00388, 0.7450, 0.8142, 0.7781, 0.7001]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00386, 0.7500, 0.8852, 0.8120, 0.6961]\n",
      "e_8 * test rst: [0.00393, 0.7379, 0.8306, 0.7815, 0.6962]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00392, 0.7465, 0.8689, 0.8030, 0.6965]\n",
      "e_10 * test rst: [0.00398, 0.7451, 0.8306, 0.7855, 0.6964]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.16 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261,0.69582,0.77366,0.81720,0.18448,0.74510,0.83060,0.78553\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00381, 0.6347, 0.8154, 0.7138, 0.6992]\n",
      "e_2 * test rst: [0.00428, 0.6698, 0.5462, 0.6017, 0.6943]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00404, 0.6398, 0.7923, 0.7079, 0.6866]\n",
      "e_4 * test rst: [0.00407, 0.7500, 0.6462, 0.6942, 0.7177]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00412, 0.6870, 0.6923, 0.6897, 0.7041]\n",
      "e_6 * test rst: [0.00388, 0.7131, 0.6692, 0.6905, 0.7385]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00389, 0.7063, 0.6846, 0.6953, 0.7357]\n",
      "e_8 * test rst: [0.00401, 0.6866, 0.7077, 0.6970, 0.7295]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00401, 0.7000, 0.7000, 0.7000, 0.7288]\n",
      "e_10 * test rst: [0.00400, 0.6838, 0.7154, 0.6992, 0.7284]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.29 s\n",
      "173,0.65318,0.71895,0.71538,0.11450,0.68382,0.71538,0.69925\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00451, 0.7037, 0.5135, 0.5938, 0.6298]\n",
      "e_2 * test rst: [0.00372, 0.7028, 0.7883, 0.7431, 0.7462]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00363, 0.7355, 0.8018, 0.7672, 0.7569]\n",
      "e_4 * test rst: [0.00370, 0.8093, 0.7072, 0.7548, 0.7779]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00362, 0.7810, 0.7387, 0.7593, 0.7787]\n",
      "e_6 * test rst: [0.00370, 0.7910, 0.7162, 0.7518, 0.7768]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00362, 0.7437, 0.7973, 0.7696, 0.7799]\n",
      "e_8 * test rst: [0.00379, 0.8079, 0.7387, 0.7718, 0.7741]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00381, 0.8088, 0.7432, 0.7746, 0.7776]\n",
      "e_10 * test rst: [0.00369, 0.7990, 0.7523, 0.7749, 0.7858]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.16 s\n",
      "230,0.78355,0.87755,0.74888,0.19338,0.79904,0.75225,0.77494\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00453, 0.6296, 0.6939, 0.6602, 0.6347]\n",
      "e_2 * test rst: [0.00505, 0.7143, 0.5102, 0.5952, 0.6689]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00434, 0.6953, 0.8265, 0.7552, 0.6985]\n",
      "e_4 * test rst: [0.00452, 0.6777, 0.7296, 0.7027, 0.6975]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00436, 0.6866, 0.7602, 0.7215, 0.7083]\n",
      "e_6 * test rst: [0.00500, 0.7378, 0.6173, 0.6722, 0.6767]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00469, 0.7128, 0.7092, 0.7110, 0.7022]\n",
      "e_8 * test rst: [0.00452, 0.6667, 0.8265, 0.7380, 0.7070]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00468, 0.7216, 0.7143, 0.7179, 0.6989]\n",
      "e_10 * test rst: [0.00492, 0.7273, 0.6531, 0.6882, 0.6873]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.07 s\n",
      "192,0.72539,0.82759,0.64646,0.12977,0.72727,0.65306,0.68817\n",
      "*********\n",
      "CV end\n",
      "161.0,0.67702,0.82609,0.62420,0.10687,0.68531,0.62821,0.65552\n",
      "261.0,0.69582,0.77366,0.81720,0.18448,0.74510,0.83060,0.78553\n",
      "173.0,0.65318,0.71895,0.71538,0.11450,0.68382,0.71538,0.69925\n",
      "230.0,0.78355,0.87755,0.74888,0.19338,0.79904,0.75225,0.77494\n",
      "192.0,0.72539,0.82759,0.64646,0.12977,0.72727,0.65306,0.68817\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[203.4, 0.7069907398744334, 0.8047681967265593, 0.7104272616694797, 0.14580152671755725, 0.7281104086831641, 0.7159008626455934, 0.7206820538164472]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1017,0.7110675808031341,0.8039906103286385,0.7136465324384788,0.7290076335877863\n",
      "--- 506.7053258419037 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_05.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 6\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00445, 0.6284, 0.7372, 0.6785, 0.6075]\n",
      "e_2 * test rst: [0.00439, 0.6267, 0.8718, 0.7292, 0.6049]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00459, 0.6364, 0.6282, 0.6323, 0.6061]\n",
      "e_4 * test rst: [0.00498, 0.6786, 0.4872, 0.5672, 0.6177]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00501, 0.6408, 0.5833, 0.6107, 0.6153]\n",
      "e_6 * test rst: [0.00506, 0.5956, 0.5192, 0.5548, 0.6054]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00502, 0.6504, 0.5128, 0.5735, 0.6206]\n",
      "e_8 * test rst: [0.00504, 0.6028, 0.5449, 0.5724, 0.6152]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00507, 0.5828, 0.6090, 0.5956, 0.5950]\n",
      "e_10 * test rst: [0.00503, 0.6370, 0.5513, 0.5911, 0.6181]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.47 s\n",
      "149,0.65101,0.82857,0.54777,0.09669,0.63704,0.55128,0.59107\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00380, 0.7030, 0.7760, 0.7377, 0.6778]\n",
      "e_2 * test rst: [0.00399, 0.7391, 0.7432, 0.7411, 0.6992]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00363, 0.7452, 0.8470, 0.7928, 0.7181]\n",
      "e_4 * test rst: [0.00354, 0.7198, 0.9126, 0.8048, 0.7415]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00377, 0.7149, 0.8634, 0.7822, 0.7164]\n",
      "e_6 * test rst: [0.00361, 0.7248, 0.8634, 0.7880, 0.7318]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00406, 0.7150, 0.8087, 0.7590, 0.7053]\n",
      "e_8 * test rst: [0.00393, 0.7130, 0.8415, 0.7719, 0.7106]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00400, 0.7225, 0.8251, 0.7704, 0.7116]\n",
      "e_10 * test rst: [0.00401, 0.7225, 0.8251, 0.7704, 0.7105]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 93.10 s\n",
      "262,0.69697,0.77686,0.81183,0.18193,0.72249,0.82514,0.77041\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00384, 0.6744, 0.6692, 0.6718, 0.6919]\n",
      "e_2 * test rst: [0.00372, 0.6940, 0.7154, 0.7045, 0.7241]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00369, 0.6918, 0.7769, 0.7319, 0.7480]\n",
      "e_4 * test rst: [0.00386, 0.7217, 0.6385, 0.6776, 0.7389]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00365, 0.7099, 0.7154, 0.7126, 0.7484]\n",
      "e_6 * test rst: [0.00356, 0.7192, 0.8077, 0.7609, 0.7590]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00360, 0.7067, 0.8154, 0.7571, 0.7634]\n",
      "e_8 * test rst: [0.00366, 0.7063, 0.7769, 0.7399, 0.7632]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00375, 0.7176, 0.7231, 0.7203, 0.7587]\n",
      "e_10 * test rst: [0.00370, 0.7143, 0.7308, 0.7224, 0.7587]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.16 s\n",
      "166,0.68072,0.80147,0.73077,0.11578,0.71429,0.73077,0.72243\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00401, 0.6203, 0.8829, 0.7286, 0.7190]\n",
      "e_2 * test rst: [0.00369, 0.7674, 0.7432, 0.7551, 0.7686]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00448, 0.6355, 0.9189, 0.7514, 0.7618]\n",
      "e_4 * test rst: [0.00384, 0.6966, 0.8378, 0.7607, 0.7535]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00388, 0.7137, 0.8423, 0.7727, 0.7546]\n",
      "e_6 * test rst: [0.00372, 0.7360, 0.8288, 0.7797, 0.7598]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00387, 0.7404, 0.6937, 0.7163, 0.7475]\n",
      "e_8 * test rst: [0.00390, 0.7320, 0.8243, 0.7754, 0.7433]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00395, 0.7590, 0.6667, 0.7098, 0.7436]\n",
      "e_10 * test rst: [0.00397, 0.7554, 0.6261, 0.6847, 0.7486]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.23 s\n",
      "224,0.74222,0.82653,0.62332,0.16285,0.75543,0.62613,0.68473\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00467, 0.6044, 0.6939, 0.6461, 0.5993]\n",
      "e_2 * test rst: [0.00504, 0.6569, 0.6837, 0.6700, 0.6184]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00556, 0.7895, 0.3827, 0.5155, 0.6444]\n",
      "e_4 * test rst: [0.00533, 0.7870, 0.4337, 0.5592, 0.6571]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00464, 0.6697, 0.7551, 0.7098, 0.6539]\n",
      "e_6 * test rst: [0.00503, 0.7320, 0.5714, 0.6418, 0.6642]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00485, 0.6989, 0.6276, 0.6613, 0.6618]\n",
      "e_8 * test rst: [0.00486, 0.7006, 0.6327, 0.6649, 0.6656]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00501, 0.7134, 0.5969, 0.6500, 0.6618]\n",
      "e_10 * test rst: [0.00494, 0.7118, 0.6173, 0.6612, 0.6659]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.53 s\n",
      "199,0.70500,0.79630,0.61111,0.13104,0.71176,0.61735,0.66120\n",
      "*********\n",
      "CV end\n",
      "149.0,0.65101,0.82857,0.54777,0.09669,0.63704,0.55128,0.59107\n",
      "262.0,0.69697,0.77686,0.81183,0.18193,0.72249,0.82514,0.77041\n",
      "166.0,0.68072,0.80147,0.73077,0.11578,0.71429,0.73077,0.72243\n",
      "224.0,0.74222,0.82653,0.62332,0.16285,0.75543,0.62613,0.68473\n",
      "199.0,0.70500,0.79630,0.61111,0.13104,0.71176,0.61735,0.66120\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[200.0, 0.6951843044335161, 0.8059456858960296, 0.6649594770313512, 0.1376590330788804, 0.7082020556182624, 0.6701321917949553, 0.6859676330538662]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1000,0.6992031872509961,0.8026159334126041,0.6621923937360179,0.688295165394402\n",
      "--- 506.10492753982544 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_06.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 7\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00446, 0.6079, 0.8846, 0.7206, 0.5968]\n",
      "e_2 * test rst: [0.00420, 0.6429, 0.8077, 0.7159, 0.6412]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00516, 0.6071, 0.5449, 0.5743, 0.5766]\n",
      "e_4 * test rst: [0.00420, 0.6425, 0.7949, 0.7106, 0.6540]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00447, 0.6287, 0.8141, 0.7095, 0.6334]\n",
      "e_6 * test rst: [0.00441, 0.6262, 0.8269, 0.7127, 0.6153]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00453, 0.6302, 0.7756, 0.6954, 0.6210]\n",
      "e_8 * test rst: [0.00468, 0.6281, 0.8013, 0.7042, 0.6104]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00454, 0.6230, 0.7308, 0.6726, 0.6414]\n",
      "e_10 * test rst: [0.00461, 0.6269, 0.7756, 0.6934, 0.6317]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.84 s\n",
      "251,0.63745,0.73980,0.77070,0.13486,0.62694,0.77564,0.69341\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00406, 0.6883, 0.8689, 0.7681, 0.6004]\n",
      "e_2 * test rst: [0.00396, 0.7035, 0.7650, 0.7330, 0.6619]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00406, 0.6916, 0.8087, 0.7456, 0.6346]\n",
      "e_4 * test rst: [0.00435, 0.7225, 0.6831, 0.7022, 0.6547]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00399, 0.7110, 0.8470, 0.7731, 0.6610]\n",
      "e_6 * test rst: [0.00396, 0.7070, 0.8306, 0.7638, 0.6705]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00405, 0.7212, 0.8197, 0.7673, 0.6672]\n",
      "e_8 * test rst: [0.00430, 0.7167, 0.7049, 0.7107, 0.6646]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00456, 0.7143, 0.5464, 0.6192, 0.6634]\n",
      "e_10 * test rst: [0.00426, 0.7191, 0.6995, 0.7091, 0.6676]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.24 s\n",
      "210,0.69340,0.80000,0.68817,0.15394,0.71910,0.69945,0.70914\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00375, 0.6420, 0.8000, 0.7123, 0.7133]\n",
      "e_2 * test rst: [0.00375, 0.7459, 0.7000, 0.7222, 0.7306]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00362, 0.6959, 0.7923, 0.7410, 0.7342]\n",
      "e_4 * test rst: [0.00349, 0.7152, 0.8308, 0.7687, 0.7608]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00346, 0.7391, 0.7846, 0.7612, 0.7609]\n",
      "e_6 * test rst: [0.00365, 0.7248, 0.8308, 0.7742, 0.7387]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00361, 0.7463, 0.7692, 0.7576, 0.7578]\n",
      "e_8 * test rst: [0.00360, 0.7500, 0.7615, 0.7557, 0.7655]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00355, 0.7465, 0.8154, 0.7794, 0.7618]\n",
      "e_10 * test rst: [0.00354, 0.7518, 0.8154, 0.7823, 0.7649]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.22 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181,0.70166,0.75155,0.81538,0.12723,0.75177,0.81538,0.78229\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00394, 0.6880, 0.7748, 0.7288, 0.7156]\n",
      "e_2 * test rst: [0.00460, 0.7829, 0.4550, 0.5755, 0.7066]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00424, 0.7857, 0.5450, 0.6436, 0.6988]\n",
      "e_4 * test rst: [0.00405, 0.7588, 0.6802, 0.7173, 0.7220]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00372, 0.6958, 0.8243, 0.7546, 0.7484]\n",
      "e_6 * test rst: [0.00370, 0.7500, 0.7568, 0.7534, 0.7599]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00375, 0.7548, 0.7072, 0.7302, 0.7594]\n",
      "e_8 * test rst: [0.00371, 0.7566, 0.7703, 0.7634, 0.7664]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00372, 0.7545, 0.7613, 0.7578, 0.7648]\n",
      "e_10 * test rst: [0.00374, 0.7568, 0.7568, 0.7568, 0.7641]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 90.00 s\n",
      "251,0.73810,0.81860,0.75336,0.19338,0.75676,0.75676,0.75676\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00481, 0.6164, 0.7296, 0.6682, 0.5960]\n",
      "e_2 * test rst: [0.00507, 0.7467, 0.5714, 0.6474, 0.6405]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00479, 0.7333, 0.6173, 0.6704, 0.6778]\n",
      "e_4 * test rst: [0.00474, 0.6436, 0.9490, 0.7670, 0.6525]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00452, 0.6732, 0.8827, 0.7638, 0.6823]\n",
      "e_6 * test rst: [0.00457, 0.6789, 0.8520, 0.7557, 0.6789]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00460, 0.6743, 0.7500, 0.7101, 0.6829]\n",
      "e_8 * test rst: [0.00470, 0.6571, 0.8214, 0.7302, 0.6714]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00472, 0.6622, 0.7602, 0.7078, 0.6765]\n",
      "e_10 * test rst: [0.00482, 0.6893, 0.7245, 0.7065, 0.6836]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.34 s\n",
      "223,0.69643,0.84932,0.71717,0.13232,0.68932,0.72449,0.70647\n",
      "*********\n",
      "CV end\n",
      "251.0,0.63745,0.73980,0.77070,0.13486,0.62694,0.77564,0.69341\n",
      "210.0,0.69340,0.80000,0.68817,0.15394,0.71910,0.69945,0.70914\n",
      "181.0,0.70166,0.75155,0.81538,0.12723,0.75177,0.81538,0.78229\n",
      "251.0,0.73810,0.81860,0.75336,0.19338,0.75676,0.75676,0.75676\n",
      "223.0,0.69643,0.84932,0.71717,0.13232,0.68932,0.72449,0.70647\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[223.2, 0.6934055387411254, 0.7918536866108689, 0.7489584482418624, 0.14834605597964373, 0.7087788647057028, 0.7543451491226667, 0.7296126515370197]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1116,0.6928571428571428,0.7907488986784141,0.743847874720358,0.7417302798982188\n",
      "--- 506.3850679397583 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_07.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 8\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00431, 0.6056, 0.8269, 0.6992, 0.6225]\n",
      "e_2 * test rst: [0.00440, 0.5931, 0.7756, 0.6722, 0.6016]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00444, 0.6150, 0.7885, 0.6910, 0.6272]\n",
      "e_4 * test rst: [0.00426, 0.6798, 0.7756, 0.7246, 0.6617]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00454, 0.6605, 0.6859, 0.6730, 0.6373]\n",
      "e_6 * test rst: [0.00465, 0.6303, 0.6667, 0.6480, 0.6256]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00462, 0.6536, 0.6410, 0.6472, 0.6338]\n",
      "e_8 * test rst: [0.00495, 0.6242, 0.6282, 0.6262, 0.6093]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00501, 0.6266, 0.6346, 0.6306, 0.6042]\n",
      "e_10 * test rst: [0.00496, 0.6242, 0.6603, 0.6417, 0.6077]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.81 s\n",
      "236,0.61864,0.71429,0.65605,0.11069,0.62424,0.66026,0.64174\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00426, 0.6517, 0.7158, 0.6823, 0.5678]\n",
      "e_2 * test rst: [0.00557, 0.6738, 0.5191, 0.5864, 0.5330]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00499, 0.6316, 0.6557, 0.6434, 0.5357]\n",
      "e_4 * test rst: [0.00459, 0.6748, 0.6011, 0.6358, 0.5965]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00501, 0.6667, 0.5246, 0.5872, 0.5996]\n",
      "e_6 * test rst: [0.00517, 0.6818, 0.4098, 0.5119, 0.6022]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00530, 0.6789, 0.4044, 0.5068, 0.6096]\n",
      "e_8 * test rst: [0.00497, 0.6917, 0.5027, 0.5823, 0.6210]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00529, 0.6719, 0.4699, 0.5531, 0.5966]\n",
      "e_10 * test rst: [0.00542, 0.6639, 0.4426, 0.5311, 0.5991]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.05 s\n",
      "134,0.66176,0.82353,0.43548,0.09796,0.66393,0.44262,0.53115\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00383, 0.6623, 0.7692, 0.7117, 0.7068]\n",
      "e_2 * test rst: [0.00358, 0.7360, 0.7077, 0.7216, 0.7462]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00351, 0.7034, 0.7846, 0.7418, 0.7532]\n",
      "e_4 * test rst: [0.00376, 0.7460, 0.7231, 0.7344, 0.7534]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00402, 0.7913, 0.7000, 0.7429, 0.7523]\n",
      "e_6 * test rst: [0.00333, 0.7357, 0.7923, 0.7630, 0.7890]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00352, 0.7519, 0.7692, 0.7605, 0.7708]\n",
      "e_8 * test rst: [0.00367, 0.7797, 0.7077, 0.7419, 0.7688]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00342, 0.7554, 0.8077, 0.7807, 0.7829]\n",
      "e_10 * test rst: [0.00349, 0.7717, 0.7538, 0.7626, 0.7793]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.16 s\n",
      "189,0.67196,0.73333,0.75385,0.11705,0.77165,0.75385,0.76265\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00432, 0.7158, 0.5901, 0.6469, 0.6691]\n",
      "e_2 * test rst: [0.00447, 0.7333, 0.5946, 0.6567, 0.6817]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00418, 0.7379, 0.6847, 0.7103, 0.7108]\n",
      "e_4 * test rst: [0.00402, 0.7137, 0.7297, 0.7216, 0.7104]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00416, 0.6703, 0.8333, 0.7430, 0.6976]\n",
      "e_6 * test rst: [0.00409, 0.7186, 0.7477, 0.7329, 0.7165]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00429, 0.7163, 0.6712, 0.6930, 0.6913]\n",
      "e_8 * test rst: [0.00393, 0.7511, 0.7748, 0.7627, 0.7250]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00396, 0.7403, 0.7703, 0.7550, 0.7218]\n",
      "e_10 * test rst: [0.00396, 0.7273, 0.8288, 0.7747, 0.7238]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.55 s\n",
      "295,0.69257,0.78099,0.82511,0.20611,0.72727,0.82883,0.77474\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00478, 0.6043, 0.7245, 0.6589, 0.5587]\n",
      "e_2 * test rst: [0.00513, 0.7405, 0.4949, 0.5933, 0.6450]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00487, 0.7045, 0.6327, 0.6667, 0.6420]\n",
      "e_4 * test rst: [0.00439, 0.6786, 0.8724, 0.7634, 0.6887]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00459, 0.6557, 0.8163, 0.7273, 0.6684]\n",
      "e_6 * test rst: [0.00450, 0.6814, 0.7857, 0.7299, 0.6877]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00460, 0.7056, 0.7092, 0.7074, 0.6888]\n",
      "e_8 * test rst: [0.00472, 0.7348, 0.6786, 0.7056, 0.6891]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00461, 0.7035, 0.7143, 0.7089, 0.6936]\n",
      "e_10 * test rst: [0.00474, 0.7333, 0.6735, 0.7021, 0.6927]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.28 s\n",
      "191,0.72396,0.87302,0.66667,0.12341,0.73333,0.67347,0.70213\n",
      "*********\n",
      "CV end\n",
      "236.0,0.61864,0.71429,0.65605,0.11069,0.62424,0.66026,0.64174\n",
      "134.0,0.66176,0.82353,0.43548,0.09796,0.66393,0.44262,0.53115\n",
      "189.0,0.67196,0.73333,0.75385,0.11705,0.77165,0.75385,0.76265\n",
      "295.0,0.69257,0.78099,0.82511,0.20611,0.72727,0.82883,0.77474\n",
      "191.0,0.72396,0.87302,0.66667,0.12341,0.73333,0.67347,0.70213\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[209.0, 0.6737784693075073, 0.7850312135873633, 0.6674319509035787, 0.13104325699745548, 0.7040872908770159, 0.6718047463012333, 0.6824805010693658]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1045,0.6739752144899904,0.7769784172661871,0.668903803131991,0.6552162849872774\n",
      "--- 505.69505548477173 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_08.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 9\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00437, 0.6327, 0.7949, 0.7045, 0.6064]\n",
      "e_2 * test rst: [0.00433, 0.6599, 0.8333, 0.7365, 0.6311]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00425, 0.6719, 0.8269, 0.7414, 0.6405]\n",
      "e_4 * test rst: [0.00543, 0.6400, 0.6154, 0.6275, 0.6026]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00497, 0.6389, 0.5897, 0.6133, 0.6117]\n",
      "e_6 * test rst: [0.00511, 0.6250, 0.5769, 0.6000, 0.6105]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00432, 0.6667, 0.8333, 0.7407, 0.6376]\n",
      "e_8 * test rst: [0.00479, 0.6509, 0.7051, 0.6769, 0.6165]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00463, 0.6592, 0.7564, 0.7045, 0.6293]\n",
      "e_10 * test rst: [0.00437, 0.6650, 0.8397, 0.7422, 0.6451]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.94 s\n",
      "233,0.66094,0.79191,0.83439,0.14504,0.66497,0.83974,0.74221\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00417, 0.6715, 0.7596, 0.7128, 0.5823]\n",
      "e_2 * test rst: [0.00402, 0.6961, 0.7760, 0.7339, 0.6557]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00376, 0.7106, 0.9126, 0.7990, 0.6686]\n",
      "e_4 * test rst: [0.00405, 0.6995, 0.8142, 0.7525, 0.6504]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00389, 0.7056, 0.8907, 0.7874, 0.6611]\n",
      "e_6 * test rst: [0.00397, 0.7156, 0.8251, 0.7665, 0.6700]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00407, 0.7075, 0.8197, 0.7595, 0.6677]\n",
      "e_8 * test rst: [0.00429, 0.7039, 0.7923, 0.7455, 0.6652]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00431, 0.7079, 0.7814, 0.7429, 0.6664]\n",
      "e_10 * test rst: [0.00413, 0.6991, 0.8251, 0.7569, 0.6706]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.71 s\n",
      "276,0.65108,0.73790,0.81183,0.17939,0.69907,0.82514,0.75689\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00390, 0.6296, 0.7846, 0.6986, 0.6572]\n",
      "e_2 * test rst: [0.00427, 0.6607, 0.5692, 0.6116, 0.6517]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00431, 0.7143, 0.5769, 0.6383, 0.6974]\n",
      "e_4 * test rst: [0.00392, 0.7055, 0.7923, 0.7464, 0.6971]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00386, 0.7014, 0.7769, 0.7372, 0.7253]\n",
      "e_6 * test rst: [0.00394, 0.7034, 0.7846, 0.7418, 0.7225]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00386, 0.7063, 0.7769, 0.7399, 0.7357]\n",
      "e_8 * test rst: [0.00389, 0.6986, 0.7846, 0.7391, 0.7366]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00385, 0.7185, 0.7462, 0.7321, 0.7434]\n",
      "e_10 * test rst: [0.00385, 0.7063, 0.7769, 0.7399, 0.7437]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.84 s\n",
      "173,0.68786,0.76871,0.77692,0.12087,0.70629,0.77692,0.73993\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00396, 0.6421, 0.8649, 0.7370, 0.7180]\n",
      "e_2 * test rst: [0.00380, 0.7273, 0.7928, 0.7586, 0.7336]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00390, 0.7766, 0.6892, 0.7303, 0.7442]\n",
      "e_4 * test rst: [0.00402, 0.7845, 0.6396, 0.7047, 0.7473]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00359, 0.7437, 0.7973, 0.7696, 0.7764]\n",
      "e_6 * test rst: [0.00362, 0.7448, 0.8018, 0.7722, 0.7843]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00367, 0.7773, 0.7703, 0.7738, 0.7836]\n",
      "e_8 * test rst: [0.00371, 0.7814, 0.7568, 0.7689, 0.7843]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00374, 0.7808, 0.7703, 0.7755, 0.7853]\n",
      "e_10 * test rst: [0.00377, 0.7870, 0.7658, 0.7763, 0.7878]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.49 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242,0.76955,0.86408,0.76233,0.19720,0.78704,0.76577,0.77626\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00479, 0.6104, 0.7194, 0.6604, 0.5973]\n",
      "e_2 * test rst: [0.00467, 0.6298, 0.8418, 0.7205, 0.5861]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00468, 0.6613, 0.8367, 0.7387, 0.6426]\n",
      "e_4 * test rst: [0.00473, 0.6682, 0.7194, 0.6929, 0.6625]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00486, 0.7043, 0.6684, 0.6859, 0.6749]\n",
      "e_6 * test rst: [0.00461, 0.6654, 0.9133, 0.7699, 0.6923]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00471, 0.6919, 0.6990, 0.6954, 0.6884]\n",
      "e_8 * test rst: [0.00479, 0.6939, 0.6939, 0.6939, 0.6918]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00493, 0.7021, 0.6735, 0.6875, 0.6906]\n",
      "e_10 * test rst: [0.00489, 0.6884, 0.6990, 0.6937, 0.6939]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 91.79 s\n",
      "217,0.67890,0.82394,0.69192,0.12723,0.68844,0.69898,0.69367\n",
      "*********\n",
      "CV end\n",
      "233.0,0.66094,0.79191,0.83439,0.14504,0.66497,0.83974,0.74221\n",
      "276.0,0.65108,0.73790,0.81183,0.17939,0.69907,0.82514,0.75689\n",
      "173.0,0.68786,0.76871,0.77692,0.12087,0.70629,0.77692,0.73993\n",
      "242.0,0.76955,0.86408,0.76233,0.19720,0.78704,0.76577,0.77626\n",
      "217.0,0.67890,0.82394,0.69192,0.12723,0.68844,0.69898,0.69367\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[228.2, 0.6896662044094439, 0.7973079110250519, 0.7754793937710275, 0.15394402035623408, 0.7091643295498867, 0.781309727258205, 0.7417910392139466]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1141,0.6890829694323144,0.7947598253275109,0.7718120805369127,0.7697201017811705\n",
      "--- 506.71428775787354 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_09.tsv\n",
      "****\n",
      "begin training 10X RENET2 models 10\n",
      "\n",
      "CV run 1/5\n",
      "training dataset size 1438\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00433, 0.6172, 0.8269, 0.7068, 0.6195]\n",
      "e_2 * test rst: [0.00512, 0.6939, 0.4359, 0.5354, 0.6142]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00440, 0.6839, 0.6795, 0.6817, 0.6709]\n",
      "e_4 * test rst: [0.00427, 0.6390, 0.8397, 0.7258, 0.6571]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00438, 0.6807, 0.5192, 0.5891, 0.6647]\n",
      "e_6 * test rst: [0.00462, 0.6621, 0.6154, 0.6379, 0.6498]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00468, 0.6202, 0.5128, 0.5614, 0.6280]\n",
      "e_8 * test rst: [0.00449, 0.6716, 0.5769, 0.6207, 0.6560]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00456, 0.6597, 0.6090, 0.6333, 0.6405]\n",
      "e_10 * test rst: [0.00466, 0.6525, 0.4936, 0.5620, 0.6480]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 93.81 s\n",
      "145,0.66897,0.82857,0.49045,0.08524,0.65254,0.49359,0.56204\n",
      "*********\n",
      "CV run 2/5\n",
      "training dataset size 1434\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00424, 0.6974, 0.5792, 0.6328, 0.6159]\n",
      "e_2 * test rst: [0.00442, 0.6875, 0.7213, 0.7040, 0.6165]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00372, 0.7416, 0.8470, 0.7908, 0.6890]\n",
      "e_4 * test rst: [0.00364, 0.7285, 0.8798, 0.7970, 0.6909]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00411, 0.7128, 0.7596, 0.7354, 0.6603]\n",
      "e_6 * test rst: [0.00398, 0.7103, 0.8306, 0.7657, 0.6466]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00387, 0.7193, 0.8962, 0.7981, 0.6612]\n",
      "e_8 * test rst: [0.00414, 0.7286, 0.7923, 0.7592, 0.6638]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00417, 0.7216, 0.7650, 0.7427, 0.6633]\n",
      "e_10 * test rst: [0.00424, 0.7194, 0.7705, 0.7441, 0.6625]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 92.80 s\n",
      "263,0.65283,0.72951,0.75806,0.17048,0.71939,0.77049,0.74406\n",
      "*********\n",
      "CV run 3/5\n",
      "training dataset size 1481\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00396, 0.6507, 0.7308, 0.6884, 0.6779]\n",
      "e_2 * test rst: [0.00351, 0.7463, 0.7692, 0.7576, 0.7565]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00359, 0.7197, 0.7308, 0.7252, 0.7468]\n",
      "e_4 * test rst: [0.00358, 0.7424, 0.7538, 0.7481, 0.7492]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00371, 0.7826, 0.6923, 0.7347, 0.7472]\n",
      "e_6 * test rst: [0.00364, 0.7460, 0.7231, 0.7344, 0.7489]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00378, 0.7500, 0.6923, 0.7200, 0.7437]\n",
      "e_8 * test rst: [0.00371, 0.7583, 0.7000, 0.7280, 0.7504]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00375, 0.7521, 0.7000, 0.7251, 0.7489]\n",
      "e_10 * test rst: [0.00389, 0.7679, 0.6615, 0.7107, 0.7497]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 94.17 s\n",
      "135,0.71852,0.80672,0.66154,0.10814,0.76786,0.66154,0.71074\n",
      "*********\n",
      "CV run 4/5\n",
      "training dataset size 1343\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00415, 0.6766, 0.7162, 0.6958, 0.6734]\n",
      "e_2 * test rst: [0.00387, 0.7718, 0.7162, 0.7430, 0.7519]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00389, 0.7237, 0.7432, 0.7333, 0.7374]\n",
      "e_4 * test rst: [0.00374, 0.7011, 0.8243, 0.7578, 0.7407]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n",
      "e_5 * test rst: [0.00390, 0.6871, 0.8604, 0.7640, 0.7259]\n",
      "e_6 * test rst: [0.00369, 0.7752, 0.7613, 0.7682, 0.7729]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00373, 0.7302, 0.8288, 0.7764, 0.7635]\n",
      "e_8 * test rst: [0.00373, 0.7186, 0.7477, 0.7329, 0.7595]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00375, 0.7521, 0.7928, 0.7719, 0.7649]\n",
      "e_10 * test rst: [0.00372, 0.7479, 0.8018, 0.7739, 0.7678]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 89.74 s\n",
      "284,0.70175,0.77016,0.79821,0.20738,0.74790,0.80180,0.77391\n",
      "*********\n",
      "CV run 5/5\n",
      "training dataset size 1400\n",
      "training------\n",
      "training begin\n",
      "e_1 * test rst: [0.00506, 0.6725, 0.5867, 0.6267, 0.5741]\n",
      "e_2 * test rst: [0.00467, 0.6356, 0.8010, 0.7088, 0.6351]\n",
      "Epoch     2: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 4.0000e-04.\n",
      "e_3 * test rst: [0.00493, 0.6845, 0.6531, 0.6684, 0.6393]\n",
      "e_4 * test rst: [0.00463, 0.6816, 0.7755, 0.7255, 0.6740]\n",
      "Epoch     4: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch     4: reducing learning rate of group 1 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_5 * test rst: [0.00492, 0.7105, 0.6888, 0.6995, 0.6718]\n",
      "e_6 * test rst: [0.00504, 0.7193, 0.6276, 0.6703, 0.6729]\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 1.0000e-04.\n",
      "e_7 * test rst: [0.00505, 0.7198, 0.6684, 0.6931, 0.6762]\n",
      "e_8 * test rst: [0.00506, 0.7112, 0.6786, 0.6945, 0.6782]\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch     8: reducing learning rate of group 1 to 5.0000e-05.\n",
      "e_9 * test rst: [0.00486, 0.6938, 0.7398, 0.7160, 0.6780]\n",
      "e_10 * test rst: [0.00501, 0.7047, 0.6939, 0.6992, 0.6780]\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    10: reducing learning rate of group 1 to 2.5000e-05.\n",
      "training end, used 90.91 s\n",
      "204,0.71220,0.89394,0.68687,0.12977,0.70466,0.69388,0.69923\n",
      "*********\n",
      "CV end\n",
      "145.0,0.66897,0.82857,0.49045,0.08524,0.65254,0.49359,0.56204\n",
      "263.0,0.65283,0.72951,0.75806,0.17048,0.71939,0.77049,0.74406\n",
      "135.0,0.71852,0.80672,0.66154,0.10814,0.76786,0.66154,0.71074\n",
      "284.0,0.70175,0.77016,0.79821,0.20738,0.74790,0.80180,0.77391\n",
      "204.0,0.71220,0.89394,0.68687,0.12977,0.70466,0.69388,0.69923\n",
      "CV mean, positive #, precision, precision[-], recall, recall[-]\n",
      "[206.2, 0.690852746471055, 0.805780599726069, 0.6790247604871396, 0.14020356234096693, 0.7184699285879276, 0.6842598722458206, 0.6979985514127257]\n",
      "mlt train ended\n",
      "CV merge, positive #, precision, precision[-], recall, recall[-]\n",
      "1031,0.6888888888888889,0.7900943396226415,0.6912751677852349,0.7010178117048346\n",
      "--- 507.3390531539917 seconds ---\n",
      "rst at ../data/ft_data/cls_rst/ft_cv_dev_10.tsv\n",
      "--- 5066.3210735321045 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# RST = run_dev()\n",
    "\n",
    "all_RST = []\n",
    "\n",
    "for _i in range(1, 11):\n",
    "#     continue\n",
    "    print('****\\nbegin training 10X RENET2 models {}\\n'.format(_i))\n",
    "    RST = run_dev()\n",
    "    all_RST.append(RST)\n",
    "    mer_pd_rst = RST[1]\n",
    "#     cls_rst_file = os.path.join(args.raw_data_dir, \"ft_rst_ann_2nd_dev_exp_w%d_%02d.tsv\" % (IG_N, _i))\n",
    "\n",
    "    cls_rst_file = os.path.join(args.raw_data_dir, \"cls_rst/ft_cv_dev_%02d.tsv\" % (_i))\n",
    "    mer_pd_rst.to_csv(cls_rst_file, sep='\\t', index=False)\n",
    "    print('rst at {}'.format(cls_rst_file))\n",
    "    #break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10X #, positive #, precision, precision[-], recall, recall[-]\n",
      "1119.0,0.691006233303651,0.79,0.7662192393736018,0.7582697201017812\n",
      "1063.0,0.6710402999062793,0.7617411225658648,0.6845637583892618,0.683206106870229\n",
      "1023.0,0.6932814021421616,0.7897862232779097,0.6957494407158836,0.7010178117048346\n",
      "972.0,0.7141393442622951,0.8180677540777918,0.6834451901565995,0.6895674300254453\n",
      "1017.0,0.7110675808031341,0.8039906103286385,0.7136465324384788,0.7290076335877863\n",
      "1000.0,0.6992031872509961,0.8026159334126041,0.6621923937360179,0.688295165394402\n",
      "1116.0,0.6928571428571428,0.7907488986784141,0.743847874720358,0.7417302798982188\n",
      "1045.0,0.6739752144899904,0.7769784172661871,0.668903803131991,0.6552162849872774\n",
      "1141.0,0.6890829694323144,0.7947598253275109,0.7718120805369127,0.7697201017811705\n",
      "1031.0,0.6888888888888889,0.7900943396226415,0.6912751677852349,0.7010178117048346\n",
      "avg,1052.7,0.6924542263336855,0.7918783124557564,0.7081655480984341,0.711704834605598\n",
      "assemble, positive #, precision, precision[-], recall, recall[-]\n",
      "1,1731,0.6184438040345821,0.686382393397524,0.9004474272930649,0.8982188295165394\n",
      "2,1425,0.6522043386983905,0.7315208156329651,0.843400447427293,0.8384223918575063\n",
      "3,1294,0.6648690292758089,0.7521206409048068,0.8154362416107382,0.8142493638676844\n",
      "4,1161,0.6866952789699571,0.7801268498942917,0.785234899328859,0.7837150127226463\n",
      "5,1042,0.7122370936902486,0.8210023866348448,0.7449664429530202,0.7442748091603053\n",
      "6,971,0.7241025641025641,0.8414948453608248,0.7080536912751678,0.7073791348600509\n",
      "7,885,0.7322834645669292,0.8581560283687943,0.6633109619686801,0.6653944020356234\n",
      "8,793,0.7465495608531995,0.879746835443038,0.6152125279642058,0.6195928753180662\n",
      "9,693,0.7618364418938307,0.8947368421052632,0.558165548098434,0.5776081424936387\n",
      "10,532,0.7798507462686567,0.9070796460176991,0.44742729306487694,0.4681933842239186\n",
      "selected assemble models at 1+, shape (13641, 4), result:\n",
      "(1731, 0.6184438040345821, 0.686382393397524, 0.9004474272930649, 0.8982188295165394)\n"
     ]
    }
   ],
   "source": [
    "m_df = 0\n",
    "s_arr = []\n",
    "for _i in range(10):\n",
    "    cls_file = os.path.join(args.raw_data_dir, \"cls_rst/ft_cv_dev_%02d.tsv\" % (_i+1))\n",
    "\n",
    "    y_info = pd.read_csv(cls_file, sep='\\t')\n",
    "    y_info['pmid'] = y_info['pmid'].astype(str)\n",
    "    y_info['geneId'] = y_info['geneId'].astype(str)\n",
    "    y_info['diseaseId'] = y_info['diseaseId'].astype(str)\n",
    "    y_info['label'] = y_info['label'].astype(str)\n",
    "\n",
    "    S = evaluate_rst_all_info_err(y_info, 'pred', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "#     print(S)\n",
    "    s_arr.append(list(S))\n",
    "\n",
    "    y_info.rename(columns={'prob':'prob_%02d'%(_i+1)}, inplace=True)\n",
    "    y_info.rename(columns={'pred':'pred_%02d'%(_i+1)}, inplace=True)\n",
    "#     y_info = y_info[y_info['pred'] == 1]\n",
    "    m_df = y_info if _i == 0 else m_df.merge(y_info, on=['pmid', 'geneId','diseaseId', 'label'], how='outer')\n",
    "\n",
    "\n",
    "m_df['hit_cnt'] = m_df.apply(lambda x: sum([x['pred_%02d'%(_i+1)] for _i in range(10)]), axis=1)\n",
    "\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "print('10X #, positive #, precision, precision[-], recall, recall[-]')\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "#print(list(np.mean(s_arr, axis=0)))\n",
    "\n",
    "\n",
    "avg_l = list(np.mean(s_arr, axis=0))\n",
    "avg_l = ['avg'] + avg_l\n",
    "\n",
    "print(','.join(list(map(str, avg_l))))\n",
    "\n",
    "print('assemble, positive #, precision, precision[-], recall, recall[-]')\n",
    "for _i in range(1, 11):\n",
    "    ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "    ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= _i else 0, axis=1)\n",
    "\n",
    "    _rst = evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner)\n",
    "    _rst = [_i] + list(_rst)\n",
    "    print(','.join(list(map(str, _rst))))\n",
    "\n",
    "the_p_cnt = 1\n",
    "ReNet_df = m_df[['pmid', 'geneId', 'diseaseId']].copy()\n",
    "ReNet_df['is_renet'] = m_df.apply(lambda x: 1 if x['hit_cnt'] >= the_p_cnt else 0, axis=1)\n",
    "\n",
    "print('selected assemble models at {}+, shape {}, result:'.format(the_p_cnt, ReNet_df.shape))\n",
    "print(evaluate_rst_all_info_err(ReNet_df, 'is_renet', abs_s_df, _abs_s_df, abs_s_df_ner))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/ft_data/'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.raw_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s ../data/abs_data/ori_test/cached_all_doc_0_32_54\n",
      "loading ended\n",
      "creading dataset, positive GDA 1338, all GDA 4729, positive rate 28.29%\n"
     ]
    }
   ],
   "source": [
    "args.raw_data_dir = \"../data/abs_data/ori_test/\"\n",
    "args.label_f_name = \"labels.txt\"\n",
    "\n",
    "    \n",
    "features_ori_t = load_and_cache_data(args)\n",
    "\n",
    "dataset_ori_t, _, _ = convert_features_to_dataset_single(features_ori_t)\n",
    "dataloader_ori_t = DataLoader(dataset_ori_t, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 4,\n",
      " 'l2_weight_decay': 5e-06,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 1e-07,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "init model\n",
      "training begin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e6d538bd874800a1ff47f99c69aae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4353.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5ddcb0b34b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtrain_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_dataloader_rm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader_rm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_ori_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/bal31/jhsu/home/projects/RENET2/renet2/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, dev_dataloader, args, test_dataloader, save_model)\u001b[0m\n\u001b[1;32m    801\u001b[0m                       \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                       'scheduler': scheduler}\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved checkpoint in %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bal31/jhsu/home/anaconda3/envs/venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bal31/jhsu/home/anaconda3/envs/venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bal31/jhsu/home/anaconda3/envs/venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/bal31/jhsu/home/anaconda3/envs/venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optional training a base model on disgenet data\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "args.epochs = 4\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-7\n",
    "args.l2_weight_decay = 5e-6\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = False\n",
    "\n",
    "args.device = device\n",
    "\n",
    "args.modle_dir = '../models/abs_models'\n",
    "model_name_prefix = 'DisGeNet_abs'\n",
    "args.checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "args.config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "torch.save(config, args.config_save_f)\n",
    "\n",
    "model = Base_Net(config).to(device)\n",
    "model_init_w(model)\n",
    "optimizer, scheduler = init_model_optimizer(model, args)\n",
    "\n",
    "train_dt, dev_dt, test_dt = tr_dataloader_rm, dev_dataloader_rm, dataloader_ori_t\n",
    "\n",
    "_ = train(model, optimizer, scheduler, train_dt, dev_dt, args, test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s ../data/abs_data/1st_ann/cached_all_doc_0_32_54\n",
      "loading ended\n",
      "creading dataset, positive GDA 975, all GDA 2813, positive rate 34.68%\n"
     ]
    }
   ],
   "source": [
    "args.raw_data_dir = \"../data/abs_data/1st_ann/\"\n",
    "args.label_f_name = \"labels.txt\"\n",
    "    \n",
    "features_ann_1 = load_and_cache_data(args)\n",
    "\n",
    "dataset_ann_1, _, _ = convert_features_to_dataset_single(features_ann_1)\n",
    "dataloader_ann_1 = DataLoader(dataset_ann_1, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s ../data/abs_data/2nd_ann/cached_all_doc_0_32_54\n",
      "loading ended\n",
      "creading dataset, positive GDA 882, all GDA 2734, positive rate 32.28%\n"
     ]
    }
   ],
   "source": [
    "args.raw_data_dir = \"../data/abs_data/2nd_ann/\"\n",
    "args.label_f_name = \"labels.txt\"\n",
    "    \n",
    "features_ss_aug = load_and_cache_data(args)\n",
    "\n",
    "dataset_ss_aug, _, _ = convert_features_to_dataset_single(features_ss_aug)\n",
    "dataloader_ss_aug = DataLoader(dataset_ss_aug, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_merge = np.concatenate((features_ann_1[0], features_ss_aug[0]), axis=0), \\\n",
    "                pd.concat([features_ann_1[1], features_ss_aug[1]])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creading dataset, positive GDA 1858, all GDA 5547, positive rate 33.50%\n"
     ]
    }
   ],
   "source": [
    "dataset_merge, _, _ = convert_features_to_dataset_single(features_merge)\n",
    "dataloader_merge = DataLoader(dataset_merge, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512a675d7b1a406cb69265c702f80065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=88.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0.004669049611630317, 0.7086513994910941, 0.6402298850574712, 0.6727053140096618, 0.8448485870291822)\n"
     ]
    }
   ],
   "source": [
    "args.modle_dir = '../models/abs_models'\n",
    "model_name_prefix = 'DisGeNet_abs'\n",
    "checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "config = torch.load(config_save_f)\n",
    "model, _, _ = load_checkpoint(config, checkpoint_f)\n",
    "\n",
    "config.device =  args.device\n",
    "\n",
    "args.is_iterare_info = False\n",
    "args.threshold = config.threshold \n",
    "args.l2_weight_decay = config.l2_weight_decay\n",
    "model.update_model_config(config)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "pred_l, tru_l, S, pred_o = eval(model, dataloader_ann_1, args, 'test')\n",
    "_, _, _, f1, auc = S\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00431, 0.7246, 0.5882, 0.6494, 0.8418]\n",
      "e_2 * test rst: [0.00424, 0.8088, 0.6471, 0.7190, 0.8548]\n",
      "e_3 * test rst: [0.00451, 0.8387, 0.6118, 0.7075, 0.8594]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00439, 0.6649, 0.7353, 0.6983, 0.8652]\n",
      "e_5 * test rst: [0.00461, 0.8468, 0.6176, 0.7143, 0.8604]\n",
      "e_6 * test rst: [0.00489, 0.6382, 0.7471, 0.6883, 0.8656]\n",
      "e_7 * test rst: [0.00478, 0.7296, 0.6824, 0.7052, 0.8631]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00476, 0.7262, 0.7176, 0.7219, 0.8613]\n",
      "e_9 * test rst: [0.00493, 0.7857, 0.6471, 0.7097, 0.8611]\n",
      "e_10 * test rst: [0.00475, 0.7135, 0.7176, 0.7155, 0.8648]\n",
      "e_11 * test rst: [0.00502, 0.6632, 0.7412, 0.7000, 0.8660]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00516, 0.6632, 0.7412, 0.7000, 0.8645]\n",
      "e_13 * test rst: [0.00501, 0.7052, 0.7176, 0.7114, 0.8638]\n",
      "e_14 * test rst: [0.00528, 0.6649, 0.7471, 0.7036, 0.8638]\n",
      "e_15 * test rst: [0.00499, 0.7052, 0.7176, 0.7114, 0.8635]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00506, 0.6954, 0.7118, 0.7035, 0.8634]\n",
      "e_17 * test rst: [0.00522, 0.6614, 0.7353, 0.6964, 0.8646]\n",
      "e_18 * test rst: [0.00516, 0.6796, 0.7235, 0.7009, 0.8645]\n",
      "training end, used 303.05 s\n",
      "(0.0051615752054785325, 0.6795580110497238, 0.7235294117647059, 0.7008547008547009, 0.8644514294267325)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00504, 0.8018, 0.5235, 0.6335, 0.8668]\n",
      "e_2 * test rst: [0.00466, 0.8300, 0.4882, 0.6148, 0.8780]\n",
      "e_3 * test rst: [0.00457, 0.6882, 0.6882, 0.6882, 0.8771]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00435, 0.6964, 0.6882, 0.6923, 0.8783]\n",
      "e_5 * test rst: [0.00467, 0.7450, 0.6529, 0.6959, 0.8831]\n",
      "e_6 * test rst: [0.00464, 0.6923, 0.6882, 0.6903, 0.8836]\n",
      "e_7 * test rst: [0.00475, 0.7290, 0.6647, 0.6954, 0.8878]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00480, 0.7233, 0.6765, 0.6991, 0.8884]\n",
      "e_9 * test rst: [0.00481, 0.6879, 0.7000, 0.6939, 0.8888]\n",
      "e_10 * test rst: [0.00488, 0.7205, 0.6824, 0.7009, 0.8904]\n",
      "e_11 * test rst: [0.00474, 0.6932, 0.7176, 0.7052, 0.8925]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00470, 0.7186, 0.7059, 0.7122, 0.8925]\n",
      "e_13 * test rst: [0.00476, 0.7069, 0.7235, 0.7151, 0.8943]\n",
      "e_14 * test rst: [0.00471, 0.7580, 0.7000, 0.7278, 0.8947]\n",
      "e_15 * test rst: [0.00467, 0.7628, 0.7000, 0.7301, 0.8944]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00471, 0.7516, 0.6941, 0.7217, 0.8942]\n",
      "e_17 * test rst: [0.00467, 0.7289, 0.7118, 0.7202, 0.8955]\n",
      "e_18 * test rst: [0.00476, 0.7186, 0.7059, 0.7122, 0.8947]\n",
      "training end, used 304.68 s\n",
      "(0.004759230689008325, 0.718562874251497, 0.7058823529411765, 0.712166172106825, 0.8947013920071847)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00562, 0.7760, 0.4949, 0.6044, 0.8318]\n",
      "e_2 * test rst: [0.00495, 0.6578, 0.7551, 0.7031, 0.8592]\n",
      "e_3 * test rst: [0.00472, 0.7472, 0.6786, 0.7112, 0.8613]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00472, 0.6881, 0.7653, 0.7246, 0.8624]\n",
      "e_5 * test rst: [0.00464, 0.7500, 0.6888, 0.7181, 0.8654]\n",
      "e_6 * test rst: [0.00491, 0.7627, 0.6888, 0.7239, 0.8678]\n",
      "e_7 * test rst: [0.00484, 0.6808, 0.7398, 0.7090, 0.8636]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00486, 0.6942, 0.7296, 0.7114, 0.8663]\n",
      "e_9 * test rst: [0.00514, 0.7120, 0.6939, 0.7028, 0.8654]\n",
      "e_10 * test rst: [0.00534, 0.6842, 0.7296, 0.7062, 0.8640]\n",
      "e_11 * test rst: [0.00513, 0.7312, 0.6939, 0.7120, 0.8694]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00523, 0.6948, 0.7551, 0.7237, 0.8670]\n",
      "e_13 * test rst: [0.00516, 0.7316, 0.7092, 0.7202, 0.8688]\n",
      "e_14 * test rst: [0.00519, 0.7340, 0.7041, 0.7188, 0.8689]\n",
      "e_15 * test rst: [0.00512, 0.7344, 0.7194, 0.7268, 0.8673]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00511, 0.7330, 0.7143, 0.7235, 0.8678]\n",
      "e_17 * test rst: [0.00513, 0.7214, 0.7398, 0.7305, 0.8683]\n",
      "e_18 * test rst: [0.00511, 0.7245, 0.7245, 0.7245, 0.8684]\n",
      "training end, used 309.44 s\n",
      "(0.005109578993032499, 0.7244897959183674, 0.7244897959183674, 0.7244897959183674, 0.8683617861313463)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "4985 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00508, 0.6855, 0.5030, 0.5802, 0.7954]\n",
      "e_2 * test rst: [0.00500, 0.6992, 0.5089, 0.5890, 0.8079]\n",
      "e_3 * test rst: [0.00610, 0.8462, 0.3905, 0.5344, 0.8075]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00543, 0.7478, 0.5089, 0.6056, 0.8252]\n",
      "e_5 * test rst: [0.00535, 0.6644, 0.5740, 0.6159, 0.8152]\n",
      "e_6 * test rst: [0.00576, 0.7063, 0.5266, 0.6034, 0.8213]\n",
      "e_7 * test rst: [0.00591, 0.8000, 0.4734, 0.5948, 0.8216]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00544, 0.6790, 0.6509, 0.6647, 0.8329]\n",
      "e_9 * test rst: [0.00560, 0.6986, 0.6036, 0.6476, 0.8320]\n",
      "e_10 * test rst: [0.00537, 0.7447, 0.6213, 0.6774, 0.8394]\n",
      "e_11 * test rst: [0.00576, 0.7500, 0.5503, 0.6348, 0.8325]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00565, 0.7364, 0.5621, 0.6376, 0.8337]\n",
      "e_13 * test rst: [0.00582, 0.7603, 0.5444, 0.6345, 0.8330]\n",
      "e_14 * test rst: [0.00560, 0.7313, 0.5799, 0.6469, 0.8323]\n",
      "e_15 * test rst: [0.00561, 0.7143, 0.6213, 0.6646, 0.8326]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00584, 0.7027, 0.6154, 0.6562, 0.8312]\n",
      "e_17 * test rst: [0.00570, 0.7266, 0.5976, 0.6558, 0.8336]\n",
      "e_18 * test rst: [0.00576, 0.7239, 0.5740, 0.6403, 0.8347]\n",
      "training end, used 311.99 s\n",
      "(0.00575801624097858, 0.7238805970149254, 0.5739644970414202, 0.6402640264026402, 0.8346507671228751)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "4985 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00432, 0.7982, 0.5515, 0.6523, 0.8396]\n",
      "e_2 * test rst: [0.00430, 0.8257, 0.5455, 0.6569, 0.8535]\n",
      "e_3 * test rst: [0.00398, 0.7969, 0.6182, 0.6962, 0.8627]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00432, 0.7338, 0.6848, 0.7085, 0.8507]\n",
      "e_5 * test rst: [0.00428, 0.7500, 0.6909, 0.7192, 0.8554]\n",
      "e_6 * test rst: [0.00499, 0.6443, 0.7576, 0.6964, 0.8592]\n",
      "e_7 * test rst: [0.00452, 0.7535, 0.6485, 0.6971, 0.8544]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00440, 0.7534, 0.6667, 0.7074, 0.8571]\n",
      "e_9 * test rst: [0.00452, 0.7169, 0.7212, 0.7190, 0.8571]\n",
      "e_10 * test rst: [0.00456, 0.7250, 0.7030, 0.7138, 0.8567]\n",
      "e_11 * test rst: [0.00464, 0.7417, 0.6788, 0.7089, 0.8505]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00478, 0.7652, 0.6121, 0.6801, 0.8441]\n",
      "e_13 * test rst: [0.00464, 0.7169, 0.7212, 0.7190, 0.8563]\n",
      "e_14 * test rst: [0.00466, 0.7285, 0.6667, 0.6962, 0.8496]\n",
      "e_15 * test rst: [0.00457, 0.7358, 0.7091, 0.7222, 0.8513]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00466, 0.7329, 0.7152, 0.7239, 0.8509]\n",
      "e_17 * test rst: [0.00467, 0.7450, 0.6727, 0.7070, 0.8494]\n",
      "e_18 * test rst: [0.00498, 0.7153, 0.6242, 0.6667, 0.8463]\n",
      "training end, used 309.54 s\n",
      "(0.004978535981895236, 0.7152777777777778, 0.6242424242424243, 0.6666666666666666, 0.8463476070528967)\n",
      "total label sum 184.5\n",
      "0.0051615752054785325,0.6795580110497238,0.7235294117647059,0.7008547008547009,0.8644514294267325\n",
      "0.004759230689008325,0.718562874251497,0.7058823529411765,0.712166172106825,0.8947013920071847\n",
      "0.005109578993032499,0.7244897959183674,0.7244897959183674,0.7244897959183674,0.8683617861313463\n",
      "0.00575801624097858,0.7238805970149254,0.5739644970414202,0.6402640264026402,0.8346507671228751\n",
      "0.004978535981895236,0.7152777777777778,0.6242424242424243,0.6666666666666666,0.8463476070528967\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.005153387422078636, 0.7123538112024582, 0.6704216963816189, 0.6888882723898401, 0.8617025963482071]\n",
      "mlt train ended\n",
      "0.711679,0.672414,0.691489,0.856749\n"
     ]
    }
   ],
   "source": [
    "args.no_ambiguous_label = True\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.l2_weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv_aug(tar_feature, train_dev_idx, features_ss_aug), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00428, 0.7615, 0.5824, 0.6600, 0.8577]\n",
      "e_2 * test rst: [0.00434, 0.6667, 0.6941, 0.6801, 0.8583]\n",
      "e_3 * test rst: [0.00426, 0.8443, 0.6059, 0.7055, 0.8679]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00459, 0.8333, 0.6176, 0.7095, 0.8705]\n",
      "e_5 * test rst: [0.00438, 0.6736, 0.7647, 0.7163, 0.8676]\n",
      "e_6 * test rst: [0.00434, 0.7468, 0.6765, 0.7099, 0.8651]\n",
      "e_7 * test rst: [0.00445, 0.7102, 0.7353, 0.7225, 0.8687]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00458, 0.7151, 0.7235, 0.7193, 0.8688]\n",
      "e_9 * test rst: [0.00468, 0.7229, 0.7059, 0.7143, 0.8662]\n",
      "e_10 * test rst: [0.00471, 0.7186, 0.7059, 0.7122, 0.8633]\n",
      "e_11 * test rst: [0.00476, 0.6995, 0.7529, 0.7252, 0.8630]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00471, 0.7143, 0.7353, 0.7246, 0.8638]\n",
      "e_13 * test rst: [0.00468, 0.7566, 0.6765, 0.7143, 0.8651]\n",
      "e_14 * test rst: [0.00466, 0.6940, 0.7471, 0.7195, 0.8678]\n",
      "e_15 * test rst: [0.00471, 0.7468, 0.6941, 0.7195, 0.8643]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00466, 0.7613, 0.6941, 0.7262, 0.8639]\n",
      "e_17 * test rst: [0.00463, 0.7305, 0.7176, 0.7240, 0.8646]\n",
      "e_18 * test rst: [0.00464, 0.7209, 0.7294, 0.7251, 0.8654]\n",
      "training end, used 325.32 s\n",
      "(0.004637554890044417, 0.7209302325581395, 0.7294117647058823, 0.7251461988304093, 0.8653944020356235)\n",
      "total label sum 197.5\n",
      "0.004637554890044417,0.7209302325581395,0.7294117647058823,0.7251461988304093,0.8653944020356235\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.004637554890044417, 0.7209302325581395, 0.7294117647058823, 0.7251461988304093, 0.8653944020356235]\n",
      "mlt train ended\n",
      "0.720930,0.729412,0.725146,0.865394\n"
     ]
    }
   ],
   "source": [
    "args.no_ambiguous_label = True\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.l2_weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv_aug(tar_feature, train_dev_idx, features_ss_aug), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    break\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.no_ambiguous_label = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00409, 0.7118, 0.7118, 0.7118, 0.8532]\n",
      "e_2 * test rst: [0.00407, 0.7535, 0.6294, 0.6859, 0.8575]\n",
      "e_3 * test rst: [0.00388, 0.7241, 0.7412, 0.7326, 0.8707]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00405, 0.7902, 0.6647, 0.7220, 0.8663]\n",
      "e_5 * test rst: [0.00404, 0.7310, 0.7353, 0.7331, 0.8664]\n",
      "e_6 * test rst: [0.00433, 0.8058, 0.6588, 0.7249, 0.8652]\n",
      "e_7 * test rst: [0.00423, 0.7610, 0.7118, 0.7356, 0.8645]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00415, 0.7225, 0.7353, 0.7289, 0.8676]\n",
      "e_9 * test rst: [0.00426, 0.6717, 0.7824, 0.7228, 0.8724]\n",
      "e_10 * test rst: [0.00422, 0.7111, 0.7529, 0.7314, 0.8710]\n",
      "e_11 * test rst: [0.00436, 0.7625, 0.7176, 0.7394, 0.8685]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00428, 0.7305, 0.7176, 0.7240, 0.8681]\n",
      "e_13 * test rst: [0.00436, 0.7273, 0.7529, 0.7399, 0.8683]\n",
      "e_14 * test rst: [0.00436, 0.7011, 0.7588, 0.7288, 0.8709]\n",
      "e_15 * test rst: [0.00427, 0.7396, 0.7353, 0.7375, 0.8703]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00431, 0.7530, 0.7353, 0.7440, 0.8702]\n",
      "e_17 * test rst: [0.00430, 0.7299, 0.7471, 0.7384, 0.8708]\n",
      "e_18 * test rst: [0.00432, 0.7299, 0.7471, 0.7384, 0.8708]\n",
      "training end, used 310.04 s\n",
      "(0.004320633566241916, 0.7298850574712644, 0.7470588235294118, 0.7383720930232558, 0.8708277204011374)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00443, 0.7188, 0.6765, 0.6970, 0.8691]\n",
      "e_2 * test rst: [0.00457, 0.7308, 0.6706, 0.6994, 0.8737]\n",
      "e_3 * test rst: [0.00482, 0.6345, 0.7353, 0.6812, 0.8762]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00469, 0.6760, 0.7118, 0.6934, 0.8753]\n",
      "e_5 * test rst: [0.00448, 0.7317, 0.7059, 0.7186, 0.8850]\n",
      "e_6 * test rst: [0.00434, 0.7186, 0.7059, 0.7122, 0.8888]\n",
      "e_7 * test rst: [0.00446, 0.6816, 0.7176, 0.6991, 0.8883]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00448, 0.7676, 0.6412, 0.6987, 0.8912]\n",
      "e_9 * test rst: [0.00439, 0.6632, 0.7412, 0.7000, 0.8958]\n",
      "e_10 * test rst: [0.00435, 0.7438, 0.7000, 0.7212, 0.8934]\n",
      "e_11 * test rst: [0.00445, 0.7312, 0.6882, 0.7091, 0.8916]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00441, 0.7273, 0.7059, 0.7164, 0.8947]\n",
      "e_13 * test rst: [0.00436, 0.7389, 0.6824, 0.7095, 0.8957]\n",
      "e_14 * test rst: [0.00444, 0.7000, 0.7412, 0.7200, 0.8977]\n",
      "e_15 * test rst: [0.00444, 0.6961, 0.7412, 0.7179, 0.8966]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00438, 0.7126, 0.7294, 0.7209, 0.8965]\n",
      "e_17 * test rst: [0.00442, 0.7006, 0.7294, 0.7147, 0.8968]\n",
      "e_18 * test rst: [0.00445, 0.7358, 0.6882, 0.7112, 0.8944]\n",
      "training end, used 311.17 s\n",
      "(0.004448002770040218, 0.7358490566037735, 0.6882352941176471, 0.7112462006079028, 0.8943571321658433)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "4984 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00488, 0.6524, 0.6990, 0.6749, 0.8398]\n",
      "e_2 * test rst: [0.00432, 0.7337, 0.6888, 0.7105, 0.8545]\n",
      "e_3 * test rst: [0.00475, 0.7059, 0.6735, 0.6893, 0.8500]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00472, 0.6833, 0.7704, 0.7242, 0.8572]\n",
      "e_5 * test rst: [0.00459, 0.7157, 0.7449, 0.7300, 0.8667]\n",
      "e_6 * test rst: [0.00471, 0.6981, 0.7551, 0.7255, 0.8683]\n",
      "e_7 * test rst: [0.00468, 0.7009, 0.7653, 0.7317, 0.8719]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00475, 0.7070, 0.7755, 0.7397, 0.8721]\n",
      "e_9 * test rst: [0.00471, 0.7268, 0.7194, 0.7231, 0.8761]\n",
      "e_10 * test rst: [0.00495, 0.6711, 0.7704, 0.7173, 0.8730]\n",
      "e_11 * test rst: [0.00489, 0.7030, 0.7245, 0.7136, 0.8745]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00497, 0.6898, 0.7602, 0.7233, 0.8743]\n",
      "e_13 * test rst: [0.00484, 0.6995, 0.7602, 0.7286, 0.8744]\n",
      "e_14 * test rst: [0.00482, 0.7150, 0.7296, 0.7222, 0.8754]\n",
      "e_15 * test rst: [0.00505, 0.6881, 0.7653, 0.7246, 0.8735]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00498, 0.6881, 0.7653, 0.7246, 0.8740]\n",
      "e_17 * test rst: [0.00490, 0.7028, 0.7602, 0.7304, 0.8753]\n",
      "e_18 * test rst: [0.00486, 0.6995, 0.7602, 0.7286, 0.8748]\n",
      "training end, used 313.16 s\n",
      "(0.004855950000707464, 0.6995305164319249, 0.7602040816326531, 0.7286063569682152, 0.874784518712117)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "4985 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00520, 0.6242, 0.5799, 0.6012, 0.7988]\n",
      "e_2 * test rst: [0.00530, 0.6250, 0.5621, 0.5919, 0.7980]\n",
      "e_3 * test rst: [0.00532, 0.6711, 0.5917, 0.6289, 0.8109]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00510, 0.6906, 0.5680, 0.6234, 0.8242]\n",
      "e_5 * test rst: [0.00528, 0.6358, 0.6509, 0.6433, 0.8300]\n",
      "e_6 * test rst: [0.00521, 0.7481, 0.5799, 0.6533, 0.8230]\n",
      "e_7 * test rst: [0.00510, 0.7344, 0.5562, 0.6330, 0.8291]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00498, 0.6948, 0.6331, 0.6625, 0.8372]\n",
      "e_9 * test rst: [0.00517, 0.6933, 0.6154, 0.6520, 0.8347]\n",
      "e_10 * test rst: [0.00532, 0.7557, 0.5858, 0.6600, 0.8357]\n",
      "e_11 * test rst: [0.00519, 0.6948, 0.6331, 0.6625, 0.8399]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00516, 0.6903, 0.6331, 0.6605, 0.8404]\n",
      "e_13 * test rst: [0.00519, 0.6937, 0.6568, 0.6748, 0.8405]\n",
      "e_14 * test rst: [0.00521, 0.6908, 0.6213, 0.6542, 0.8403]\n",
      "e_15 * test rst: [0.00510, 0.6879, 0.6391, 0.6626, 0.8419]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00516, 0.6894, 0.6568, 0.6727, 0.8427]\n",
      "e_17 * test rst: [0.00511, 0.6962, 0.6509, 0.6728, 0.8440]\n",
      "e_18 * test rst: [0.00522, 0.6855, 0.6450, 0.6646, 0.8412]\n",
      "training end, used 309.75 s\n",
      "(0.005221198952601049, 0.6855345911949685, 0.6449704142011834, 0.6646341463414633, 0.8412304078775011)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "4985 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00433, 0.7534, 0.6667, 0.7074, 0.8458]\n",
      "e_2 * test rst: [0.00404, 0.6940, 0.7697, 0.7299, 0.8727]\n",
      "e_3 * test rst: [0.00364, 0.7756, 0.7333, 0.7539, 0.8825]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00396, 0.8000, 0.6303, 0.7051, 0.8723]\n",
      "e_5 * test rst: [0.00452, 0.6238, 0.7939, 0.6987, 0.8817]\n",
      "e_6 * test rst: [0.00394, 0.7200, 0.7636, 0.7412, 0.8828]\n",
      "e_7 * test rst: [0.00428, 0.6919, 0.7758, 0.7314, 0.8811]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00409, 0.7267, 0.7576, 0.7418, 0.8825]\n",
      "e_9 * test rst: [0.00423, 0.7394, 0.7394, 0.7394, 0.8772]\n",
      "e_10 * test rst: [0.00432, 0.7151, 0.7455, 0.7300, 0.8801]\n",
      "e_11 * test rst: [0.00415, 0.7235, 0.7455, 0.7343, 0.8800]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00430, 0.6833, 0.7455, 0.7130, 0.8812]\n",
      "e_13 * test rst: [0.00410, 0.7531, 0.7394, 0.7462, 0.8764]\n",
      "e_14 * test rst: [0.00422, 0.7235, 0.7455, 0.7343, 0.8761]\n",
      "e_15 * test rst: [0.00417, 0.7365, 0.7455, 0.7410, 0.8764]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00447, 0.7022, 0.7576, 0.7289, 0.8756]\n",
      "e_17 * test rst: [0.00429, 0.7110, 0.7455, 0.7278, 0.8751]\n",
      "e_18 * test rst: [0.00432, 0.7151, 0.7455, 0.7300, 0.8741]\n",
      "training end, used 310.64 s\n",
      "(0.004323586516639092, 0.7151162790697675, 0.7454545454545455, 0.7299703264094957, 0.874085947637585)\n",
      "total label sum 184.5\n",
      "0.004320633566241916,0.7298850574712644,0.7470588235294118,0.7383720930232558,0.8708277204011374\n",
      "0.004448002770040218,0.7358490566037735,0.6882352941176471,0.7112462006079028,0.8943571321658433\n",
      "0.004855950000707464,0.6995305164319249,0.7602040816326531,0.7286063569682152,0.874784518712117\n",
      "0.005221198952601049,0.6855345911949685,0.6449704142011834,0.6646341463414633,0.8412304078775011\n",
      "0.004323586516639092,0.7151162790697675,0.7454545454545455,0.7299703264094957,0.874085947637585\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.004633874361245948, 0.7131831001543397, 0.7171846317870882, 0.7145658246700666, 0.8710571453588368]\n",
      "mlt train ended\n",
      "0.712657,0.718391,0.715512,0.869120\n"
     ]
    }
   ],
   "source": [
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.l2_weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv_aug(tar_feature, train_dev_idx, features_ss_aug), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00674, 1.0000, 0.0000, 0.0000, 0.7596]\n",
      "e_2 * test rst: [0.00843, 0.3618, 0.9471, 0.5236, 0.7982]\n",
      "e_3 * test rst: [0.00470, 0.6564, 0.6294, 0.6426, 0.8214]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00486, 0.5701, 0.7412, 0.6445, 0.8192]\n",
      "e_5 * test rst: [0.00534, 0.5479, 0.8412, 0.6636, 0.8241]\n",
      "e_6 * test rst: [0.00467, 0.6283, 0.7059, 0.6648, 0.8236]\n",
      "e_7 * test rst: [0.00480, 0.6238, 0.7412, 0.6774, 0.8357]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00491, 0.6448, 0.6941, 0.6686, 0.8298]\n",
      "e_9 * test rst: [0.00524, 0.7080, 0.5706, 0.6319, 0.8244]\n",
      "e_10 * test rst: [0.00511, 0.6752, 0.6235, 0.6483, 0.8296]\n",
      "e_11 * test rst: [0.00501, 0.6667, 0.6353, 0.6506, 0.8278]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00489, 0.6378, 0.6941, 0.6648, 0.8306]\n",
      "e_13 * test rst: [0.00502, 0.6310, 0.6941, 0.6611, 0.8327]\n",
      "e_14 * test rst: [0.00503, 0.6350, 0.7471, 0.6865, 0.8340]\n",
      "e_15 * test rst: [0.00512, 0.6500, 0.6882, 0.6686, 0.8313]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00518, 0.6477, 0.6706, 0.6590, 0.8326]\n",
      "e_17 * test rst: [0.00515, 0.6364, 0.7000, 0.6667, 0.8334]\n",
      "e_18 * test rst: [0.00523, 0.6369, 0.6706, 0.6533, 0.8325]\n",
      "training end, used 35.38 s\n",
      "(0.005229096509000124, 0.6368715083798883, 0.6705882352941176, 0.6532951289398281, 0.8324951354587637)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00661, 1.0000, 0.0000, 0.0000, 0.8389]\n",
      "e_2 * test rst: [0.00470, 0.6552, 0.6706, 0.6628, 0.8516]\n",
      "e_3 * test rst: [0.00499, 0.5890, 0.7588, 0.6632, 0.8497]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00579, 0.5222, 0.8294, 0.6409, 0.8447]\n",
      "e_5 * test rst: [0.00533, 0.5907, 0.7471, 0.6597, 0.8558]\n",
      "e_6 * test rst: [0.00540, 0.5620, 0.8000, 0.6602, 0.8525]\n",
      "e_7 * test rst: [0.00518, 0.6484, 0.6941, 0.6705, 0.8562]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00504, 0.7200, 0.6353, 0.6750, 0.8553]\n",
      "e_9 * test rst: [0.00530, 0.6467, 0.7000, 0.6723, 0.8568]\n",
      "e_10 * test rst: [0.00520, 0.6784, 0.6824, 0.6804, 0.8581]\n",
      "e_11 * test rst: [0.00518, 0.7063, 0.6647, 0.6848, 0.8560]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00524, 0.6914, 0.6588, 0.6747, 0.8564]\n",
      "e_13 * test rst: [0.00547, 0.6364, 0.7000, 0.6667, 0.8589]\n",
      "e_14 * test rst: [0.00531, 0.7044, 0.6588, 0.6809, 0.8556]\n",
      "e_15 * test rst: [0.00538, 0.6871, 0.6588, 0.6727, 0.8565]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00546, 0.6667, 0.6824, 0.6744, 0.8571]\n",
      "e_17 * test rst: [0.00542, 0.6871, 0.6588, 0.6727, 0.8564]\n",
      "e_18 * test rst: [0.00542, 0.6914, 0.6588, 0.6747, 0.8564]\n",
      "training end, used 35.41 s\n",
      "(0.0054215119910123715, 0.691358024691358, 0.6588235294117647, 0.674698795180723, 0.8564286783415656)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00707, 1.0000, 0.0000, 0.0000, 0.7632]\n",
      "e_2 * test rst: [0.00642, 0.7885, 0.4184, 0.5467, 0.8041]\n",
      "e_3 * test rst: [0.00546, 0.6705, 0.5918, 0.6287, 0.7969]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00577, 0.6988, 0.5918, 0.6409, 0.8116]\n",
      "e_5 * test rst: [0.00560, 0.7612, 0.5204, 0.6182, 0.8149]\n",
      "e_6 * test rst: [0.00593, 0.7519, 0.5102, 0.6079, 0.8214]\n",
      "e_7 * test rst: [0.00609, 0.7727, 0.5204, 0.6220, 0.8185]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00540, 0.7151, 0.6276, 0.6685, 0.8188]\n",
      "e_9 * test rst: [0.00597, 0.5904, 0.7500, 0.6607, 0.8171]\n",
      "e_10 * test rst: [0.00552, 0.6828, 0.6480, 0.6649, 0.8223]\n",
      "e_11 * test rst: [0.00590, 0.7273, 0.5714, 0.6400, 0.8231]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00560, 0.6526, 0.7092, 0.6797, 0.8236]\n",
      "e_13 * test rst: [0.00569, 0.7006, 0.6327, 0.6649, 0.8237]\n",
      "e_14 * test rst: [0.00568, 0.6603, 0.7041, 0.6815, 0.8241]\n",
      "e_15 * test rst: [0.00576, 0.6910, 0.6276, 0.6578, 0.8239]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00574, 0.6700, 0.6837, 0.6768, 0.8237]\n",
      "e_17 * test rst: [0.00578, 0.6634, 0.6939, 0.6783, 0.8234]\n",
      "e_18 * test rst: [0.00580, 0.6735, 0.6735, 0.6735, 0.8235]\n",
      "training end, used 35.76 s\n",
      "(0.005797962206506814, 0.673469387755102, 0.673469387755102, 0.673469387755102, 0.8234721681588166)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "500 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00694, 1.0000, 0.0000, 0.0000, 0.7211]\n",
      "e_2 * test rst: [0.00576, 0.5371, 0.5562, 0.5465, 0.7493]\n",
      "e_3 * test rst: [0.00557, 0.6698, 0.4201, 0.5164, 0.7819]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00563, 0.6545, 0.4260, 0.5161, 0.7806]\n",
      "e_5 * test rst: [0.00581, 0.7556, 0.4024, 0.5251, 0.7794]\n",
      "e_6 * test rst: [0.00540, 0.6640, 0.4911, 0.5646, 0.7842]\n",
      "e_7 * test rst: [0.00533, 0.6449, 0.5266, 0.5798, 0.7902]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00599, 0.8250, 0.3905, 0.5301, 0.7897]\n",
      "e_9 * test rst: [0.00543, 0.6026, 0.5562, 0.5785, 0.7946]\n",
      "e_10 * test rst: [0.00551, 0.5988, 0.5917, 0.5952, 0.7922]\n",
      "e_11 * test rst: [0.00572, 0.6327, 0.5503, 0.5886, 0.7918]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00580, 0.6742, 0.5266, 0.5914, 0.7904]\n",
      "e_13 * test rst: [0.00578, 0.6115, 0.5680, 0.5890, 0.7912]\n",
      "e_14 * test rst: [0.00589, 0.6069, 0.6213, 0.6140, 0.7904]\n",
      "e_15 * test rst: [0.00584, 0.6154, 0.5680, 0.5908, 0.7901]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00588, 0.6414, 0.5503, 0.5924, 0.7900]\n",
      "e_17 * test rst: [0.00598, 0.6291, 0.5621, 0.5938, 0.7890]\n",
      "e_18 * test rst: [0.00602, 0.6458, 0.5503, 0.5942, 0.7885]\n",
      "training end, used 35.34 s\n",
      "(0.006022691938800744, 0.6458333333333334, 0.5502958579881657, 0.5942492012779553, 0.7885330562958279)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "500 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00663, 1.0000, 0.0000, 0.0000, 0.8107]\n",
      "e_2 * test rst: [0.00546, 0.5325, 0.7455, 0.6212, 0.8082]\n",
      "e_3 * test rst: [0.00478, 0.7879, 0.4727, 0.5909, 0.8237]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00471, 0.6348, 0.6848, 0.6589, 0.8256]\n",
      "e_5 * test rst: [0.00451, 0.7561, 0.5636, 0.6458, 0.8330]\n",
      "e_6 * test rst: [0.00484, 0.7417, 0.5394, 0.6246, 0.8205]\n",
      "e_7 * test rst: [0.00483, 0.8056, 0.5273, 0.6374, 0.8133]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00456, 0.7183, 0.6182, 0.6645, 0.8194]\n",
      "e_9 * test rst: [0.00472, 0.6437, 0.6788, 0.6608, 0.8247]\n",
      "e_10 * test rst: [0.00468, 0.6774, 0.6364, 0.6562, 0.8256]\n",
      "e_11 * test rst: [0.00536, 0.6122, 0.7273, 0.6648, 0.8238]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00493, 0.6647, 0.6727, 0.6687, 0.8214]\n",
      "e_13 * test rst: [0.00483, 0.6818, 0.6364, 0.6583, 0.8221]\n",
      "e_14 * test rst: [0.00505, 0.6400, 0.6788, 0.6588, 0.8204]\n",
      "e_15 * test rst: [0.00501, 0.6667, 0.6667, 0.6667, 0.8181]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00511, 0.6550, 0.6788, 0.6667, 0.8174]\n",
      "e_17 * test rst: [0.00501, 0.6792, 0.6545, 0.6667, 0.8161]\n",
      "e_18 * test rst: [0.00509, 0.6667, 0.6667, 0.6667, 0.8157]\n",
      "training end, used 35.70 s\n",
      "(0.005087302686162691, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.8157316235401879)\n",
      "total label sum 184.5\n",
      "0.005229096509000124,0.6368715083798883,0.6705882352941176,0.6532951289398281,0.8324951354587637\n",
      "0.0054215119910123715,0.691358024691358,0.6588235294117647,0.674698795180723,0.8564286783415656\n",
      "0.005797962206506814,0.673469387755102,0.673469387755102,0.673469387755102,0.8234721681588166\n",
      "0.006022691938800744,0.6458333333333334,0.5502958579881657,0.5942492012779553,0.7885330562958279\n",
      "0.005087302686162691,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.8157316235401879\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.005511713066296549, 0.6628397841652697, 0.6439687354231632, 0.652475835964055, 0.8233321323590322]\n",
      "mlt train ended\n",
      "0.663121,0.644828,0.653846,0.817134\n"
     ]
    }
   ],
   "source": [
    "train_s_sz = 500\n",
    "\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "args.l2_weight_decay = 1e-4\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    train_dev_idx = train_dev_idx[:train_s_sz]\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv(tar_feature, train_dev_idx), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "1000 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00485, 0.7217, 0.4882, 0.5825, 0.8195]\n",
      "e_2 * test rst: [0.00607, 0.8393, 0.2765, 0.4159, 0.8056]\n",
      "e_3 * test rst: [0.00451, 0.6486, 0.7059, 0.6761, 0.8342]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00465, 0.7133, 0.6294, 0.6687, 0.8307]\n",
      "e_5 * test rst: [0.00477, 0.6425, 0.6765, 0.6590, 0.8305]\n",
      "e_6 * test rst: [0.00478, 0.6848, 0.6647, 0.6746, 0.8347]\n",
      "e_7 * test rst: [0.00495, 0.6313, 0.7353, 0.6793, 0.8351]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00491, 0.6805, 0.6765, 0.6785, 0.8346]\n",
      "e_9 * test rst: [0.00508, 0.6704, 0.7059, 0.6877, 0.8326]\n",
      "e_10 * test rst: [0.00513, 0.6826, 0.6706, 0.6766, 0.8320]\n",
      "e_11 * test rst: [0.00538, 0.6181, 0.7235, 0.6667, 0.8312]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00519, 0.6614, 0.7353, 0.6964, 0.8341]\n",
      "e_13 * test rst: [0.00531, 0.6219, 0.7353, 0.6739, 0.8329]\n",
      "e_14 * test rst: [0.00532, 0.6188, 0.7353, 0.6720, 0.8339]\n",
      "e_15 * test rst: [0.00522, 0.6802, 0.6882, 0.6842, 0.8343]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00528, 0.6458, 0.7294, 0.6851, 0.8342]\n",
      "e_17 * test rst: [0.00533, 0.6474, 0.7235, 0.6833, 0.8337]\n",
      "e_18 * test rst: [0.00529, 0.6782, 0.6941, 0.6860, 0.8337]\n",
      "training end, used 66.00 s\n",
      "(0.005289771681780501, 0.6781609195402298, 0.6941176470588235, 0.6860465116279069, 0.8336775931746745)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "1000 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00504, 0.6458, 0.7294, 0.6851, 0.8538]\n",
      "e_2 * test rst: [0.00519, 0.7553, 0.4176, 0.5379, 0.8401]\n",
      "e_3 * test rst: [0.00459, 0.6263, 0.7294, 0.6739, 0.8540]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00503, 0.6386, 0.7588, 0.6935, 0.8646]\n",
      "e_5 * test rst: [0.00492, 0.6954, 0.6176, 0.6542, 0.8646]\n",
      "e_6 * test rst: [0.00539, 0.6055, 0.7765, 0.6804, 0.8671]\n",
      "e_7 * test rst: [0.00492, 0.6790, 0.6471, 0.6627, 0.8671]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00501, 0.6730, 0.6294, 0.6505, 0.8628]\n",
      "e_9 * test rst: [0.00496, 0.6522, 0.7059, 0.6780, 0.8691]\n",
      "e_10 * test rst: [0.00519, 0.6867, 0.6706, 0.6786, 0.8626]\n",
      "e_11 * test rst: [0.00507, 0.6489, 0.7176, 0.6816, 0.8717]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00517, 0.6871, 0.6588, 0.6727, 0.8656]\n",
      "e_13 * test rst: [0.00516, 0.6890, 0.6647, 0.6766, 0.8656]\n",
      "e_14 * test rst: [0.00526, 0.6918, 0.6471, 0.6687, 0.8629]\n",
      "e_15 * test rst: [0.00518, 0.6780, 0.7059, 0.6916, 0.8689]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00520, 0.6848, 0.6647, 0.6746, 0.8654]\n",
      "e_17 * test rst: [0.00525, 0.6848, 0.6647, 0.6746, 0.8645]\n",
      "e_18 * test rst: [0.00523, 0.6905, 0.6824, 0.6864, 0.8656]\n",
      "training end, used 66.23 s\n",
      "(0.005232901790478725, 0.6904761904761905, 0.6823529411764706, 0.6863905325443787, 0.8656189193234545)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "1000 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00639, 0.5151, 0.7857, 0.6222, 0.7895]\n",
      "e_2 * test rst: [0.00553, 0.5755, 0.7194, 0.6395, 0.8023]\n",
      "e_3 * test rst: [0.00514, 0.7557, 0.5051, 0.6055, 0.8105]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00574, 0.5924, 0.7194, 0.6498, 0.8186]\n",
      "e_5 * test rst: [0.00539, 0.6667, 0.6633, 0.6650, 0.8097]\n",
      "e_6 * test rst: [0.00600, 0.7372, 0.5153, 0.6066, 0.8128]\n",
      "e_7 * test rst: [0.00606, 0.6250, 0.7143, 0.6667, 0.8085]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00598, 0.6703, 0.6224, 0.6455, 0.8084]\n",
      "e_9 * test rst: [0.00637, 0.6099, 0.6939, 0.6492, 0.8091]\n",
      "e_10 * test rst: [0.00626, 0.6505, 0.6837, 0.6667, 0.8090]\n",
      "e_11 * test rst: [0.00652, 0.5950, 0.7347, 0.6575, 0.8079]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00626, 0.6351, 0.6837, 0.6585, 0.8105]\n",
      "e_13 * test rst: [0.00635, 0.6326, 0.6939, 0.6618, 0.8090]\n",
      "e_14 * test rst: [0.00633, 0.6337, 0.6531, 0.6432, 0.8101]\n",
      "e_15 * test rst: [0.00632, 0.6359, 0.6684, 0.6517, 0.8107]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00640, 0.6414, 0.6480, 0.6447, 0.8114]\n",
      "e_17 * test rst: [0.00648, 0.6324, 0.6582, 0.6450, 0.8115]\n",
      "e_18 * test rst: [0.00653, 0.6308, 0.6888, 0.6585, 0.8120]\n",
      "training end, used 66.47 s\n",
      "(0.006530242361670912, 0.6308411214953271, 0.6887755102040817, 0.6585365853658538, 0.8120308068731579)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "1000 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00591, 0.5381, 0.6686, 0.5963, 0.7505]\n",
      "e_2 * test rst: [0.00540, 0.5741, 0.5503, 0.5619, 0.7731]\n",
      "e_3 * test rst: [0.00610, 0.8333, 0.2367, 0.3687, 0.7810]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00525, 0.6168, 0.6095, 0.6131, 0.7863]\n",
      "e_5 * test rst: [0.00518, 0.6506, 0.6391, 0.6448, 0.7865]\n",
      "e_6 * test rst: [0.00553, 0.6150, 0.6805, 0.6461, 0.7868]\n",
      "e_7 * test rst: [0.00528, 0.6917, 0.5444, 0.6093, 0.7911]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00542, 0.6603, 0.6095, 0.6338, 0.7901]\n",
      "e_9 * test rst: [0.00553, 0.6337, 0.6450, 0.6393, 0.7893]\n",
      "e_10 * test rst: [0.00555, 0.6456, 0.6036, 0.6239, 0.7879]\n",
      "e_11 * test rst: [0.00562, 0.6689, 0.5976, 0.6313, 0.7900]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00560, 0.6478, 0.6095, 0.6280, 0.7882]\n",
      "e_13 * test rst: [0.00583, 0.6412, 0.6450, 0.6431, 0.7865]\n",
      "e_14 * test rst: [0.00575, 0.6380, 0.6154, 0.6265, 0.7877]\n",
      "e_15 * test rst: [0.00580, 0.6375, 0.6036, 0.6201, 0.7877]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00586, 0.6398, 0.6095, 0.6242, 0.7867]\n",
      "e_17 * test rst: [0.00590, 0.6398, 0.6095, 0.6242, 0.7864]\n",
      "e_18 * test rst: [0.00588, 0.6398, 0.6095, 0.6242, 0.7869]\n",
      "training end, used 65.67 s\n",
      "(0.005878518484665406, 0.639751552795031, 0.6094674556213018, 0.6242424242424243, 0.7868919102037129)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "1000 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00515, 0.7043, 0.4909, 0.5786, 0.7864]\n",
      "e_2 * test rst: [0.00470, 0.7333, 0.5333, 0.6175, 0.8125]\n",
      "e_3 * test rst: [0.00459, 0.6886, 0.6970, 0.6928, 0.8417]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00537, 0.5857, 0.7455, 0.6560, 0.8353]\n",
      "e_5 * test rst: [0.00471, 0.6590, 0.6909, 0.6746, 0.8332]\n",
      "e_6 * test rst: [0.00462, 0.7051, 0.6667, 0.6854, 0.8346]\n",
      "e_7 * test rst: [0.00473, 0.7192, 0.6364, 0.6752, 0.8258]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00540, 0.6000, 0.7636, 0.6720, 0.8324]\n",
      "e_9 * test rst: [0.00500, 0.6505, 0.7333, 0.6895, 0.8287]\n",
      "e_10 * test rst: [0.00470, 0.7025, 0.6727, 0.6873, 0.8266]\n",
      "e_11 * test rst: [0.00506, 0.6536, 0.7091, 0.6802, 0.8275]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00511, 0.6500, 0.7091, 0.6783, 0.8273]\n",
      "e_13 * test rst: [0.00516, 0.6556, 0.7152, 0.6841, 0.8249]\n",
      "e_14 * test rst: [0.00510, 0.6628, 0.6909, 0.6766, 0.8235]\n",
      "e_15 * test rst: [0.00514, 0.6667, 0.7030, 0.6844, 0.8236]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00522, 0.6591, 0.7030, 0.6804, 0.8225]\n",
      "e_17 * test rst: [0.00530, 0.6591, 0.7030, 0.6804, 0.8221]\n",
      "e_18 * test rst: [0.00535, 0.6517, 0.7030, 0.6764, 0.8218]\n",
      "training end, used 66.00 s\n",
      "(0.005351491743082254, 0.651685393258427, 0.703030303030303, 0.6763848396501458, 0.8218303946263643)\n",
      "total label sum 184.5\n",
      "0.005289771681780501,0.6781609195402298,0.6941176470588235,0.6860465116279069,0.8336775931746745\n",
      "0.005232901790478725,0.6904761904761905,0.6823529411764706,0.6863905325443787,0.8656189193234545\n",
      "0.006530242361670912,0.6308411214953271,0.6887755102040817,0.6585365853658538,0.8120308068731579\n",
      "0.005878518484665406,0.639751552795031,0.6094674556213018,0.6242424242424243,0.7868919102037129\n",
      "0.005351491743082254,0.651685393258427,0.703030303030303,0.6763848396501458,0.8218303946263643\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.00565658521233556, 0.658183035513041, 0.6755487714181962, 0.6663201786861419, 0.8240099248402728]\n",
      "mlt train ended\n",
      "0.656983,0.675862,0.666289,0.816887\n"
     ]
    }
   ],
   "source": [
    "train_s_sz = 1000\n",
    "\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "args.l2_weight_decay = 1e-4\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    train_dev_idx = train_dev_idx[:train_s_sz]\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv(tar_feature, train_dev_idx), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "1500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00494, 0.6601, 0.5941, 0.6254, 0.8121]\n",
      "e_2 * test rst: [0.00476, 0.7524, 0.4647, 0.5745, 0.8259]\n",
      "e_3 * test rst: [0.00442, 0.7107, 0.6647, 0.6869, 0.8380]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00476, 0.7698, 0.5706, 0.6554, 0.8423]\n",
      "e_5 * test rst: [0.00515, 0.5422, 0.7941, 0.6444, 0.8405]\n",
      "e_6 * test rst: [0.00461, 0.6095, 0.7529, 0.6737, 0.8430]\n",
      "e_7 * test rst: [0.00480, 0.7778, 0.5765, 0.6622, 0.8427]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00465, 0.7063, 0.6647, 0.6848, 0.8443]\n",
      "e_9 * test rst: [0.00477, 0.6821, 0.6941, 0.6880, 0.8429]\n",
      "e_10 * test rst: [0.00480, 0.6842, 0.6882, 0.6862, 0.8449]\n",
      "e_11 * test rst: [0.00482, 0.7467, 0.6588, 0.7000, 0.8408]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00485, 0.6933, 0.6647, 0.6787, 0.8430]\n",
      "e_13 * test rst: [0.00508, 0.6443, 0.7353, 0.6868, 0.8420]\n",
      "e_14 * test rst: [0.00484, 0.6890, 0.6647, 0.6766, 0.8434]\n",
      "e_15 * test rst: [0.00491, 0.6933, 0.6647, 0.6787, 0.8412]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00491, 0.6864, 0.6824, 0.6844, 0.8428]\n",
      "e_17 * test rst: [0.00494, 0.6839, 0.7000, 0.6919, 0.8434]\n",
      "e_18 * test rst: [0.00494, 0.6786, 0.6706, 0.6746, 0.8428]\n",
      "training end, used 97.34 s\n",
      "(0.004939408221846152, 0.6785714285714286, 0.6705882352941176, 0.6745562130177514, 0.8427929950606197)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "1500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00521, 0.8158, 0.3647, 0.5041, 0.8377]\n",
      "e_2 * test rst: [0.00484, 0.6527, 0.6412, 0.6469, 0.8502]\n",
      "e_3 * test rst: [0.00497, 0.6359, 0.7294, 0.6795, 0.8497]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00493, 0.6772, 0.6294, 0.6524, 0.8583]\n",
      "e_5 * test rst: [0.00510, 0.6957, 0.6588, 0.6767, 0.8571]\n",
      "e_6 * test rst: [0.00482, 0.6852, 0.6529, 0.6687, 0.8605]\n",
      "e_7 * test rst: [0.00558, 0.6231, 0.7294, 0.6721, 0.8565]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00516, 0.7115, 0.6529, 0.6810, 0.8550]\n",
      "e_9 * test rst: [0.00524, 0.6524, 0.7176, 0.6835, 0.8595]\n",
      "e_10 * test rst: [0.00522, 0.6933, 0.6647, 0.6787, 0.8583]\n",
      "e_11 * test rst: [0.00548, 0.6685, 0.7118, 0.6895, 0.8612]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00537, 0.6611, 0.7000, 0.6800, 0.8599]\n",
      "e_13 * test rst: [0.00546, 0.6611, 0.7000, 0.6800, 0.8589]\n",
      "e_14 * test rst: [0.00536, 0.6824, 0.6824, 0.6824, 0.8588]\n",
      "e_15 * test rst: [0.00538, 0.6705, 0.6824, 0.6764, 0.8597]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00544, 0.6648, 0.7118, 0.6875, 0.8614]\n",
      "e_17 * test rst: [0.00539, 0.6743, 0.6941, 0.6841, 0.8602]\n",
      "e_18 * test rst: [0.00546, 0.6648, 0.7118, 0.6875, 0.8607]\n",
      "training end, used 96.57 s\n",
      "(0.0054589010523648715, 0.6648351648351648, 0.711764705882353, 0.6875, 0.8606945068103578)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "1500 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00521, 0.7308, 0.5816, 0.6477, 0.8254]\n",
      "e_2 * test rst: [0.00493, 0.7208, 0.5663, 0.6343, 0.8287]\n",
      "e_3 * test rst: [0.00625, 0.4916, 0.8929, 0.6341, 0.8325]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00500, 0.6333, 0.7755, 0.6972, 0.8411]\n",
      "e_5 * test rst: [0.00490, 0.6711, 0.7806, 0.7217, 0.8445]\n",
      "e_6 * test rst: [0.00526, 0.6954, 0.6990, 0.6972, 0.8458]\n",
      "e_7 * test rst: [0.00531, 0.6593, 0.7602, 0.7062, 0.8381]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00530, 0.6757, 0.7653, 0.7177, 0.8402]\n",
      "e_9 * test rst: [0.00542, 0.6773, 0.7602, 0.7163, 0.8383]\n",
      "e_10 * test rst: [0.00566, 0.6456, 0.7806, 0.7067, 0.8362]\n",
      "e_11 * test rst: [0.00592, 0.6062, 0.8010, 0.6901, 0.8348]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00564, 0.6837, 0.7500, 0.7153, 0.8365]\n",
      "e_13 * test rst: [0.00568, 0.6806, 0.7500, 0.7136, 0.8368]\n",
      "e_14 * test rst: [0.00579, 0.6759, 0.7449, 0.7087, 0.8364]\n",
      "e_15 * test rst: [0.00581, 0.6834, 0.6939, 0.6886, 0.8359]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00580, 0.6774, 0.7500, 0.7119, 0.8345]\n",
      "e_17 * test rst: [0.00582, 0.6779, 0.7194, 0.6980, 0.8350]\n",
      "e_18 * test rst: [0.00588, 0.6794, 0.7245, 0.7012, 0.8352]\n",
      "training end, used 96.92 s\n",
      "(0.00587733550457082, 0.6794258373205742, 0.7244897959183674, 0.7012345679012346, 0.8351637657787911)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "1500 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00558, 0.6557, 0.4734, 0.5498, 0.7678]\n",
      "e_2 * test rst: [0.00537, 0.5942, 0.4852, 0.5342, 0.7749]\n",
      "e_3 * test rst: [0.00507, 0.6420, 0.6154, 0.6284, 0.8005]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00523, 0.6714, 0.5562, 0.6084, 0.7977]\n",
      "e_5 * test rst: [0.00567, 0.6415, 0.6036, 0.6220, 0.8027]\n",
      "e_6 * test rst: [0.00557, 0.6507, 0.5621, 0.6032, 0.7991]\n",
      "e_7 * test rst: [0.00549, 0.5989, 0.6627, 0.6292, 0.8041]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00567, 0.6600, 0.5858, 0.6207, 0.8006]\n",
      "e_9 * test rst: [0.00566, 0.6442, 0.6213, 0.6325, 0.8016]\n",
      "e_10 * test rst: [0.00584, 0.6337, 0.6450, 0.6393, 0.8018]\n",
      "e_11 * test rst: [0.00589, 0.6424, 0.6272, 0.6347, 0.8026]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00576, 0.6541, 0.6154, 0.6341, 0.8034]\n",
      "e_13 * test rst: [0.00583, 0.6689, 0.5976, 0.6313, 0.8030]\n",
      "e_14 * test rst: [0.00579, 0.6538, 0.6036, 0.6277, 0.8042]\n",
      "e_15 * test rst: [0.00589, 0.6601, 0.5976, 0.6273, 0.8040]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00589, 0.6541, 0.6154, 0.6341, 0.8040]\n",
      "e_17 * test rst: [0.00589, 0.6538, 0.6036, 0.6277, 0.8036]\n",
      "e_18 * test rst: [0.00590, 0.6623, 0.6036, 0.6316, 0.8038]\n",
      "training end, used 97.29 s\n",
      "(0.00590137111228556, 0.6623376623376623, 0.6035502958579881, 0.631578947368421, 0.8037701190960145)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "1500 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00531, 0.5368, 0.7515, 0.6263, 0.8154]\n",
      "e_2 * test rst: [0.00477, 0.6103, 0.7212, 0.6611, 0.8283]\n",
      "e_3 * test rst: [0.00455, 0.7768, 0.5273, 0.6282, 0.8461]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00438, 0.6613, 0.7455, 0.7009, 0.8560]\n",
      "e_5 * test rst: [0.00449, 0.6758, 0.7455, 0.7089, 0.8529]\n",
      "e_6 * test rst: [0.00431, 0.7035, 0.7333, 0.7181, 0.8498]\n",
      "e_7 * test rst: [0.00447, 0.7143, 0.7273, 0.7207, 0.8509]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00461, 0.6949, 0.7455, 0.7193, 0.8482]\n",
      "e_9 * test rst: [0.00458, 0.7284, 0.7152, 0.7217, 0.8423]\n",
      "e_10 * test rst: [0.00466, 0.7093, 0.7394, 0.7240, 0.8369]\n",
      "e_11 * test rst: [0.00473, 0.6954, 0.7333, 0.7139, 0.8374]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00476, 0.6914, 0.7333, 0.7118, 0.8385]\n",
      "e_13 * test rst: [0.00492, 0.6721, 0.7455, 0.7069, 0.8358]\n",
      "e_14 * test rst: [0.00509, 0.6632, 0.7636, 0.7099, 0.8344]\n",
      "e_15 * test rst: [0.00520, 0.6562, 0.7636, 0.7059, 0.8352]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00498, 0.6816, 0.7394, 0.7093, 0.8347]\n",
      "e_17 * test rst: [0.00513, 0.6685, 0.7455, 0.7049, 0.8347]\n",
      "e_18 * test rst: [0.00523, 0.6578, 0.7455, 0.6989, 0.8342]\n",
      "training end, used 98.16 s\n",
      "(0.005230676932956401, 0.6577540106951871, 0.7454545454545455, 0.6988636363636364, 0.8342111289214564)\n",
      "total label sum 184.5\n",
      "0.004939408221846152,0.6785714285714286,0.6705882352941176,0.6745562130177514,0.8427929950606197\n",
      "0.0054589010523648715,0.6648351648351648,0.711764705882353,0.6875,0.8606945068103578\n",
      "0.00587733550457082,0.6794258373205742,0.7244897959183674,0.7012345679012346,0.8351637657787911\n",
      "0.00590137111228556,0.6623376623376623,0.6035502958579881,0.631578947368421,0.8037701190960145\n",
      "0.005230676932956401,0.6577540106951871,0.7454545454545455,0.6988636363636364,0.8342111289214564\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.0054815385648047605, 0.6685848207520035, 0.6911695156814743, 0.6787466729302086, 0.835326503133448]\n",
      "mlt train ended\n",
      "0.668889,0.691954,0.680226,0.828261\n"
     ]
    }
   ],
   "source": [
    "train_s_sz = 1500\n",
    "\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "args.l2_weight_decay = 1e-4\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    train_dev_idx = train_dev_idx[:train_s_sz]\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv(tar_feature, train_dev_idx), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00566, 0.8125, 0.3059, 0.4444, 0.8213]\n",
      "e_2 * test rst: [0.00431, 0.6882, 0.6882, 0.6882, 0.8370]\n",
      "e_3 * test rst: [0.00483, 0.8000, 0.5412, 0.6456, 0.8397]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00464, 0.6743, 0.6941, 0.6841, 0.8403]\n",
      "e_5 * test rst: [0.00465, 0.6905, 0.6824, 0.6864, 0.8463]\n",
      "e_6 * test rst: [0.00531, 0.5895, 0.7941, 0.6767, 0.8444]\n",
      "e_7 * test rst: [0.00472, 0.7879, 0.6118, 0.6887, 0.8404]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00462, 0.7059, 0.7059, 0.7059, 0.8458]\n",
      "e_9 * test rst: [0.00467, 0.7222, 0.6882, 0.7048, 0.8436]\n",
      "e_10 * test rst: [0.00467, 0.7436, 0.6824, 0.7117, 0.8460]\n",
      "e_11 * test rst: [0.00471, 0.7117, 0.6824, 0.6967, 0.8477]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00477, 0.7338, 0.6647, 0.6975, 0.8476]\n",
      "e_13 * test rst: [0.00477, 0.7178, 0.6882, 0.7027, 0.8488]\n",
      "e_14 * test rst: [0.00474, 0.7233, 0.6765, 0.6991, 0.8497]\n",
      "e_15 * test rst: [0.00479, 0.6875, 0.7118, 0.6994, 0.8521]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00480, 0.7041, 0.7000, 0.7021, 0.8501]\n",
      "e_17 * test rst: [0.00483, 0.6897, 0.7059, 0.6977, 0.8511]\n",
      "e_18 * test rst: [0.00484, 0.6936, 0.7059, 0.6997, 0.8518]\n",
      "training end, used 144.27 s\n",
      "(0.004842654611670526, 0.6936416184971098, 0.7058823529411765, 0.6997084548104957, 0.8518185900314323)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00475, 0.7398, 0.5353, 0.6212, 0.8555]\n",
      "e_2 * test rst: [0.00464, 0.6864, 0.6824, 0.6844, 0.8667]\n",
      "e_3 * test rst: [0.00490, 0.7385, 0.5647, 0.6400, 0.8568]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00503, 0.7429, 0.6118, 0.6710, 0.8589]\n",
      "e_5 * test rst: [0.00477, 0.6648, 0.7118, 0.6875, 0.8634]\n",
      "e_6 * test rst: [0.00479, 0.7292, 0.6176, 0.6688, 0.8620]\n",
      "e_7 * test rst: [0.00466, 0.7273, 0.6588, 0.6914, 0.8651]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00513, 0.7179, 0.6588, 0.6871, 0.8609]\n",
      "e_9 * test rst: [0.00488, 0.6886, 0.6765, 0.6825, 0.8652]\n",
      "e_10 * test rst: [0.00499, 0.7244, 0.6647, 0.6933, 0.8621]\n",
      "e_11 * test rst: [0.00514, 0.7171, 0.6412, 0.6770, 0.8607]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00511, 0.7379, 0.6294, 0.6794, 0.8608]\n",
      "e_13 * test rst: [0.00518, 0.7255, 0.6529, 0.6873, 0.8615]\n",
      "e_14 * test rst: [0.00514, 0.7303, 0.6529, 0.6894, 0.8616]\n",
      "e_15 * test rst: [0.00515, 0.7273, 0.6588, 0.6914, 0.8610]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00515, 0.7303, 0.6529, 0.6894, 0.8616]\n",
      "e_17 * test rst: [0.00515, 0.7415, 0.6412, 0.6877, 0.8606]\n",
      "e_18 * test rst: [0.00515, 0.7432, 0.6471, 0.6918, 0.8608]\n",
      "training end, used 144.62 s\n",
      "(0.005145402737689696, 0.7432432432432432, 0.6470588235294118, 0.6918238993710691, 0.8607843137254901)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00525, 0.6373, 0.6276, 0.6324, 0.8095]\n",
      "e_2 * test rst: [0.00485, 0.6701, 0.6735, 0.6718, 0.8248]\n",
      "e_3 * test rst: [0.00534, 0.6973, 0.6582, 0.6772, 0.8341]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00520, 0.6823, 0.6684, 0.6753, 0.8398]\n",
      "e_5 * test rst: [0.00538, 0.6620, 0.7296, 0.6942, 0.8404]\n",
      "e_6 * test rst: [0.00525, 0.6516, 0.7347, 0.6906, 0.8372]\n",
      "e_7 * test rst: [0.00555, 0.6383, 0.7653, 0.6961, 0.8463]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00529, 0.6620, 0.7296, 0.6942, 0.8470]\n",
      "e_9 * test rst: [0.00577, 0.6157, 0.7602, 0.6804, 0.8459]\n",
      "e_10 * test rst: [0.00579, 0.6198, 0.7653, 0.6849, 0.8446]\n",
      "e_11 * test rst: [0.00565, 0.6395, 0.7602, 0.6946, 0.8453]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00561, 0.6637, 0.7551, 0.7064, 0.8462]\n",
      "e_13 * test rst: [0.00562, 0.6606, 0.7449, 0.7002, 0.8456]\n",
      "e_14 * test rst: [0.00565, 0.6591, 0.7398, 0.6971, 0.8459]\n",
      "e_15 * test rst: [0.00571, 0.6520, 0.7551, 0.6998, 0.8464]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00568, 0.6591, 0.7398, 0.6971, 0.8451]\n",
      "e_17 * test rst: [0.00569, 0.6636, 0.7449, 0.7019, 0.8456]\n",
      "e_18 * test rst: [0.00569, 0.6621, 0.7398, 0.6988, 0.8462]\n",
      "training end, used 144.95 s\n",
      "(0.0056917830295926935, 0.6621004566210046, 0.7397959183673469, 0.6987951807228916, 0.8461602624701107)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "2251 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00622, 0.7600, 0.1124, 0.1959, 0.7685]\n",
      "e_2 * test rst: [0.00522, 0.5663, 0.6568, 0.6082, 0.7949]\n",
      "e_3 * test rst: [0.00513, 0.6056, 0.6450, 0.6246, 0.7936]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00553, 0.6552, 0.5621, 0.6051, 0.7940]\n",
      "e_5 * test rst: [0.00571, 0.6022, 0.6450, 0.6229, 0.7960]\n",
      "e_6 * test rst: [0.00562, 0.6415, 0.6036, 0.6220, 0.7940]\n",
      "e_7 * test rst: [0.00609, 0.5944, 0.6331, 0.6132, 0.7913]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00597, 0.6154, 0.6154, 0.6154, 0.7936]\n",
      "e_9 * test rst: [0.00603, 0.6483, 0.5562, 0.5987, 0.7946]\n",
      "e_10 * test rst: [0.00605, 0.6364, 0.5799, 0.6068, 0.7948]\n",
      "e_11 * test rst: [0.00598, 0.6306, 0.5858, 0.6074, 0.7946]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00614, 0.6370, 0.5503, 0.5905, 0.7942]\n",
      "e_13 * test rst: [0.00617, 0.6194, 0.5680, 0.5926, 0.7949]\n",
      "e_14 * test rst: [0.00628, 0.6327, 0.5503, 0.5886, 0.7947]\n",
      "e_15 * test rst: [0.00617, 0.6351, 0.5562, 0.5931, 0.7943]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00623, 0.6419, 0.5621, 0.5994, 0.7949]\n",
      "e_17 * test rst: [0.00626, 0.6458, 0.5503, 0.5942, 0.7953]\n",
      "e_18 * test rst: [0.00619, 0.6370, 0.5503, 0.5905, 0.7952]\n",
      "training end, used 144.47 s\n",
      "(0.006191744194018035, 0.636986301369863, 0.5502958579881657, 0.5904761904761905, 0.7951879789812848)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "2251 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00467, 0.7404, 0.4667, 0.5725, 0.8121]\n",
      "e_2 * test rst: [0.00434, 0.6629, 0.7030, 0.6824, 0.8357]\n",
      "e_3 * test rst: [0.00426, 0.6954, 0.6364, 0.6646, 0.8323]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00431, 0.6739, 0.7515, 0.7106, 0.8420]\n",
      "e_5 * test rst: [0.00527, 0.6168, 0.8000, 0.6966, 0.8449]\n",
      "e_6 * test rst: [0.00513, 0.6095, 0.7758, 0.6827, 0.8420]\n",
      "e_7 * test rst: [0.00448, 0.7202, 0.7333, 0.7267, 0.8402]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00487, 0.6418, 0.7818, 0.7049, 0.8461]\n",
      "e_9 * test rst: [0.00491, 0.6414, 0.7697, 0.6997, 0.8434]\n",
      "e_10 * test rst: [0.00463, 0.6796, 0.7455, 0.7110, 0.8394]\n",
      "e_11 * test rst: [0.00508, 0.6458, 0.7515, 0.6947, 0.8387]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00492, 0.6510, 0.7576, 0.7003, 0.8410]\n",
      "e_13 * test rst: [0.00508, 0.6410, 0.7576, 0.6944, 0.8436]\n",
      "e_14 * test rst: [0.00503, 0.6508, 0.7455, 0.6949, 0.8433]\n",
      "e_15 * test rst: [0.00500, 0.6508, 0.7455, 0.6949, 0.8430]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00515, 0.6480, 0.7697, 0.7036, 0.8426]\n",
      "e_17 * test rst: [0.00499, 0.6595, 0.7394, 0.6971, 0.8421]\n",
      "e_18 * test rst: [0.00513, 0.6492, 0.7515, 0.6966, 0.8417]\n",
      "training end, used 142.25 s\n",
      "(0.005129241328222471, 0.6492146596858639, 0.7515151515151515, 0.6966292134831461, 0.841660941912831)\n",
      "total label sum 184.5\n",
      "0.004842654611670526,0.6936416184971098,0.7058823529411765,0.6997084548104957,0.8518185900314323\n",
      "0.005145402737689696,0.7432432432432432,0.6470588235294118,0.6918238993710691,0.8607843137254901\n",
      "0.0056917830295926935,0.6621004566210046,0.7397959183673469,0.6987951807228916,0.8461602624701107\n",
      "0.006191744194018035,0.636986301369863,0.5502958579881657,0.5904761904761905,0.7951879789812848\n",
      "0.005129241328222471,0.6492146596858639,0.7515151515151515,0.6966292134831461,0.841660941912831\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.005400165180238685, 0.6770372558834169, 0.6789096208682505, 0.6754865877727585, 0.8391224174242298]\n",
      "mlt train ended\n",
      "0.675029,0.680460,0.677733,0.835435\n"
     ]
    }
   ],
   "source": [
    "# train_s_sz = 1500\n",
    "\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "args.l2_weight_decay = 1e-4\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    train_dev_idx = train_dev_idx[:]\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv(tar_feature, train_dev_idx), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.3,\n",
      " 'FC_dp': 0.1,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 100,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 18,\n",
      " 'l2_weight_decay': 0.0001,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 5.0,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 64,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 2,\n",
      " 'rnn_num_directions': 2,\n",
      " 'rnn_out_f_n': 68,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0.0001,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00548, 0.8095, 0.3000, 0.4378, 0.8145]\n",
      "e_2 * test rst: [0.00559, 0.5550, 0.7118, 0.6237, 0.8157]\n",
      "e_3 * test rst: [0.00670, 0.7143, 0.2941, 0.4167, 0.7969]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00463, 0.6687, 0.6294, 0.6485, 0.8352]\n",
      "e_5 * test rst: [0.00475, 0.7500, 0.5647, 0.6443, 0.8343]\n",
      "e_6 * test rst: [0.00462, 0.7114, 0.6235, 0.6646, 0.8389]\n",
      "e_7 * test rst: [0.00490, 0.6219, 0.7353, 0.6739, 0.8421]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00487, 0.7442, 0.5647, 0.6421, 0.8340]\n",
      "e_9 * test rst: [0.00473, 0.7574, 0.6059, 0.6732, 0.8372]\n",
      "e_10 * test rst: [0.00484, 0.7115, 0.6529, 0.6810, 0.8387]\n",
      "e_11 * test rst: [0.00486, 0.6725, 0.6765, 0.6745, 0.8407]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00547, 0.5733, 0.7824, 0.6617, 0.8416]\n",
      "e_13 * test rst: [0.00545, 0.5804, 0.7647, 0.6599, 0.8417]\n",
      "e_14 * test rst: [0.00528, 0.5860, 0.7412, 0.6545, 0.8426]\n",
      "e_15 * test rst: [0.00534, 0.5841, 0.7353, 0.6510, 0.8439]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00474, 0.7000, 0.6588, 0.6788, 0.8431]\n",
      "e_17 * test rst: [0.00474, 0.7315, 0.6412, 0.6834, 0.8414]\n",
      "e_18 * test rst: [0.00477, 0.7466, 0.6412, 0.6899, 0.8415]\n",
      "training end, used 134.90 s\n",
      "(0.004765982340961748, 0.7465753424657534, 0.6411764705882353, 0.689873417721519, 0.8415207304295764)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00521, 0.7857, 0.4529, 0.5746, 0.8444]\n",
      "e_2 * test rst: [0.00548, 0.8451, 0.3529, 0.4979, 0.8433]\n",
      "e_3 * test rst: [0.00456, 0.7447, 0.6176, 0.6752, 0.8567]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00452, 0.7320, 0.6588, 0.6935, 0.8639]\n",
      "e_5 * test rst: [0.00464, 0.6784, 0.6824, 0.6804, 0.8541]\n",
      "e_6 * test rst: [0.00547, 0.7885, 0.4824, 0.5985, 0.8509]\n",
      "e_7 * test rst: [0.00468, 0.7361, 0.6235, 0.6752, 0.8636]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00478, 0.6410, 0.7353, 0.6849, 0.8682]\n",
      "e_9 * test rst: [0.00475, 0.6721, 0.7235, 0.6969, 0.8634]\n",
      "e_10 * test rst: [0.00491, 0.6758, 0.7235, 0.6989, 0.8633]\n",
      "e_11 * test rst: [0.00534, 0.6305, 0.7529, 0.6863, 0.8665]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00571, 0.5966, 0.8176, 0.6898, 0.8691]\n",
      "e_13 * test rst: [0.00595, 0.5772, 0.8353, 0.6827, 0.8675]\n",
      "e_14 * test rst: [0.00554, 0.6233, 0.7882, 0.6961, 0.8669]\n",
      "e_15 * test rst: [0.00528, 0.6256, 0.7765, 0.6929, 0.8710]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00468, 0.7233, 0.6765, 0.6991, 0.8683]\n",
      "e_17 * test rst: [0.00468, 0.7219, 0.6412, 0.6791, 0.8683]\n",
      "e_18 * test rst: [0.00465, 0.7296, 0.6824, 0.7052, 0.8689]\n",
      "training end, used 134.90 s\n",
      "(0.004649069090746012, 0.7295597484276729, 0.6823529411764706, 0.7051671732522795, 0.8688968717257896)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "2250 563\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00624, 0.8137, 0.4235, 0.5570, 0.8096]\n",
      "e_2 * test rst: [0.00524, 0.6718, 0.6684, 0.6701, 0.8419]\n",
      "e_3 * test rst: [0.00551, 0.7246, 0.5102, 0.5988, 0.8352]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00503, 0.6619, 0.7092, 0.6847, 0.8426]\n",
      "e_5 * test rst: [0.00524, 0.7126, 0.6327, 0.6703, 0.8449]\n",
      "e_6 * test rst: [0.00541, 0.6651, 0.7398, 0.7005, 0.8387]\n",
      "e_7 * test rst: [0.00518, 0.6383, 0.7653, 0.6961, 0.8424]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00528, 0.6943, 0.6837, 0.6889, 0.8442]\n",
      "e_9 * test rst: [0.00558, 0.7143, 0.6378, 0.6739, 0.8425]\n",
      "e_10 * test rst: [0.00567, 0.7161, 0.5663, 0.6325, 0.8402]\n",
      "e_11 * test rst: [0.00592, 0.7520, 0.4796, 0.5857, 0.8402]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00533, 0.6538, 0.7806, 0.7116, 0.8418]\n",
      "e_13 * test rst: [0.00548, 0.6511, 0.7806, 0.7100, 0.8403]\n",
      "e_14 * test rst: [0.00564, 0.6245, 0.7806, 0.6939, 0.8388]\n",
      "e_15 * test rst: [0.00599, 0.6015, 0.8010, 0.6871, 0.8370]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00598, 0.6070, 0.7959, 0.6887, 0.8367]\n",
      "e_17 * test rst: [0.00590, 0.6126, 0.7908, 0.6904, 0.8374]\n",
      "e_18 * test rst: [0.00576, 0.6220, 0.7806, 0.6923, 0.8392]\n",
      "training end, used 134.73 s\n",
      "(0.005758338827858174, 0.6219512195121951, 0.7806122448979592, 0.6923076923076923, 0.8392231552021354)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "2251 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00575, 0.8182, 0.3195, 0.4596, 0.7702]\n",
      "e_2 * test rst: [0.00616, 0.6074, 0.5858, 0.5964, 0.7727]\n",
      "e_3 * test rst: [0.00584, 0.7447, 0.4142, 0.5323, 0.7811]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00593, 0.7805, 0.3787, 0.5100, 0.7791]\n",
      "e_5 * test rst: [0.00548, 0.6667, 0.5444, 0.5993, 0.7874]\n",
      "e_6 * test rst: [0.00575, 0.6410, 0.5917, 0.6154, 0.7880]\n",
      "e_7 * test rst: [0.00575, 0.6452, 0.5917, 0.6173, 0.7990]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n",
      "e_8 * test rst: [0.00598, 0.5594, 0.6686, 0.6092, 0.7991]\n",
      "e_9 * test rst: [0.00575, 0.6447, 0.5799, 0.6106, 0.8011]\n",
      "e_10 * test rst: [0.00598, 0.7188, 0.5444, 0.6195, 0.7923]\n",
      "e_11 * test rst: [0.00606, 0.7431, 0.4793, 0.5827, 0.7944]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00588, 0.6250, 0.6213, 0.6231, 0.8008]\n",
      "e_13 * test rst: [0.00597, 0.6325, 0.6213, 0.6269, 0.8005]\n",
      "e_14 * test rst: [0.00605, 0.6287, 0.6213, 0.6250, 0.8009]\n",
      "e_15 * test rst: [0.00623, 0.6171, 0.6391, 0.6279, 0.8008]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00615, 0.6335, 0.6036, 0.6182, 0.8001]\n",
      "e_17 * test rst: [0.00613, 0.6369, 0.5917, 0.6135, 0.8010]\n",
      "e_18 * test rst: [0.00614, 0.6494, 0.5917, 0.6192, 0.8012]\n",
      "training end, used 134.34 s\n",
      "(0.006140521744073922, 0.6493506493506493, 0.591715976331361, 0.6191950464396285, 0.8012331180270111)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "2251 562\n",
      "init model\n",
      "training begin\n",
      "e_1 * test rst: [0.00466, 0.6581, 0.6182, 0.6375, 0.8245]\n",
      "e_2 * test rst: [0.00478, 0.7119, 0.5091, 0.5936, 0.8167]\n",
      "e_3 * test rst: [0.00443, 0.7206, 0.5939, 0.6512, 0.8509]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.00398, 0.7718, 0.6970, 0.7325, 0.8461]\n",
      "e_5 * test rst: [0.00440, 0.6630, 0.7394, 0.6991, 0.8287]\n",
      "e_6 * test rst: [0.00584, 0.5366, 0.8000, 0.6423, 0.8376]\n",
      "e_7 * test rst: [0.00425, 0.6739, 0.7515, 0.7106, 0.8499]\n",
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch     6: reducing learning rate of group 1 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_8 * test rst: [0.00521, 0.5837, 0.7818, 0.6684, 0.8456]\n",
      "e_9 * test rst: [0.00594, 0.5344, 0.8000, 0.6408, 0.8429]\n",
      "e_10 * test rst: [0.00519, 0.5945, 0.7818, 0.6754, 0.8367]\n",
      "e_11 * test rst: [0.00452, 0.6703, 0.7515, 0.7086, 0.8360]\n",
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    10: reducing learning rate of group 1 to 1.2500e-04.\n",
      "e_12 * test rst: [0.00440, 0.6757, 0.7576, 0.7143, 0.8374]\n",
      "e_13 * test rst: [0.00452, 0.6720, 0.7697, 0.7175, 0.8384]\n",
      "e_14 * test rst: [0.00455, 0.6739, 0.7515, 0.7106, 0.8381]\n",
      "e_15 * test rst: [0.00459, 0.6798, 0.7333, 0.7055, 0.8380]\n",
      "Epoch    14: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    14: reducing learning rate of group 1 to 6.2500e-05.\n",
      "e_16 * test rst: [0.00434, 0.7375, 0.7152, 0.7262, 0.8358]\n",
      "e_17 * test rst: [0.00435, 0.7389, 0.7030, 0.7205, 0.8349]\n",
      "e_18 * test rst: [0.00441, 0.7405, 0.7091, 0.7245, 0.8348]\n",
      "training end, used 134.54 s\n",
      "(0.004412792878245332, 0.740506329113924, 0.7090909090909091, 0.7244582043343654, 0.8348370353408138)\n",
      "total label sum 184.5\n",
      "0.004765982340961748,0.7465753424657534,0.6411764705882353,0.689873417721519,0.8415207304295764\n",
      "0.004649069090746012,0.7295597484276729,0.6823529411764706,0.7051671732522795,0.8688968717257896\n",
      "0.005758338827858174,0.6219512195121951,0.7806122448979592,0.6923076923076923,0.8392231552021354\n",
      "0.006140521744073922,0.6493506493506493,0.591715976331361,0.6191950464396285,0.8012331180270111\n",
      "0.004412792878245332,0.740506329113924,0.7090909090909091,0.7244582043343654,0.8348370353408138\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.005145340976377038, 0.697588657774039, 0.680989708416987, 0.686200306811097, 0.8371421821450653]\n",
      "mlt train ended\n",
      "0.689455,0.683908,0.686671,0.822552\n"
     ]
    }
   ],
   "source": [
    "# all ann 1\n",
    "\n",
    "\n",
    "args.num_embedding = 64\n",
    "args.cnn_out_c = 100\n",
    "args.rnn_out_f_n = 68\n",
    "\n",
    "args.rnn_num_directions = 2\n",
    "args.rnn_layers = 2\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.3\n",
    "args.FC_dp = 0.1\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = False\n",
    "\n",
    "\n",
    "\n",
    "args.epochs = 18\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 1e-4\n",
    "args.max_grad_norm = 5.0\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "\n",
    "args.l2_weight_decay = 1e-4\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    train_dev_idx = train_dev_idx[:]\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv(tar_feature, train_dev_idx), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net(config).to(device)\n",
    "    model_init_w(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.0,\n",
      " 'FC_dp': 0.0,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 25,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 4,\n",
      " 'l2_weight_decay': 0,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 2,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 20,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 200,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 1,\n",
      " 'rnn_num_directions': 1,\n",
      " 'rnn_out_f_n': 100,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7480aebad84b8f975ced3d278e6c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4353.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e_1 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63b4c29da334756b3f1adf27f8d1741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev: 0.00885, 0.706, 0.733, 0.719, 0.917 *"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab24c3c59184bb8980a0047d8c9c31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=148.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test rst: [0.00801, 0.7928, 0.8438, 0.8175, 0.9520]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e7edd2be784b3d86af09fee5aaa2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4353.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e_2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5872fd374c064751b9f81db9b6eeed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev: 0.00970, 0.743, 0.682, 0.711, 0.913 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82992949e0f4d9dbe83e8a1a6208a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=148.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test rst: [0.00818, 0.8271, 0.8154, 0.8212, 0.9506]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06a3b2495704518817dd1f5229d23ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4353.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e_3 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdd7e83ecf643f98d79ad717a4c9569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev: 0.01040, 0.694, 0.735, 0.714, 0.912 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c57423019c640988978e27791ddd255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=148.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test rst: [0.00859, 0.7837, 0.8341, 0.8081, 0.9506]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7ea8e1098b47178926816025f74c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=4353.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e_4 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa298cb99b9404ca9b6139414c2d851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=157.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev: 0.01144, 0.707, 0.730, 0.718, 0.922 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8bb3109fe94a64af9de2bacb0e2327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=148.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test rst: [0.00962, 0.8059, 0.8438, 0.8244, 0.9543]\n",
      "saved checkpoint in ../models/abs_models/Ori_DisGeNet_abs.ckp\n",
      "training end, used 1666.61 s\n"
     ]
    }
   ],
   "source": [
    "# train on original model\n",
    "\n",
    "args.num_embedding = 200\n",
    "args.cnn_out_c = 25\n",
    "args.rnn_out_f_n = 100\n",
    "\n",
    "args.rnn_num_directions = 1\n",
    "args.rnn_layers = 1\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.0\n",
    "args.FC_dp = 0.0\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = True\n",
    "\n",
    "args.epochs = 4\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 0\n",
    "args.max_grad_norm = 20\n",
    "args.lr_reduce_factor = .5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 2\n",
    "args.use_loss_sh = True\n",
    "args.is_iterare_info = False\n",
    "args.l2_weight_decay = 0\n",
    "\n",
    "args.device = device\n",
    "\n",
    "args.modle_dir = '../models/abs_models'\n",
    "model_name_prefix = 'Ori_DisGeNet_abs'\n",
    "args.checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "args.config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "torch.save(config, args.config_save_f)\n",
    "\n",
    "model = Base_Net_Ori(config).to(device)\n",
    "model_init_w_ori(model)\n",
    "optimizer, scheduler = init_model_optimizer(model, args)\n",
    "\n",
    "train_dt, dev_dt, test_dt = tr_dataloader_rm, dev_dataloader_rm, dataloader_ori_t\n",
    "\n",
    "_ = train(model, optimizer, scheduler, train_dt, dev_dt, args, test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bff42943e64473858b754b213d5e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='evaluation', max=88.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0.028007570905773146, 0.6958174904942965, 0.6310344827586207, 0.6618444846292947, 0.8289799516093728)\n"
     ]
    }
   ],
   "source": [
    "args.modle_dir = '../models/abs_models'\n",
    "model_name_prefix = 'Ori_DisGeNet_abs'\n",
    "checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "config = torch.load(config_save_f)\n",
    "# model, _, _ = load_checkpoint(config, checkpoint_f)\n",
    "model, optimizer, scheduler = load_checkpoint(config, checkpoint_f, Base_Net_Ori)\n",
    "\n",
    "config.device =  args.device\n",
    "\n",
    "args.is_iterare_info = False\n",
    "args.threshold = config.threshold \n",
    "args.l2_weight_decay = config.l2_weight_decay\n",
    "# model.update_model_config(config)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "pred_l, tru_l, S, pred_o = eval(model, dataloader_ann_1, args, 'test')\n",
    "_, _, _, f1, auc = S\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ----------------\n",
      "{'EB_dp': 0.0,\n",
      " 'FC_dp': 0.0,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'batch_size': 32,\n",
      " 'cnn_out_c': 25,\n",
      " 'device': device(type='cuda'),\n",
      " 'epochs': 8,\n",
      " 'l2_weight_decay': 0,\n",
      " 'learning_rate': 0.001,\n",
      " 'lr_cooldown': 10,\n",
      " 'lr_reduce_factor': 0.5,\n",
      " 'max_grad_norm': 20,\n",
      " 'max_token_n': 54,\n",
      " 'not_x_feature': False,\n",
      " 'num_embedding': 200,\n",
      " 'num_words': 82949,\n",
      " 'patience_epoch': 3,\n",
      " 'rnn_layers': 1,\n",
      " 'rnn_num_directions': 1,\n",
      " 'rnn_out_f_n': 100,\n",
      " 'threshold': 0.5,\n",
      " 'use_new_loss': False,\n",
      " 'warmup_epoch': 0,\n",
      " 'weight_decay': 0,\n",
      " 'window_sizes': [2, 3, 4, 5]}\n",
      "       ----------------\n",
      "cv, step 1\n",
      "4984 563\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n",
      "e_1 * test rst: [0.01497, 0.6603, 0.6059, 0.6319, 0.8219]\n",
      "e_2 * test rst: [0.01502, 0.7500, 0.6882, 0.7178, 0.8578]\n",
      "e_3 * test rst: [0.01670, 0.6597, 0.7412, 0.6981, 0.8549]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.01726, 0.7342, 0.6824, 0.7073, 0.8515]\n",
      "e_5 * test rst: [0.02137, 0.7342, 0.6824, 0.7073, 0.8474]\n",
      "e_6 * test rst: [0.02530, 0.6117, 0.7412, 0.6702, 0.8480]\n",
      "e_7 * test rst: [0.02479, 0.6462, 0.7412, 0.6904, 0.8509]\n",
      "e_8 * test rst: [0.02595, 0.7289, 0.7118, 0.7202, 0.8502]\n",
      "training end, used 113.32 s\n",
      "(0.02595440800538283, 0.7289156626506024, 0.711764705882353, 0.7202380952380952, 0.8501870977398591)\n",
      "total label sum 197.5\n",
      "cv, step 2\n",
      "4984 563\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n",
      "e_1 * test rst: [0.01496, 0.7333, 0.4529, 0.5600, 0.8346]\n",
      "e_2 * test rst: [0.01447, 0.6807, 0.6647, 0.6726, 0.8646]\n",
      "e_3 * test rst: [0.01470, 0.6802, 0.6882, 0.6842, 0.8634]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.01407, 0.7639, 0.6471, 0.7006, 0.8788]\n",
      "e_5 * test rst: [0.02034, 0.8000, 0.6118, 0.6933, 0.8714]\n",
      "e_6 * test rst: [0.01964, 0.7362, 0.7059, 0.7207, 0.8752]\n",
      "e_7 * test rst: [0.02142, 0.7255, 0.6529, 0.6873, 0.8705]\n",
      "e_8 * test rst: [0.02179, 0.6422, 0.7706, 0.7005, 0.8751]\n",
      "training end, used 113.02 s\n",
      "(0.021789751351409016, 0.6421568627450981, 0.7705882352941177, 0.7005347593582888, 0.8751234845083071)\n",
      "total label sum 186.5\n",
      "cv, step 3\n",
      "4984 563\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n",
      "e_1 * test rst: [0.01567, 0.7305, 0.5255, 0.6113, 0.8170]\n",
      "e_2 * test rst: [0.01548, 0.6489, 0.7449, 0.6936, 0.8590]\n",
      "e_3 * test rst: [0.01537, 0.7414, 0.6582, 0.6973, 0.8602]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.01538, 0.7377, 0.6888, 0.7124, 0.8710]\n",
      "e_5 * test rst: [0.01827, 0.7500, 0.7194, 0.7344, 0.8777]\n",
      "e_6 * test rst: [0.02075, 0.7254, 0.7143, 0.7198, 0.8684]\n",
      "e_7 * test rst: [0.02040, 0.6792, 0.7347, 0.7059, 0.8663]\n",
      "e_8 * test rst: [0.02287, 0.6930, 0.7602, 0.7251, 0.8702]\n",
      "training end, used 116.04 s\n",
      "(0.022869113336447925, 0.6930232558139535, 0.7602040816326531, 0.7250608272506083, 0.8702107546015682)\n",
      "total label sum 219.0\n",
      "cv, step 4\n",
      "4985 562\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n",
      "e_1 * test rst: [0.01688, 0.6102, 0.4260, 0.5017, 0.7638]\n",
      "e_2 * test rst: [0.02067, 0.5975, 0.5621, 0.5793, 0.7828]\n",
      "e_3 * test rst: [0.02186, 0.5445, 0.6154, 0.5778, 0.7907]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.02055, 0.6443, 0.5680, 0.6038, 0.8031]\n",
      "e_5 * test rst: [0.02778, 0.7500, 0.4793, 0.5848, 0.8001]\n",
      "e_6 * test rst: [0.02819, 0.7177, 0.5266, 0.6075, 0.8056]\n",
      "e_7 * test rst: [0.02952, 0.6644, 0.5740, 0.6159, 0.7949]\n",
      "e_8 * test rst: [0.02961, 0.6071, 0.6036, 0.6053, 0.7987]\n",
      "training end, used 116.90 s\n",
      "(0.029605821596982215, 0.6071428571428571, 0.6035502958579881, 0.6053412462908011, 0.7986810605718415)\n",
      "total label sum 188.0\n",
      "cv, step 5\n",
      "4985 562\n",
      "init model\n",
      "Embedding(82949, 200, padding_idx=0)\n",
      "Linear(in_features=100, out_features=100, bias=True)\n",
      "Linear(in_features=100, out_features=1, bias=True)\n",
      "training begin\n",
      "e_1 * test rst: [0.01524, 0.6853, 0.5939, 0.6364, 0.7978]\n",
      "e_2 * test rst: [0.01527, 0.6188, 0.6788, 0.6474, 0.8456]\n",
      "e_3 * test rst: [0.01584, 0.6561, 0.7515, 0.7006, 0.8617]\n",
      "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     2: reducing learning rate of group 1 to 5.0000e-04.\n",
      "e_4 * test rst: [0.01440, 0.7413, 0.6424, 0.6883, 0.8788]\n",
      "e_5 * test rst: [0.02049, 0.7899, 0.5697, 0.6620, 0.8762]\n",
      "e_6 * test rst: [0.01999, 0.7812, 0.6061, 0.6826, 0.8791]\n",
      "e_7 * test rst: [0.02115, 0.7626, 0.6424, 0.6974, 0.8768]\n",
      "e_8 * test rst: [0.02020, 0.6667, 0.6909, 0.6786, 0.8795]\n",
      "training end, used 116.16 s\n",
      "(0.020197112630270554, 0.6666666666666666, 0.6909090909090909, 0.6785714285714286, 0.8795206472788337)\n",
      "total label sum 184.5\n",
      "0.02595440800538283,0.7289156626506024,0.711764705882353,0.7202380952380952,0.8501870977398591\n",
      "0.021789751351409016,0.6421568627450981,0.7705882352941177,0.7005347593582888,0.8751234845083071\n",
      "0.022869113336447925,0.6930232558139535,0.7602040816326531,0.7250608272506083,0.8702107546015682\n",
      "0.029605821596982215,0.6071428571428571,0.6035502958579881,0.6053412462908011,0.7986810605718415\n",
      "0.020197112630270554,0.6666666666666666,0.6909090909090909,0.6785714285714286,0.8795206472788337\n",
      "mean loss, prec, recall, f1, auc\n",
      "[0.024083241384098508, 0.6675810610038356, 0.7074032819152405, 0.6859492713418444, 0.854744608940082]\n",
      "mlt train ended\n",
      "0.667749,0.709195,0.687848,0.851868\n"
     ]
    }
   ],
   "source": [
    "args.num_embedding = 200\n",
    "args.cnn_out_c = 25\n",
    "args.rnn_out_f_n = 100\n",
    "\n",
    "args.rnn_num_directions = 1\n",
    "args.rnn_layers = 1\n",
    "args.window_sizes = [2, 3, 4, 5]\n",
    "args.EB_dp = 0.0\n",
    "args.FC_dp = 0.0\n",
    "\n",
    "args.use_new_loss = False\n",
    "args.use_cls_loss = True\n",
    "\n",
    "args.epochs = 8\n",
    "args.warmup_epoch = 0\n",
    "args.patience_epoch = 3\n",
    "\n",
    "args.learning_rate = 1e-3\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 0\n",
    "args.max_grad_norm = 20\n",
    "args.lr_reduce_factor = 0.5\n",
    "args.threshold = .5\n",
    "args.lr_cooldown = 10\n",
    "args.use_loss_sh = False\n",
    "args.is_iterare_info = True\n",
    "args.l2_weight_decay = 0\n",
    "\n",
    "\n",
    "config = set_model_config(args)\n",
    "    \n",
    "    \n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "s_arr = []\n",
    "run_cls_df_l = []\n",
    "\n",
    "tar_feature = features_ann_1\n",
    "for idx, (train_dev_idx, test_idx) in enumerate(kf.split(tar_feature[1])):\n",
    "    cv_train_dev_ds, cv_test_ds = convert_features_to_dataset_cv_aug(tar_feature, train_dev_idx, features_ss_aug), \\\n",
    "                             convert_features_to_dataset_cv(tar_feature, test_idx)\n",
    "    \n",
    "    print('cv, step {}'.format(idx+1))\n",
    "    print(len(cv_train_dev_ds), len(cv_test_ds))\n",
    "    \n",
    "    train_dl = DataLoader(cv_train_dev_ds, batch_size=args.batch_size)\n",
    "    test_dl = DataLoader(cv_test_ds, batch_size=args.batch_size)\n",
    "\n",
    "\n",
    "    model = Base_Net_Ori(config).to(device)\n",
    "    model_init_w_ori(model)\n",
    "    optimizer, scheduler = init_model_optimizer(model, args)\n",
    "    _ = train(model, optimizer, scheduler, train_dl, None, args, test_dl, False)\n",
    "\n",
    "\n",
    "    #test\n",
    "    pred_l, tru_l, S, pre_o = eval(model, test_dl, args, 'test')\n",
    "    _, _, _, f1, auc = S\n",
    "    print(S)\n",
    "    \n",
    "    s_arr.append(list(S))\n",
    "    \n",
    "    y_info = tar_feature[1].iloc[test_idx].copy()\n",
    "    y_info['pred'] = pred_l\n",
    "    y_info['prob'] = pre_o\n",
    "    run_cls_df_l.append(y_info)\n",
    "    print('total label sum', sum(y_info.label))\n",
    "    \n",
    "mer_pd_rst = pd.concat(run_cls_df_l)\n",
    "s_arr = np.array(s_arr)\n",
    "\n",
    "for r in s_arr:\n",
    "    print(','.join([str(i) for i in r]))\n",
    "\n",
    "\n",
    "print('mean loss, prec, recall, f1, auc')\n",
    "print(list(np.mean(s_arr, axis=0)))\n",
    "print('mlt train ended')\n",
    "\n",
    "tru_l = mer_pd_rst['label'].to_numpy()\n",
    "tru_l[tru_l==.5] = 0\n",
    "pred_l = mer_pd_rst['pred'].to_numpy()\n",
    "pred_l[pred_l==.5] = 0\n",
    "pred_l = pred_l.astype(int)\n",
    "precision, recall, f1, _ = \\\n",
    "                    precision_recall_fscore_support(tru_l, pred_l, average='binary',zero_division=1)\n",
    "auc_s = roc_auc_score(mer_pd_rst['label'].to_numpy(), mer_pd_rst['prob'].to_numpy())\n",
    "print('%f,%f,%f,%f' % (precision, recall, f1, auc_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _model_idx in range(10):\n",
    "#     args.num_embedding = 64\n",
    "#     args.cnn_out_c = 100\n",
    "#     args.rnn_out_f_n = 68\n",
    "\n",
    "#     args.rnn_num_directions = 2\n",
    "#     args.rnn_layers = 2\n",
    "#     args.window_sizes = [2, 3, 4, 5]\n",
    "#     args.EB_dp = 0.3\n",
    "#     args.FC_dp = 0.1\n",
    "\n",
    "#     args.use_new_loss = False\n",
    "#     args.use_cls_loss = False\n",
    "\n",
    "\n",
    "#     args.epochs = 20\n",
    "#     args.warmup_epoch = 0\n",
    "#     args.patience_epoch = 3\n",
    "\n",
    "#     args.learning_rate = 1e-3\n",
    "#     args.adam_epsilon = 1e-8\n",
    "#     args.weight_decay = 1e-4\n",
    "#     args.l2_weight_decay = 1e-4\n",
    "#     args.max_grad_norm = 5.0\n",
    "#     args.lr_reduce_factor = .5\n",
    "#     args.threshold = .5\n",
    "#     args.lr_cooldown = 2\n",
    "#     args.use_loss_sh = False\n",
    "#     args.is_iterare_info = True\n",
    "\n",
    "\n",
    "#     args.modle_dir = '../models/abs_models'\n",
    "#     model_name_prefix = 'Ann_abs'\n",
    "#     args.checkpoint_f = os.path.join(args.modle_dir, model_name_prefix + \".ckp\")\n",
    "#     args.config_save_f = os.path.join(args.modle_dir,  model_name_prefix + \".cf\")\n",
    "\n",
    "\n",
    "#     config = set_model_config(args)\n",
    "#     torch.save(config, args.config_save_f)\n",
    "\n",
    "#     model = Base_Net(config).to(device)\n",
    "#     model_init_w(model)\n",
    "#     optimizer, scheduler = init_model_optimizer(model, args)\n",
    "\n",
    "#     train_dt, dev_dt, test_dt = dataloader_merge, None, None\n",
    "\n",
    "#     _ = train(model, optimizer, scheduler, train_dt, dev_dt, args, test_dt, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading 1st data\n",
    "raw_data_dir = \"../data/ann_table/ann_1st.tsv\"\n",
    "s_df = pd.read_csv(raw_data_dir, sep='\\t', header=0)\n",
    "\n",
    "s_df['ann_label'] = \\\n",
    "s_df.apply(lambda x: 0.5 if (x['v2_class'] == 0) and  (x['Have -'] == '1') else x['v2_class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df1 = s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1, c2 = 'label', 'ann_label'\n",
    "nc1, nc2 = 'DisGeNet label', 'Annotated label'\n",
    "tar_s = s_df.groupby([c1, c2])[c2].size()\n",
    "tar_s.index = tar_s.index.rename([nc1, nc2])\n",
    "\n",
    "tar_s = tar_s.unstack(level=-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,2)) \n",
    "ax = sn.heatmap(tar_s, cmap='Oranges', annot=True, fmt=\"d\", linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_label\n",
      "0.0    1732\n",
      "0.5     211\n",
      "1.0     870\n",
      "Name: ann_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f5a6449bfd0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(s_df.groupby(['ann_label'])['ann_label'].size())\n",
    "s_df.groupby(['ann_label'])['ann_label'].size().plot(kind='pie', subplots=True, startangle=90,\n",
    "figsize=(5,5), autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading 2nd data\n",
    "raw_data_dir = \"../data/ann_table/ann_2nd.tsv\"\n",
    "s_df = pd.read_csv(raw_data_dir, sep='\\t', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df['new_label'] = 0.\n",
    "s_df.loc[pd.isna(s_df['r1_label']), 'new_label'] = s_df[pd.isna(s_df['r1_label'])]['label']\n",
    "s_df.loc[~pd.isna(s_df['r1_label']), 'new_label'] = s_df[~pd.isna(s_df['r1_label'])]['r1_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df['ann_label'] = \\\n",
    "s_df.apply(lambda x: 0.5 if (x['new_label'] == 0) and  (x['Have -'] == '1') else x['new_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1, c2 = 'label', 'ann_label'\n",
    "nc1, nc2 = 'DisGeNet label', 'Annotated label'\n",
    "tar_s = s_df.groupby([c1, c2])[c2].size()\n",
    "tar_s.index = tar_s.index.rename([nc1, nc2])\n",
    "\n",
    "tar_s = tar_s.unstack(level=-1)\n",
    "fig, ax = plt.subplots(figsize=(5,2)) \n",
    "# tar_s = tar_s.transpose()\n",
    "ax = sn.heatmap(tar_s, cmap='Oranges', annot=True, fmt=\"d\", linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_label\n",
      "0.0    1853\n",
      "0.5      47\n",
      "1.0     834\n",
      "Name: ann_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5a64883e50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(s_df.groupby(['ann_label'])['ann_label'].size())\n",
    "s_df.groupby(['ann_label'])['ann_label'].size().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2, n3 = 'positivel', 'ambiguous', 'negative'\n",
    "\n",
    "tar_s1 = s_df1.groupby(['ann_label'])['ann_label'].size()\n",
    "tar_s1 = tar_s1.rename(index={1: n1, 0.5:n2, 0: n3})\n",
    "tar_s1 = tar_s1.to_frame()\n",
    "tar_s1 = tar_s1.rename_axis(None)\n",
    "tar_s1 = tar_s1.rename(columns={'ann_label': '1st annotated abstract dataset'})\n",
    "\n",
    "# del tar_s1.index.name\n",
    "\n",
    "tar_s = s_df.groupby(['ann_label'])['ann_label'].size()\n",
    "tar_s = tar_s.rename(index={1: n1, 0.5:n2, 0: n3})\n",
    "tar_s = tar_s.to_frame()\n",
    "tar_s = tar_s.rename_axis(None)\n",
    "tar_s = tar_s.rename(columns={'ann_label': '2nd annotated abstract dataset'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "mer_s = tar_s1.merge(tar_s, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st annotated abstract dataset</th>\n",
       "      <th>2nd annotated abstract dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1732</td>\n",
       "      <td>1853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ambiguous</th>\n",
       "      <td>211</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positivel</th>\n",
       "      <td>870</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1st annotated abstract dataset  2nd annotated abstract dataset\n",
       "negative                             1732                            1853\n",
       "ambiguous                             211                              47\n",
       "positivel                             870                             834"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mer_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mer_s.plot.pie(subplots=True, figsize=(12, 6), startangle=0,  autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           1st annotated abstract dataset\n",
       " negative                             1732\n",
       " ambiguous                             211\n",
       " positivel                             870,\n",
       "            2nd annotated abstract dataset\n",
       " negative                             1853\n",
       " ambiguous                              47\n",
       " positivel                             834)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_s1, tar_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/bal31/jhsu/home/anaconda3/envs/venv/lib/python3.7/site-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = {'Yes':pd.Series([825, 56], index=[\"Total\", \"Last 2 Month\"]), 'No':pd.Series([725, 73], index=[\"Total\", \"Last 2 Month\"])}\n",
    "df = pd.DataFrame(d)\n",
    "df = df.T\n",
    "def absolute_value(val):\n",
    "    a  = np.round(val/100.*df.values, 0)\n",
    "    return a\n",
    "\n",
    "df.plot.pie(subplots=True, figsize=(12, 6),autopct=absolute_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Last 2 Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>825</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>725</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total  Last 2 Month\n",
       "Yes    825            56\n",
       "No     725            73"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00b6a19122454087be2679522fa6f268": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_67a3c39002e54a7eb807216e0e6c7a3e",
        "IPY_MODEL_b157317f82b343a3a7f12696b764b88d"
       ],
       "layout": "IPY_MODEL_e439ca65dee24359a000a505ed41a480"
      }
     },
     "07e05f82a76149c6aef1fdb2b608388a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "08211dc7247c4ff5ac7dcdd7aea20e2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "0875182a1b4747d2aa00f5d9152b655f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "08e51381a31f428cab096054e747f71f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1fc51c75b58a409f8832651aa2f4818e",
        "IPY_MODEL_f988f4156b17413290924e4464703cb1"
       ],
       "layout": "IPY_MODEL_bdd547e134c04deb910d4bba67c985c2"
      }
     },
     "097644bf303f42ba8cb98cbf57eff30e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "09f4d332b91f46229b81ffd0a64326db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "0a236c04424d4beebc8550b66d70a604": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "0b031c8a89924c1caecc82edb9475327": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_79ace6c1f33d46db921692184459eb28",
        "IPY_MODEL_5bac2cb1977744eda283d07d3f9a49f2"
       ],
       "layout": "IPY_MODEL_835b4db9434a468facdc052491b44170"
      }
     },
     "0ca5c7c74ea0414c94c86945493c81ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.00252, 0.927, 0.904, 0.915,  0.978/ BF 0.915} : 100%",
       "layout": "IPY_MODEL_bc386895a30844329164a7dbf0783263",
       "max": 4353,
       "style": "IPY_MODEL_0fd6e2eccf5646b3a785ec6fd2c96d7b",
       "value": 4353
      }
     },
     "0fd6e2eccf5646b3a785ec6fd2c96d7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "1023fc59857d47b1bf9025273ad3b13c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "11fe27ce89cc4043959abd8cf5e6b713": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "14f0e17827ae4efc90933ea1bdd4457c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.17114, 0.665, 0.535, 0.593,  0.802/ BF 0.593} : 100%",
       "layout": "IPY_MODEL_7338072c8dc44303a7bfcbffae21a9bb",
       "max": 174,
       "style": "IPY_MODEL_a60b3bf1a0f0439ca3759ea1d2c045ed",
       "value": 174
      }
     },
     "163c863266254898a76b67c105adfd10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_78480c07ab904c448fe5888b3432c839",
        "IPY_MODEL_ee14c53b32cf4662806cdbf976869636"
       ],
       "layout": "IPY_MODEL_f8e9ec2e9f5d463da62eddad475cabba"
      }
     },
     "18975cb9da8b4a66b26a558ada57eb10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "1aabb28858544509b422938b3b06c474": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "1e2c414a8b3240c3b232f7cb9f135d64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "1e75918cf10f4cfdb71b3e33a476457e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "1fc51c75b58a409f8832651aa2f4818e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.00829, 0.871, 0.821, 0.845,  0.955/ BF 0.847} : 100%",
       "layout": "IPY_MODEL_2d3784c023ca4072a68dec782edd6840",
       "max": 4353,
       "style": "IPY_MODEL_d7cc5c9306ed43f696f31e7d7efc6d9a",
       "value": 4353
      }
     },
     "23222f0b5fe74fd1ae56c0adad692bb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "It { L 0.02153, 0.884, 0.959, 0.920,  0.966/ BF 0.927} :  41%",
       "layout": "IPY_MODEL_62520786ba34495998782320bf66f8f3",
       "max": 174,
       "style": "IPY_MODEL_11fe27ce89cc4043959abd8cf5e6b713",
       "value": 72
      }
     },
     "247aa67b0d524ce591f032e43089dca4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26f547733c0c469d8cff89e810b297d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_23222f0b5fe74fd1ae56c0adad692bb8",
        "IPY_MODEL_ed9e400772c643aa9f8410064a2b7c28"
       ],
       "layout": "IPY_MODEL_990759c2ffbd433f93934326e545a52e"
      }
     },
     "2830f753715b42a5a58bad02c53eaf60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.01268, 0.868, 0.819, 0.843,  0.956/ BF 0.845} : 100%",
       "layout": "IPY_MODEL_bdf2a6a235574886a49b75a027e57414",
       "max": 4353,
       "style": "IPY_MODEL_c5c72d85dcd049cc85e503ed5da2e933",
       "value": 4353
      }
     },
     "2af8fc86a5f64e71876a0789ce32e184": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "2ba6113093174861b6325a0d8c8ffb53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2c6247b18d654affb989d2053958ca1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "2d3784c023ca4072a68dec782edd6840": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "3065dc8950b74b4b984b66a703c0f02d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_14f0e17827ae4efc90933ea1bdd4457c",
        "IPY_MODEL_b6d3a62e47c24bb5a54198b51f6ad1f0"
       ],
       "layout": "IPY_MODEL_eb7e38089e7b4914913927da6b0847a6"
      }
     },
     "32727aeae87f4dc5b18580c3de79fb30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_999209b5ce9e4445a1c5703544d40a9d",
       "style": "IPY_MODEL_1023fc59857d47b1bf9025273ad3b13c",
       "value": " 4353/4353 [22:57&lt;00:00,  3.16it/s]"
      }
     },
     "333e1942561641b2acb241acbf0063fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "334c6b14a9d3477c805707ff58c1a8e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "34ccbf88aa5542b68da5ff76a5c7c274": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "352e7c65ac0b487c9b1ceae9dd2cdd30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "3924debf12d3457794903e64d9a182b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fa076cd196b34fdcafcac2d9110ec12e",
        "IPY_MODEL_9b5507391f684217a8a859ca9eed3e34"
       ],
       "layout": "IPY_MODEL_1e75918cf10f4cfdb71b3e33a476457e"
      }
     },
     "39c695f951e140a4bb61f4e503fc35de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3a59adf3c26447e89fceaee696ae7fab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3d6dd986de964880925afe9af62dbf28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3e8d5adc640244aa9cba6d0a57a9f742": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "400eeae29a70417189bdb3b861bf275b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "40fed54cde3c4054aff1f86e0097a003": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4704debba3574352888100dcf3a0fda4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_abf5ba7cbe574e2e8638e531bc7303f7",
        "IPY_MODEL_d2b371608d444db88dcf65840bfad964"
       ],
       "layout": "IPY_MODEL_f4b040a930b44e699f22be5c05ff6d42"
      }
     },
     "4825005b81e049bdb5b5fa385d1f1543": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_81f2a85f942b428a8cb97f3a4aaa2799",
       "style": "IPY_MODEL_cc24e45743f147fcb177e2eb2680e4aa",
       "value": " 174/174 [00:13&lt;00:00, 13.35it/s]"
      }
     },
     "49273e5401a34509914842dfc87c217d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.02530, 0.816, 0.724, 0.767,  0.920/ BF 0.767} : 100%",
       "layout": "IPY_MODEL_f4758f51c0cc4a4fa8289405657f4498",
       "max": 4353,
       "style": "IPY_MODEL_fb2ffabe3d2e4c0cb5e8ce128d3cbbc7",
       "value": 4353
      }
     },
     "4bdcd0bde26d440e9357fa93c1603392": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4cc47e2021224d65bead9a91ebab69c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "4f8407b099ae4fc7a6368fabe730f51a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "503ba4e658b140c3acc246c48017a6e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "523573c6f65a43eea83ff94dcb6ded30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.13428, 0.809, 0.804, 0.806,  0.927/ BF 0.810} : 100%",
       "layout": "IPY_MODEL_7d742e3604cb4e8eb783cb45aeee870b",
       "max": 174,
       "style": "IPY_MODEL_2af8fc86a5f64e71876a0789ce32e184",
       "value": 174
      }
     },
     "546087b1d49346a8a1f0ed6afef1fb16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "5614e8f87a2c4d3f94a1352f8de62094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "5657752a2f324c1eb41d3f435b0c7cf7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "567ca2c6436a407482b0224981a1c96d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "56b327c36bed477682e6bdf9c74cceb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5bac2cb1977744eda283d07d3f9a49f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_70268e1e8c4f4a009bfed18e971d50f0",
       "style": "IPY_MODEL_b90ec11542134681bc6acc828bba79ba",
       "value": " 174/174 [00:18&lt;00:00,  9.53it/s]"
      }
     },
     "61a710f916914b6b8df7bfbae02f5f97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a81313bae19c4577998a610e580190a6",
       "style": "IPY_MODEL_daabd551357048078a487d73f6855027",
       "value": " 174/174 [01:05&lt;00:00,  2.65it/s]"
      }
     },
     "62520786ba34495998782320bf66f8f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "627b5107c1d34e04830c105f72824a95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "65c385ff8f4046b5b7799ceb258389b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.04631, 0.888, 0.950, 0.918,  0.976/ BF 0.918} : 100%",
       "layout": "IPY_MODEL_5614e8f87a2c4d3f94a1352f8de62094",
       "max": 174,
       "style": "IPY_MODEL_0875182a1b4747d2aa00f5d9152b655f",
       "value": 174
      }
     },
     "67a3c39002e54a7eb807216e0e6c7a3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.00765, 0.900, 0.866, 0.883,  0.968/ BF 0.884} : 100%",
       "layout": "IPY_MODEL_71b2aeaa8c4d4d95b8550787ed970651",
       "max": 4353,
       "style": "IPY_MODEL_fdd0f2cb314641b8b9c6d587547751e5",
       "value": 4353
      }
     },
     "6f1feee44a774e3a85e77447f6bd51fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "70268e1e8c4f4a009bfed18e971d50f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "715a822107eb4e5a892ca8c429cb0d09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.05231, 0.872, 0.943, 0.907,  0.972/ BF 0.907} : 100%",
       "layout": "IPY_MODEL_1e2c414a8b3240c3b232f7cb9f135d64",
       "max": 174,
       "style": "IPY_MODEL_352e7c65ac0b487c9b1ceae9dd2cdd30",
       "value": 174
      }
     },
     "71b2aeaa8c4d4d95b8550787ed970651": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7277f674ec3a4df9b66f0271b2bf5492": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "7338072c8dc44303a7bfcbffae21a9bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "756633856224416b8e9c36c3605422c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "78480c07ab904c448fe5888b3432c839": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.00186, 0.927, 0.906, 0.916,  0.978/ BF 0.916} : 100%",
       "layout": "IPY_MODEL_c2c31132b9f141dd9459992fc7216de3",
       "max": 4353,
       "style": "IPY_MODEL_f66f214dcfa648f4b8fa984dab592392",
       "value": 4353
      }
     },
     "79ace6c1f33d46db921692184459eb28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.09749, 0.763, 0.709, 0.735,  0.888/ BF 0.735} : 100%",
       "layout": "IPY_MODEL_c27779c53e0746d39cf4b3ff5a8c3d77",
       "max": 174,
       "style": "IPY_MODEL_99b77728488d4a4c9a04cce6ae36442c",
       "value": 174
      }
     },
     "7b12270c35bf43dbb5ebeebbdc3768bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cbac94b603364979bd31e85c7e77dbce",
        "IPY_MODEL_ca28202015d44406b662ce463132eaa6"
       ],
       "layout": "IPY_MODEL_9cba73c6a0ef491291287c183ef2d4fd"
      }
     },
     "7d684121338d4cd4a23b542cb79f9e18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7d742e3604cb4e8eb783cb45aeee870b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "7e58ed006a444db083b69075e54b8f35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_715a822107eb4e5a892ca8c429cb0d09",
        "IPY_MODEL_cad9e078f4bc4cc58b1b19805341c202"
       ],
       "layout": "IPY_MODEL_1aabb28858544509b422938b3b06c474"
      }
     },
     "81f2a85f942b428a8cb97f3a4aaa2799": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "82f89ef8714b4505aed02f44de64f605": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_34ccbf88aa5542b68da5ff76a5c7c274",
       "style": "IPY_MODEL_40fed54cde3c4054aff1f86e0097a003",
       "value": " 4353/4353 [09:21&lt;00:00,  7.76it/s]"
      }
     },
     "835b4db9434a468facdc052491b44170": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "87965c469b284969a88920553dd1d98e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.00577, 0.902, 0.863, 0.882,  0.967/ BF 0.882} : 100%",
       "layout": "IPY_MODEL_dedfb89e8a5a4193a542d1810469051e",
       "max": 4353,
       "style": "IPY_MODEL_9c0ac6dca7614acca32433dcab0f92f0",
       "value": 4353
      }
     },
     "87c3653b856d4060a9c28a5f5bae96bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c650706bbd74aeda50c9e031c7ca16a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.05223, 0.895, 0.956, 0.925,  0.976/ BF 0.927} : 100%",
       "layout": "IPY_MODEL_8f2b076786ed46188d8eb9368c7ca017",
       "max": 174,
       "style": "IPY_MODEL_d273e9d291c1463a87d1744b9b7375f1",
       "value": 174
      }
     },
     "8dfb4981def74335b56c1cc4ea2645cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "8e20f3719373489e923dd23f58a27231": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "8f2b076786ed46188d8eb9368c7ca017": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "909b06b420f74fbe854670e115d29114": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "94cf63e8807a44788ce6f30041f517d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98de5bd2d62f488683cfede22fbf44a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_65c385ff8f4046b5b7799ceb258389b8",
        "IPY_MODEL_b9f3e5a038f44e59808d72595d4b5a80"
       ],
       "layout": "IPY_MODEL_567ca2c6436a407482b0224981a1c96d"
      }
     },
     "990759c2ffbd433f93934326e545a52e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "999209b5ce9e4445a1c5703544d40a9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99b77728488d4a4c9a04cce6ae36442c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "9aaca73fc1e244bbbcfd6ec330b25e35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.07817, 0.819, 0.858, 0.838,  0.951/ BF 0.838} : 100%",
       "layout": "IPY_MODEL_18975cb9da8b4a66b26a558ada57eb10",
       "max": 174,
       "style": "IPY_MODEL_7277f674ec3a4df9b66f0271b2bf5492",
       "value": 174
      }
     },
     "9b5507391f684217a8a859ca9eed3e34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3d6dd986de964880925afe9af62dbf28",
       "style": "IPY_MODEL_909b06b420f74fbe854670e115d29114",
       "value": " 174/174 [00:18&lt;00:00,  9.49it/s]"
      }
     },
     "9c0ac6dca7614acca32433dcab0f92f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "9cba73c6a0ef491291287c183ef2d4fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "a60b3bf1a0f0439ca3759ea1d2c045ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "a76ba3aa946c448ab3a044befcd94263": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a7a86f7dc05f47b6a496e8fcb7ef6b3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a81313bae19c4577998a610e580190a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aaf70c721e8e4f2eb73f5577f26a3b3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e72377ff852747b18af3424ad2374088",
       "style": "IPY_MODEL_3a59adf3c26447e89fceaee696ae7fab",
       "value": " 174/174 [00:18&lt;00:00,  9.49it/s]"
      }
     },
     "abf5ba7cbe574e2e8638e531bc7303f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.01887, 0.816, 0.726, 0.769,  0.920/ BF 0.769} : 100%",
       "layout": "IPY_MODEL_c78ec3d359404db1bd1d3692d8aaa433",
       "max": 4353,
       "style": "IPY_MODEL_8dfb4981def74335b56c1cc4ea2645cd",
       "value": 4353
      }
     },
     "b0da8abb43ca48309fbfece666dc75ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b157317f82b343a3a7f12696b764b88d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2ba6113093174861b6325a0d8c8ffb53",
       "style": "IPY_MODEL_400eeae29a70417189bdb3b861bf275b",
       "value": " 4353/4353 [13:37&lt;00:00,  5.32it/s]"
      }
     },
     "b24bb073ebee4d23842dd0cbbc2e44f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b2dc07ceff4249f1b549dab4864c1b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "b6d3a62e47c24bb5a54198b51f6ad1f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7a86f7dc05f47b6a496e8fcb7ef6b3a",
       "style": "IPY_MODEL_756633856224416b8e9c36c3605422c7",
       "value": " 174/174 [01:24&lt;00:00,  2.07it/s]"
      }
     },
     "b6dc047654e5402488a8c1c5f5c6709d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "b90ec11542134681bc6acc828bba79ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b9f3e5a038f44e59808d72595d4b5a80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4bdcd0bde26d440e9357fa93c1603392",
       "style": "IPY_MODEL_503ba4e658b140c3acc246c48017a6e5",
       "value": " 174/174 [13:05:57&lt;00:00, 271.02s/it]"
      }
     },
     "bc386895a30844329164a7dbf0783263": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "bce587bed1c74de2a07d75f5ff32e6b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2830f753715b42a5a58bad02c53eaf60",
        "IPY_MODEL_82f89ef8714b4505aed02f44de64f605"
       ],
       "layout": "IPY_MODEL_08211dc7247c4ff5ac7dcdd7aea20e2c"
      }
     },
     "bdd547e134c04deb910d4bba67c985c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "bdf2a6a235574886a49b75a027e57414": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "be6647cb07124cc6a9d3bd26e0d6092b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3e8d5adc640244aa9cba6d0a57a9f742",
       "style": "IPY_MODEL_b24bb073ebee4d23842dd0cbbc2e44f4",
       "value": " 4353/4353 [04:46&lt;00:00, 15.20it/s]"
      }
     },
     "c27779c53e0746d39cf4b3ff5a8c3d77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "c2c31132b9f141dd9459992fc7216de3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "c5c72d85dcd049cc85e503ed5da2e933": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c68ce0effc1b481a8828b217715cfe89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_87965c469b284969a88920553dd1d98e",
        "IPY_MODEL_f2b1b9dad3f1450a9a31406d24e2c433"
       ],
       "layout": "IPY_MODEL_627b5107c1d34e04830c105f72824a95"
      }
     },
     "c78ec3d359404db1bd1d3692d8aaa433": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "ca28202015d44406b662ce463132eaa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_247aa67b0d524ce591f032e43089dca4",
       "style": "IPY_MODEL_7d684121338d4cd4a23b542cb79f9e18",
       "value": " 174/174 [00:46&lt;00:00,  3.73it/s]"
      }
     },
     "cad9e078f4bc4cc58b1b19805341c202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_333e1942561641b2acb241acbf0063fe",
       "style": "IPY_MODEL_d28b5dd24e6f4a60b87616ee3de5b207",
       "value": " 174/174 [00:27&lt;00:00,  6.29it/s]"
      }
     },
     "cb4e89c39e5c472b951101fa40c41bc7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cbac94b603364979bd31e85c7e77dbce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.05539, 0.852, 0.908, 0.879,  0.967/ BF 0.879} : 100%",
       "layout": "IPY_MODEL_0a236c04424d4beebc8550b66d70a604",
       "max": 174,
       "style": "IPY_MODEL_b2dc07ceff4249f1b549dab4864c1b4e",
       "value": 174
      }
     },
     "cc24e45743f147fcb177e2eb2680e4aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d273e9d291c1463a87d1744b9b7375f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "d28b5dd24e6f4a60b87616ee3de5b207": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d2b371608d444db88dcf65840bfad964": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb4e89c39e5c472b951101fa40c41bc7",
       "style": "IPY_MODEL_56b327c36bed477682e6bdf9c74cceb2",
       "value": " 4353/4353 [18:43&lt;00:00,  3.87it/s]"
      }
     },
     "d36ec84dc9be4c3e95eef1dda1cf210b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d4054f2608f5456390b3415680bebb87",
        "IPY_MODEL_e94a8671f8d24c499b72c9af20597339"
       ],
       "layout": "IPY_MODEL_8e20f3719373489e923dd23f58a27231"
      }
     },
     "d4054f2608f5456390b3415680bebb87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.05134, 0.882, 0.927, 0.904,  0.973/ BF 0.907} : 100%",
       "layout": "IPY_MODEL_4cc47e2021224d65bead9a91ebab69c3",
       "max": 174,
       "style": "IPY_MODEL_4f8407b099ae4fc7a6368fabe730f51a",
       "value": 174
      }
     },
     "d61840085c7b4761b9b13adfc86210bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d68ae1766f06446699097f571f3f9cae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_523573c6f65a43eea83ff94dcb6ded30",
        "IPY_MODEL_61a710f916914b6b8df7bfbae02f5f97"
       ],
       "layout": "IPY_MODEL_09f4d332b91f46229b81ffd0a64326db"
      }
     },
     "d7cc5c9306ed43f696f31e7d7efc6d9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "d8d161dbe003448bb5131cf37113c74b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_49273e5401a34509914842dfc87c217d",
        "IPY_MODEL_32727aeae87f4dc5b18580c3de79fb30"
       ],
       "layout": "IPY_MODEL_546087b1d49346a8a1f0ed6afef1fb16"
      }
     },
     "daabd551357048078a487d73f6855027": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dc145ac6a4b84a9e9ef5015b301af9d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dedfb89e8a5a4193a542d1810469051e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "e2ee757ff1684f1f9e0a8f62cd89d531": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0ca5c7c74ea0414c94c86945493c81ab",
        "IPY_MODEL_be6647cb07124cc6a9d3bd26e0d6092b"
       ],
       "layout": "IPY_MODEL_5657752a2f324c1eb41d3f435b0c7cf7"
      }
     },
     "e439ca65dee24359a000a505ed41a480": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "e5cabab7f13f42fb81dde667eff08437": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8c650706bbd74aeda50c9e031c7ca16a",
        "IPY_MODEL_4825005b81e049bdb5b5fa385d1f1543"
       ],
       "layout": "IPY_MODEL_07e05f82a76149c6aef1fdb2b608388a"
      }
     },
     "e6b65458345c4387b105fc05d1da0347": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9aaca73fc1e244bbbcfd6ec330b25e35",
        "IPY_MODEL_aaf70c721e8e4f2eb73f5577f26a3b3b"
       ],
       "layout": "IPY_MODEL_2c6247b18d654affb989d2053958ca1d"
      }
     },
     "e72377ff852747b18af3424ad2374088": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e94a8671f8d24c499b72c9af20597339": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39c695f951e140a4bb61f4e503fc35de",
       "style": "IPY_MODEL_334c6b14a9d3477c805707ff58c1a8e6",
       "value": " 174/174 [00:18&lt;00:00,  9.40it/s]"
      }
     },
     "eb7e38089e7b4914913927da6b0847a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "ed9e400772c643aa9f8410064a2b7c28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_87c3653b856d4060a9c28a5f5bae96bb",
       "style": "IPY_MODEL_6f1feee44a774e3a85e77447f6bd51fc",
       "value": " 72/174 [00:20&lt;00:05, 20.16it/s]"
      }
     },
     "ee14c53b32cf4662806cdbf976869636": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a76ba3aa946c448ab3a044befcd94263",
       "style": "IPY_MODEL_dc145ac6a4b84a9e9ef5015b301af9d1",
       "value": " 4353/4353 [07:32&lt;00:00,  9.62it/s]"
      }
     },
     "f2b1b9dad3f1450a9a31406d24e2c433": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f4c923d28fff4d88828d37f29bb346f8",
       "style": "IPY_MODEL_d61840085c7b4761b9b13adfc86210bf",
       "value": " 4353/4353 [09:25&lt;00:00,  7.70it/s]"
      }
     },
     "f4758f51c0cc4a4fa8289405657f4498": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "f4b040a930b44e699f22be5c05ff6d42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "f4c923d28fff4d88828d37f29bb346f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f66f214dcfa648f4b8fa984dab592392": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "f8e9ec2e9f5d463da62eddad475cabba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "800px"
      }
     },
     "f988f4156b17413290924e4464703cb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_94cf63e8807a44788ce6f30041f517d7",
       "style": "IPY_MODEL_097644bf303f42ba8cb98cbf57eff30e",
       "value": " 4353/4353 [09:17&lt;00:00,  7.81it/s]"
      }
     },
     "fa076cd196b34fdcafcac2d9110ec12e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "It { L 0.06527, 0.862, 0.939, 0.899,  0.970/ BF 0.899} : 100%",
       "layout": "IPY_MODEL_b0da8abb43ca48309fbfece666dc75ee",
       "max": 174,
       "style": "IPY_MODEL_b6dc047654e5402488a8c1c5f5c6709d",
       "value": 174
      }
     },
     "fb2ffabe3d2e4c0cb5e8ce128d3cbbc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "fdd0f2cb314641b8b9c6d587547751e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
