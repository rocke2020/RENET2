{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = \"../data/ft_data/\"\n",
    "new_dir = './BeFree/n_out/'\n",
    "!cp ../data/ft_data/labels_n.txt ./BeFree/n_out/labels.txt\n",
    "FT_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_dir = \"../data/abs_data/2nd_ann/\"\n",
    "# new_dir = './BeFree/abs_out/'\n",
    "# !cp ../data/abs_data/2nd_ann/labels.txt ./BeFree/abs_out/labels.txt\n",
    "# FT_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate gene/disease annotation for BeFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "sentence_path = os.path.join(raw_data_dir, \"sentences.txt\")\n",
    "ner_path = os.path.join(raw_data_dir, \"anns.txt\")\n",
    "text_path = os.path.join(raw_data_dir, \"docs.txt\")\n",
    "\n",
    "\n",
    "\n",
    "new_gene_file = open(new_dir+\"genes_FINAL.befree\", 'w')\n",
    "new_disease_file = open(new_dir+\"diseases_FINAL.befree\", 'w')\n",
    "\n",
    "\n",
    "text_file = open(text_path, \"r\")\n",
    "sentence_file = open(sentence_path, \"r\")\n",
    "ner_file = open(ner_path, \"r\")\n",
    "\n",
    "tar_pmid_lst = None\n",
    "\n",
    "def get_sent_offset(sentences, text):\n",
    "    sent_offset = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i == 0:\n",
    "            offset = 0\n",
    "        else:\n",
    "            offset = last_offset + len(last_sentence)\n",
    "        if sentence != text[offset: offset+len(sentence)]:\n",
    "            offset = offset + text[offset:].find(sentence)\n",
    "        sent_offset.append(offset)        \n",
    "        last_offset = offset\n",
    "        last_sentence = sentence\n",
    "    return sent_offset\n",
    "\n",
    "def Sent_no(sent_offset, offset):\n",
    "    return np.digitize(offset, sent_offset).tolist() - 1\n",
    "\n",
    "\n",
    "def offset_in_sent(sentences_offset, sent_no, begin, end):\n",
    "    begin_offset = begin - sentences_offset[sent_no]\n",
    "    end_offset = end - sentences_offset[sent_no]\n",
    "    return str(begin_offset) + \"#\" + str(end_offset)\n",
    "\n",
    "def get_sentence(sentences, row):\n",
    "    return sentences[sent_no]\n",
    "    \n",
    "    \n",
    "\n",
    "_idx = 1\n",
    "while (1):\n",
    "    line = text_file.readline()\n",
    "    if line == '':\n",
    "        break\n",
    "    pmid = line.strip()\n",
    "\n",
    "    title = text_file.readline()\n",
    "    title = title[:-1] if len(title) > 0 else title\n",
    "    abstract = text_file.readline()\n",
    "    abstract= abstract[:-1] if len(abstract) > 0 else abstract \n",
    "    para = text_file.readline()\n",
    "    para = para[:-1] if len(para) > 0 else para \n",
    "    #fix space between title, abstract, para\n",
    "\n",
    "    text = title + \" \" + abstract + \" \" + para\n",
    "\n",
    "    text_file.readline()\n",
    "    #text = abstract title + text\n",
    "\n",
    "    sentences = []\n",
    "    line = sentence_file.readline()\n",
    "    if line == \"\\n\":\n",
    "        sentence_file.readline()\n",
    "    while (1):\n",
    "        line = sentence_file.readline()\n",
    "        if line == \"\\n\":\n",
    "            break\n",
    "        sentence = line.strip()\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    sent_offset = get_sent_offset(sentences, text)\n",
    "\n",
    "    anns = []\n",
    "    while (1):\n",
    "        line = ner_file.readline()\n",
    "        if line == \"\\n\":\n",
    "            break\n",
    "        ann = line.strip().split(\"\\t\")\n",
    "        if ';' in ann[5]:\n",
    "            ann[5] = ann[5].split(\";\")[1]\n",
    "            \n",
    "        # for ft only\n",
    "        if FT_MODE:\n",
    "            ann = ann[:-1]\n",
    "#         print(ann)\n",
    "        ann[1] = int(ann[1])\n",
    "        ann[2] = int(ann[2])\n",
    "        ann[-1], ann[-2] = ann[-2], ann[-1]\n",
    "        if ann[4] == 'None' or len(ann[4]) < 1:\n",
    "            continue\n",
    "        anns.append(ann[:])\n",
    "#         print(ann)\n",
    "        begin, end, mention, _id, _type = ann[1], ann[2], ann[3], ann[4], ann[5]\n",
    "        \n",
    "        if tar_pmid_lst and (pmid not in tar_pmid_lst):\n",
    "            continue\n",
    "#         print(_type)\n",
    "        sent_no = Sent_no(sent_offset, begin)\n",
    "        off_tar = offset_in_sent(sent_offset, sent_no, begin, end)\n",
    "#         print(sent_no, off_tar)\n",
    "        \n",
    "        \n",
    "        _pmid = pmid\n",
    "        _year, _journal_name, _journal_ISSN = '#', '#', '#'\n",
    "        _section, _section_nb, _sent_no = 'TITLE', 0, 00.8\n",
    "        \n",
    "        if sent_no > 0:\n",
    "            _section, _section_nb, _sent_no = 'ALL_TEXT', 1, sent_no\n",
    "\n",
    "        _entity_id = _id\n",
    "        _entity_type = 'LONGTERM|DICTIONARY'\n",
    "        _entity_norm = mention.lower()\n",
    "        _mention = mention\n",
    "        _offset = off_tar\n",
    "        _entity_parent = 'nan'\n",
    "        _sentence = sentences[sent_no]\n",
    "#     'pubmed', 'year','journal_name', \n",
    "# 'journal_ISSN', 'section', 'section_nb', 'sent_no', 'entity_id', 'entity_type', 'entity_norm', 'mention',  'offset',\n",
    "# 'entity_parent',  'sentence'\n",
    "\n",
    "    \n",
    "#         print('%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t' % (\\\n",
    "#                     _pmid, _year, _journal_name, _journal_ISSN, \\\n",
    "#                     _section, _section_nb, _sent_no, \\\n",
    "#                     _entity_id,  _entity_type,  _entity_norm, \\\n",
    "#                     _mention, _offset, _entity_parent,\\\n",
    "#                     _sentence))\n",
    "#         print(_type)\n",
    "        if _type == 'Disease':\n",
    "            new_disease_file.write('%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n' % (\\\n",
    "                    _pmid, _year, _journal_name, _journal_ISSN, \\\n",
    "                    _section, _section_nb, _sent_no, \\\n",
    "                    _entity_id,  _entity_type,  _entity_norm, \\\n",
    "                    _mention, _offset, _entity_parent,\\\n",
    "                    _sentence))\n",
    "        else:\n",
    "            new_gene_file.write('%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n' % (\\\n",
    "                    _pmid, _year, _journal_name, _journal_ISSN, \\\n",
    "                    _section, _section_nb, _sent_no, \\\n",
    "                    _entity_id,  _entity_type,  _entity_norm, \\\n",
    "                    _mention, _offset, _entity_parent,\\\n",
    "                    _sentence))\n",
    "            \n",
    "    if tar_pmid_lst and (pmid not in tar_pmid_lst):\n",
    "#     if pmid in tar_pmid_lst:\n",
    "        print(_idx, pmid)\n",
    "        _idx += 1\n",
    "#     print(abstract)\n",
    "#     print(para)\n",
    "    \n",
    "#     print(sentences)\n",
    "#     print(sent_offset)\n",
    "    \n",
    "\n",
    "\n",
    "#     print('\\n'.join([str(i) for i in anns]))\n",
    "#     break\n",
    "    \n",
    "print('end')\n",
    "    \n",
    "    \n",
    "new_gene_file.close()\n",
    "new_disease_file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "text_file.close()\n",
    "sentence_file.close()\n",
    "ner_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run BeFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbase] *",
   "language": "python",
   "name": "conda-env-nbase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
